{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Occam%27s_razor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_attrs = {'format': 'json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(url, params = my_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = soup.select_one('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Occam\\'s razor (also Ockham\\'s razor or Ocham\\'s razor; Latin: lex parsimoniae \"law of parsimony\") is the problem-solving principle that, when presented with competing hypothetical answers to a problem, one should select the answer that makes the fewest assumptions. The idea is attributed to William of Ockham (c. 1287–1347), who was an English Franciscan friar, scholastic philosopher, and theologian.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definition.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookurl = 'https://en.wikipedia.org/wiki/Book:Machine_Learning_%E2%80%93_The_Complete_Guide'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(bookurl, params = my_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resp.status_code == 200:\n",
    "    sections = soup.select('dl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<dl>\n",
       " <dt>Introduction and Main Principles</dt>\n",
       " <dd><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a></dd>\n",
       " <dd><a href=\"/wiki/Data_analysis\" title=\"Data analysis\">Data analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Occam%27s_razor\" title=\"Occam's razor\">Occam's razor</a></dd>\n",
       " <dd><a href=\"/wiki/Curse_of_dimensionality\" title=\"Curse of dimensionality\">Curse of dimensionality</a></dd>\n",
       " <dd><a href=\"/wiki/No_free_lunch_theorem\" title=\"No free lunch theorem\">No free lunch theorem</a></dd>\n",
       " <dd><a href=\"/wiki/Accuracy_paradox\" title=\"Accuracy paradox\">Accuracy paradox</a></dd>\n",
       " <dd><a href=\"/wiki/Overfitting\" title=\"Overfitting\">Overfitting</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Regularization_(machine_learning)\" title=\"Regularization (machine learning)\">Regularization (machine learning)</a></dd>\n",
       " <dd><a href=\"/wiki/Inductive_bias\" title=\"Inductive bias\">Inductive bias</a></dd>\n",
       " <dd><a href=\"/wiki/Data_dredging\" title=\"Data dredging\">Data dredging</a></dd>\n",
       " <dd><a href=\"/wiki/Ugly_duckling_theorem\" title=\"Ugly duckling theorem\">Ugly duckling theorem</a></dd>\n",
       " <dd><a href=\"/wiki/Uncertain_data\" title=\"Uncertain data\">Uncertain data</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Background and Preliminaries</dt>\n",
       " <dt>Knowledge discovery in Databases</dt>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Knowledge_discovery\" title=\"Knowledge discovery\">Knowledge discovery</a></dd>\n",
       " <dd><a href=\"/wiki/Data_mining\" title=\"Data mining\">Data mining</a></dd>\n",
       " <dd><a href=\"/wiki/Predictive_analytics\" title=\"Predictive analytics\">Predictive analytics</a></dd>\n",
       " <dd><a href=\"/wiki/Predictive_modelling\" title=\"Predictive modelling\">Predictive modelling</a></dd>\n",
       " <dd><a href=\"/wiki/Business_intelligence\" title=\"Business intelligence\">Business intelligence</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Reactive_business_intelligence\" title=\"Reactive business intelligence\">Reactive business intelligence</a></dd>\n",
       " <dd><a href=\"/wiki/Business_analytics\" title=\"Business analytics\">Business analytics</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Reactive_business_intelligence\" title=\"Reactive business intelligence\">Reactive business intelligence</a></dd>\n",
       " <dd><a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">Pattern recognition</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Reasoning</dt>\n",
       " <dd><a href=\"/wiki/Abductive_reasoning\" title=\"Abductive reasoning\">Abductive reasoning</a></dd>\n",
       " <dd><a href=\"/wiki/Inductive_reasoning\" title=\"Inductive reasoning\">Inductive reasoning</a></dd>\n",
       " <dd><a href=\"/wiki/First-order_logic\" title=\"First-order logic\">First-order logic</a></dd>\n",
       " <dd><a href=\"/wiki/Inductive_logic_programming\" title=\"Inductive logic programming\">Inductive logic programming</a></dd>\n",
       " <dd><a href=\"/wiki/Reasoning_system\" title=\"Reasoning system\">Reasoning system</a></dd>\n",
       " <dd><a href=\"/wiki/Case-based_reasoning\" title=\"Case-based reasoning\">Case-based reasoning</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Textual_case_based_reasoning\" title=\"Textual case based reasoning\">Textual case based reasoning</a></dd>\n",
       " <dd><a href=\"/wiki/Causality\" title=\"Causality\">Causality</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Search Methods</dt>\n",
       " <dd><a href=\"/wiki/Nearest_neighbor_search\" title=\"Nearest neighbor search\">Nearest neighbor search</a></dd>\n",
       " <dd><a href=\"/wiki/Stochastic_gradient_descent\" title=\"Stochastic gradient descent\">Stochastic gradient descent</a></dd>\n",
       " <dd><a href=\"/wiki/Beam_search\" title=\"Beam search\">Beam search</a></dd>\n",
       " <dd><a href=\"/wiki/Best-first_search\" title=\"Best-first search\">Best-first search</a></dd>\n",
       " <dd><a href=\"/wiki/Breadth-first_search\" title=\"Breadth-first search\">Breadth-first search</a></dd>\n",
       " <dd><a href=\"/wiki/Hill_climbing\" title=\"Hill climbing\">Hill climbing</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Grid_search\" title=\"Grid search\">Grid search</a></dd>\n",
       " <dd><a href=\"/wiki/Brute-force_search\" title=\"Brute-force search\">Brute-force search</a></dd>\n",
       " <dd><a href=\"/wiki/Depth-first_search\" title=\"Depth-first search\">Depth-first search</a></dd>\n",
       " <dd><a href=\"/wiki/Tabu_search\" title=\"Tabu search\">Tabu search</a></dd>\n",
       " <dd><a href=\"/wiki/Anytime_algorithm\" title=\"Anytime algorithm\">Anytime algorithm</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Statistics</dt>\n",
       " <dd><a href=\"/wiki/Exploratory_data_analysis\" title=\"Exploratory data analysis\">Exploratory data analysis</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Covariate\" title=\"Covariate\">Covariate</a></dd>\n",
       " <dd><a href=\"/wiki/Statistical_inference\" title=\"Statistical inference\">Statistical inference</a></dd>\n",
       " <dd><a href=\"/wiki/Algorithmic_inference\" title=\"Algorithmic inference\">Algorithmic inference</a></dd>\n",
       " <dd><a href=\"/wiki/Bayesian_inference\" title=\"Bayesian inference\">Bayesian inference</a></dd>\n",
       " <dd><a href=\"/wiki/Base_rate\" title=\"Base rate\">Base rate</a></dd>\n",
       " <dd><a href=\"/wiki/Bias_(statistics)\" title=\"Bias (statistics)\">Bias (statistics)</a></dd>\n",
       " <dd><a href=\"/wiki/Gibbs_sampling\" title=\"Gibbs sampling\">Gibbs sampling</a></dd>\n",
       " <dd><a href=\"/wiki/Cross-entropy_method\" title=\"Cross-entropy method\">Cross-entropy method</a></dd>\n",
       " <dd><a href=\"/wiki/Latent_variable\" title=\"Latent variable\">Latent variable</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Maximum_likelihood\" title=\"Maximum likelihood\">Maximum likelihood</a></dd>\n",
       " <dd><a href=\"/wiki/Maximum_a_posteriori_estimation\" title=\"Maximum a posteriori estimation\">Maximum a posteriori estimation</a></dd>\n",
       " <dd><a href=\"/wiki/Expectation%E2%80%93maximization_algorithm\" title=\"Expectation–maximization algorithm\">Expectation–maximization algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Expectation_propagation\" title=\"Expectation propagation\">Expectation propagation</a></dd>\n",
       " <dd><a href=\"/wiki/Kullback%E2%80%93Leibler_divergence\" title=\"Kullback–Leibler divergence\">Kullback–Leibler divergence</a></dd>\n",
       " <dd><a href=\"/wiki/Generative_model\" title=\"Generative model\">Generative model</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Main Learning Paradigms</dt>\n",
       " <dd><a href=\"/wiki/Supervised_learning\" title=\"Supervised learning\">Supervised learning</a></dd>\n",
       " <dd><a href=\"/wiki/Unsupervised_learning\" title=\"Unsupervised learning\">Unsupervised learning</a></dd>\n",
       " <dd><a href=\"/wiki/Active_learning_(machine_learning)\" title=\"Active learning (machine learning)\">Active learning (machine learning)</a></dd>\n",
       " <dd><a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">Reinforcement learning</a></dd>\n",
       " <dd><a href=\"/wiki/Multi-task_learning\" title=\"Multi-task learning\">Multi-task learning</a></dd>\n",
       " <dd><a href=\"/wiki/Transduction_(machine_learning)\" title=\"Transduction (machine learning)\">Transduction</a></dd>\n",
       " <dd><a href=\"/wiki/Explanation-based_learning\" title=\"Explanation-based learning\">Explanation-based learning</a></dd>\n",
       " <dd><a href=\"/wiki/Offline_learning\" title=\"Offline learning\">Offline learning</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Online_learning_model\" title=\"Online learning model\">Online learning model</a></dd>\n",
       " <dd><a href=\"/wiki/Online_machine_learning\" title=\"Online machine learning\">Online machine learning</a></dd>\n",
       " <dd><a href=\"/wiki/Hyperparameter_optimization\" title=\"Hyperparameter optimization\">Hyperparameter optimization</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Classification Tasks</dt>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Classification_in_machine_learning\" title=\"Classification in machine learning\">Classification in machine learning</a></dd>\n",
       " <dd><a href=\"/wiki/Concept_class\" title=\"Concept class\">Concept class</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Features_(pattern_recognition)\" title=\"Features (pattern recognition)\">Features (pattern recognition)</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Feature_vector\" title=\"Feature vector\">Feature vector</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Feature_space\" title=\"Feature space\">Feature space</a></dd>\n",
       " <dd><a href=\"/wiki/Concept_learning\" title=\"Concept learning\">Concept learning</a></dd>\n",
       " <dd><a href=\"/wiki/Binary_classification\" title=\"Binary classification\">Binary classification</a></dd>\n",
       " <dd><a href=\"/wiki/Decision_boundary\" title=\"Decision boundary\">Decision boundary</a></dd>\n",
       " <dd><a href=\"/wiki/Multiclass_classification\" title=\"Multiclass classification\">Multiclass classification</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Class_membership_probabilities\" title=\"Class membership probabilities\">Class membership probabilities</a></dd>\n",
       " <dd><a href=\"/wiki/Calibration_(statistics)\" title=\"Calibration (statistics)\">Calibration (statistics)</a></dd>\n",
       " <dd><a href=\"/wiki/Concept_drift\" title=\"Concept drift\">Concept drift</a></dd>\n",
       " <dd><a href=\"/wiki/Prior_knowledge_for_pattern_recognition\" title=\"Prior knowledge for pattern recognition\">Prior knowledge for pattern recognition</a></dd>\n",
       " <dd><a href=\"/wiki/Iris_flower_data_set\" title=\"Iris flower data set\">Iris flower data set</a> (<a class=\"mw-redirect\" href=\"/wiki/Classic_data_sets\" title=\"Classic data sets\">Classic data sets</a>)</dd>\n",
       " </dl>, <dl>\n",
       " <dt>Online Learning</dt>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Margin_Infused_Relaxed_Algorithm\" title=\"Margin Infused Relaxed Algorithm\">Margin Infused Relaxed Algorithm</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Semi-supervised learning</dt>\n",
       " <dd><a href=\"/wiki/Semi-supervised_learning\" title=\"Semi-supervised learning\">Semi-supervised learning</a></dd>\n",
       " <dd><a href=\"/wiki/One-class_classification\" title=\"One-class classification\">One-class classification</a></dd>\n",
       " <dd><a href=\"/wiki/Coupled_pattern_learner\" title=\"Coupled pattern learner\">Coupled pattern learner</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Lazy learning and nearest neighbors</dt>\n",
       " <dd><a href=\"/wiki/Lazy_learning\" title=\"Lazy learning\">Lazy learning</a></dd>\n",
       " <dd><a href=\"/wiki/Eager_learning\" title=\"Eager learning\">Eager learning</a></dd>\n",
       " <dd><a href=\"/wiki/Instance-based_learning\" title=\"Instance-based learning\">Instance-based learning</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Cluster_assumption\" title=\"Cluster assumption\">Cluster assumption</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/K-nearest_neighbor_algorithm\" title=\"K-nearest neighbor algorithm\">K-nearest neighbor algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/IDistance\" title=\"IDistance\">IDistance</a></dd>\n",
       " <dd><a href=\"/wiki/Large_margin_nearest_neighbor\" title=\"Large margin nearest neighbor\">Large margin nearest neighbor</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Decision Trees</dt>\n",
       " <dd><a href=\"/wiki/Decision_tree_learning\" title=\"Decision tree learning\">Decision tree learning</a></dd>\n",
       " <dd><a href=\"/wiki/Decision_stump\" title=\"Decision stump\">Decision stump</a></dd>\n",
       " <dd><a href=\"/wiki/Pruning_(decision_trees)\" title=\"Pruning (decision trees)\">Pruning (decision trees)</a></dd>\n",
       " <dd><a href=\"/wiki/Mutual_information\" title=\"Mutual information\">Mutual information</a></dd>\n",
       " <dd><a href=\"/wiki/Adjusted_mutual_information\" title=\"Adjusted mutual information\">Adjusted mutual information</a></dd>\n",
       " <dd><a href=\"/wiki/Information_gain_ratio\" title=\"Information gain ratio\">Information gain ratio</a></dd>\n",
       " <dd><a href=\"/wiki/Information_gain_in_decision_trees\" title=\"Information gain in decision trees\">Information gain in decision trees</a></dd>\n",
       " <dd><a href=\"/wiki/ID3_algorithm\" title=\"ID3 algorithm\">ID3 algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/C4.5_algorithm\" title=\"C4.5 algorithm\">C4.5 algorithm</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/CHAID\" title=\"CHAID\">CHAID</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Information_Fuzzy_Networks\" title=\"Information Fuzzy Networks\">Information Fuzzy Networks</a></dd>\n",
       " <dd><a href=\"/wiki/Grafting_(decision_trees)\" title=\"Grafting (decision trees)\">Grafting (decision trees)</a></dd>\n",
       " <dd><a href=\"/wiki/Incremental_decision_tree\" title=\"Incremental decision tree\">Incremental decision tree</a></dd>\n",
       " <dd><a href=\"/wiki/Alternating_decision_tree\" title=\"Alternating decision tree\">Alternating decision tree</a></dd>\n",
       " <dd><a href=\"/wiki/Logistic_model_tree\" title=\"Logistic model tree\">Logistic model tree</a></dd>\n",
       " <dd><a href=\"/wiki/Random_forest\" title=\"Random forest\">Random forest</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Linear Classifiers</dt>\n",
       " <dd><a href=\"/wiki/Linear_classifier\" title=\"Linear classifier\">Linear classifier</a></dd>\n",
       " <dd><a href=\"/wiki/Margin_(machine_learning)\" title=\"Margin (machine learning)\">Margin (machine learning)</a></dd>\n",
       " <dd><a href=\"/wiki/Margin_classifier\" title=\"Margin classifier\">Margin classifier</a></dd>\n",
       " <dd><a href=\"/wiki/Soft_independent_modelling_of_class_analogies\" title=\"Soft independent modelling of class analogies\">Soft independent modelling of class analogies</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Statistical classification</dt>\n",
       " <dd><a href=\"/wiki/Statistical_classification\" title=\"Statistical classification\">Statistical classification</a></dd>\n",
       " <dd><a href=\"/wiki/Probability_matching\" title=\"Probability matching\">Probability matching</a></dd>\n",
       " <dd><a href=\"/wiki/Discriminative_model\" title=\"Discriminative model\">Discriminative model</a></dd>\n",
       " <dd><a href=\"/wiki/Linear_discriminant_analysis\" title=\"Linear discriminant analysis\">Linear discriminant analysis</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Multiclass_LDA\" title=\"Multiclass LDA\">Multiclass LDA</a></dd>\n",
       " <dd><a href=\"/wiki/Multiple_discriminant_analysis\" title=\"Multiple discriminant analysis\">Multiple discriminant analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Optimal_discriminant_analysis\" title=\"Optimal discriminant analysis\">Optimal discriminant analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Fisher_kernel\" title=\"Fisher kernel\">Fisher kernel</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Discriminant_function_analysis\" title=\"Discriminant function analysis\">Discriminant function analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Multilinear_subspace_learning\" title=\"Multilinear subspace learning\">Multilinear subspace learning</a></dd>\n",
       " <dd><a href=\"/wiki/Quadratic_classifier\" title=\"Quadratic classifier\">Quadratic classifier</a></dd>\n",
       " <dd><a href=\"/wiki/Variable_kernel_density_estimation\" title=\"Variable kernel density estimation\">Variable kernel density estimation</a></dd>\n",
       " <dd><a href=\"/wiki/Category_utility\" title=\"Category utility\">Category utility</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Evaluation of Classification Models</dt>\n",
       " <dd><a href=\"/wiki/Data_classification_(business_intelligence)\" title=\"Data classification (business intelligence)\">Data classification (business intelligence)</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Training_set\" title=\"Training set\">Training set</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Test_set\" title=\"Test set\">Test set</a></dd>\n",
       " <dd><a href=\"/wiki/Synthetic_data\" title=\"Synthetic data\">Synthetic data</a></dd>\n",
       " <dd><a href=\"/wiki/Cross-validation_(statistics)\" title=\"Cross-validation (statistics)\">Cross-validation (statistics)</a></dd>\n",
       " <dd><a href=\"/wiki/Loss_function\" title=\"Loss function\">Loss function</a></dd>\n",
       " <dd><a href=\"/wiki/Hinge_loss\" title=\"Hinge loss\">Hinge loss</a></dd>\n",
       " <dd><a href=\"/wiki/Generalization_error\" title=\"Generalization error\">Generalization error</a></dd>\n",
       " <dd><a href=\"/wiki/Type_I_and_type_II_errors\" title=\"Type I and type II errors\">Type I and type II errors</a></dd>\n",
       " <dd><a href=\"/wiki/Sensitivity_and_specificity\" title=\"Sensitivity and specificity\">Sensitivity and specificity</a></dd>\n",
       " <dd><a href=\"/wiki/Precision_and_recall\" title=\"Precision and recall\">Precision and recall</a></dd>\n",
       " <dd><a href=\"/wiki/F1_score\" title=\"F1 score\">F1 score</a></dd>\n",
       " <dd><a href=\"/wiki/Confusion_matrix\" title=\"Confusion matrix\">Confusion matrix</a></dd>\n",
       " <dd><a href=\"/wiki/Matthews_correlation_coefficient\" title=\"Matthews correlation coefficient\">Matthews correlation coefficient</a></dd>\n",
       " <dd><a href=\"/wiki/Receiver_operating_characteristic\" title=\"Receiver operating characteristic\">Receiver operating characteristic</a></dd>\n",
       " <dd><a href=\"/wiki/Lift_(data_mining)\" title=\"Lift (data mining)\">Lift (data mining)</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Stability_in_learning\" title=\"Stability in learning\">Stability in learning</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Feature Creation and Optimization</dt>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Data_Pre-processing\" title=\"Data Pre-processing\">Data Pre-processing</a></dd>\n",
       " <dd><a href=\"/wiki/Discretization_of_continuous_features\" title=\"Discretization of continuous features\">Discretization of continuous features</a></dd>\n",
       " <dd><a href=\"/wiki/Feature_engineering\" title=\"Feature engineering\">Feature engineering</a></dd>\n",
       " <dd><a href=\"/wiki/Feature_selection\" title=\"Feature selection\">Feature selection</a></dd>\n",
       " <dd><a href=\"/wiki/Feature_extraction\" title=\"Feature extraction\">Feature extraction</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Dimension_reduction\" title=\"Dimension reduction\">Dimension reduction</a></dd>\n",
       " <dd><a href=\"/wiki/Principal_component_analysis\" title=\"Principal component analysis\">Principal component analysis</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Multilinear_principal-component_analysis\" title=\"Multilinear principal-component analysis\">Multilinear principal-component analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Multifactor_dimensionality_reduction\" title=\"Multifactor dimensionality reduction\">Multifactor dimensionality reduction</a></dd>\n",
       " <dd><a href=\"/wiki/Targeted_projection_pursuit\" title=\"Targeted projection pursuit\">Targeted projection pursuit</a></dd>\n",
       " <dd><a href=\"/wiki/Multidimensional_scaling\" title=\"Multidimensional scaling\">Multidimensional scaling</a></dd>\n",
       " <dd><a href=\"/wiki/Nonlinear_dimensionality_reduction\" title=\"Nonlinear dimensionality reduction\">Nonlinear dimensionality reduction</a></dd>\n",
       " <dd><a href=\"/wiki/Kernel_principal_component_analysis\" title=\"Kernel principal component analysis\">Kernel principal component analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Kernel_eigenvoice\" title=\"Kernel eigenvoice\">Kernel eigenvoice</a></dd>\n",
       " <dd><a href=\"/wiki/Gramian_matrix\" title=\"Gramian matrix\">Gramian matrix</a></dd>\n",
       " <dd><a href=\"/wiki/Gaussian_process\" title=\"Gaussian process\">Gaussian process</a></dd>\n",
       " <dd><a href=\"/wiki/Kernel_adaptive_filter\" title=\"Kernel adaptive filter\">Kernel adaptive filter</a></dd>\n",
       " <dd><a href=\"/wiki/Isomap\" title=\"Isomap\">Isomap</a></dd>\n",
       " <dd><a href=\"/wiki/Manifold_alignment\" title=\"Manifold alignment\">Manifold alignment</a></dd>\n",
       " <dd><a href=\"/wiki/Diffusion_map\" title=\"Diffusion map\">Diffusion map</a></dd>\n",
       " <dd><a href=\"/wiki/Elastic_map\" title=\"Elastic map\">Elastic map</a></dd>\n",
       " <dd><a href=\"/wiki/Locality-sensitive_hashing\" title=\"Locality-sensitive hashing\">Locality-sensitive hashing</a></dd>\n",
       " <dd><a href=\"/wiki/Spectral_clustering\" title=\"Spectral clustering\">Spectral clustering</a></dd>\n",
       " <dd><a href=\"/wiki/Minimum_redundancy_feature_selection\" title=\"Minimum redundancy feature selection\">Minimum redundancy feature selection</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Clustering</dt>\n",
       " <dd><a href=\"/wiki/Cluster_analysis\" title=\"Cluster analysis\">Cluster analysis</a></dd>\n",
       " <dd><a href=\"/wiki/K-means_clustering\" title=\"K-means clustering\">K-means clustering</a></dd>\n",
       " <dd><a href=\"/wiki/K-means%2B%2B\" title=\"K-means++\">K-means++</a></dd>\n",
       " <dd><a href=\"/wiki/K-medians_clustering\" title=\"K-medians clustering\">K-medians clustering</a></dd>\n",
       " <dd><a href=\"/wiki/K-medoids\" title=\"K-medoids\">K-medoids</a></dd>\n",
       " <dd><a href=\"/wiki/DBSCAN\" title=\"DBSCAN\">DBSCAN</a></dd>\n",
       " <dd><a href=\"/wiki/Fuzzy_clustering\" title=\"Fuzzy clustering\">Fuzzy clustering</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/BIRCH_(data_clustering)\" title=\"BIRCH (data clustering)\">BIRCH (data clustering)</a></dd>\n",
       " <dd><a href=\"/wiki/Canopy_clustering_algorithm\" title=\"Canopy clustering algorithm\">Canopy clustering algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Cluster-weighted_modeling\" title=\"Cluster-weighted modeling\">Cluster-weighted modeling</a></dd>\n",
       " <dd><a href=\"/wiki/Clustering_high-dimensional_data\" title=\"Clustering high-dimensional data\">Clustering high-dimensional data</a></dd>\n",
       " <dd><a href=\"/wiki/Cobweb_(clustering)\" title=\"Cobweb (clustering)\">Cobweb (clustering)</a></dd>\n",
       " <dd><a href=\"/wiki/Complete-linkage_clustering\" title=\"Complete-linkage clustering\">Complete-linkage clustering</a></dd>\n",
       " <dd><a href=\"/wiki/Constrained_clustering\" title=\"Constrained clustering\">Constrained clustering</a></dd>\n",
       " <dd><a href=\"/wiki/Correlation_clustering\" title=\"Correlation clustering\">Correlation clustering</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/CURE_data_clustering_algorithm\" title=\"CURE data clustering algorithm\">CURE data clustering algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Data_stream_clustering\" title=\"Data stream clustering\">Data stream clustering</a></dd>\n",
       " <dd><a href=\"/wiki/Dendrogram\" title=\"Dendrogram\">Dendrogram</a></dd>\n",
       " <dd><a href=\"/wiki/Determining_the_number_of_clusters_in_a_data_set\" title=\"Determining the number of clusters in a data set\">Determining the number of clusters in a data set</a></dd>\n",
       " <dd><a href=\"/wiki/FLAME_clustering\" title=\"FLAME clustering\">FLAME clustering</a></dd>\n",
       " <dd><a href=\"/wiki/Hierarchical_clustering\" title=\"Hierarchical clustering\">Hierarchical clustering</a></dd>\n",
       " <dd><a href=\"/wiki/Information_bottleneck_method\" title=\"Information bottleneck method\">Information bottleneck method</a></dd>\n",
       " <dd><a href=\"/wiki/Lloyd%27s_algorithm\" title=\"Lloyd's algorithm\">Lloyd's algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Nearest-neighbor_chain_algorithm\" title=\"Nearest-neighbor chain algorithm\">Nearest-neighbor chain algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Neighbor_joining\" title=\"Neighbor joining\">Neighbor joining</a></dd>\n",
       " <dd><a href=\"/wiki/OPTICS_algorithm\" title=\"OPTICS algorithm\">OPTICS algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Pitman%E2%80%93Yor_process\" title=\"Pitman–Yor process\">Pitman–Yor process</a></dd>\n",
       " <dd><a href=\"/wiki/Single-linkage_clustering\" title=\"Single-linkage clustering\">Single-linkage clustering</a></dd>\n",
       " <dd><a href=\"/wiki/SUBCLU\" title=\"SUBCLU\">SUBCLU</a></dd>\n",
       " <dd><a href=\"/wiki/Thresholding_(image_processing)\" title=\"Thresholding (image processing)\">Thresholding (image processing)</a></dd>\n",
       " <dd><a href=\"/wiki/UPGMA\" title=\"UPGMA\">UPGMA</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Evaluation of Clustering Methods</dt>\n",
       " <dd><a href=\"/wiki/Rand_index\" title=\"Rand index\">Rand index</a></dd>\n",
       " <dd><a href=\"/wiki/Dunn_index\" title=\"Dunn index\">Dunn index</a></dd>\n",
       " <dd><a href=\"/wiki/Davies%E2%80%93Bouldin_index\" title=\"Davies–Bouldin index\">Davies–Bouldin index</a></dd>\n",
       " <dd><a href=\"/wiki/Jaccard_index\" title=\"Jaccard index\">Jaccard index</a></dd>\n",
       " <dd><a href=\"/wiki/MinHash\" title=\"MinHash\">MinHash</a></dd>\n",
       " <dd><a href=\"/wiki/K_q-flats\" title=\"K q-flats\">K q-flats</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Rule Induction</dt>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Decision_rules\" title=\"Decision rules\">Decision rules</a></dd>\n",
       " <dd><a href=\"/wiki/Rule_induction\" title=\"Rule induction\">Rule induction</a></dd>\n",
       " <dd><a href=\"/wiki/Classification_rule\" title=\"Classification rule\">Classification rule</a></dd>\n",
       " <dd><a href=\"/wiki/CN2_algorithm\" title=\"CN2 algorithm\">CN2 algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Decision_list\" title=\"Decision list\">Decision list</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/First_Order_Inductive_Learner\" title=\"First Order Inductive Learner\">First Order Inductive Learner</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Association rules and Frequent Item Sets</dt>\n",
       " <dd><a href=\"/wiki/Association_rule_learning\" title=\"Association rule learning\">Association rule learning</a></dd>\n",
       " <dd><a href=\"/wiki/Apriori_algorithm\" title=\"Apriori algorithm\">Apriori algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Contrast_set_learning\" title=\"Contrast set learning\">Contrast set learning</a></dd>\n",
       " <dd><a href=\"/wiki/Affinity_analysis\" title=\"Affinity analysis\">Affinity analysis</a></dd>\n",
       " <dd><a href=\"/wiki/K-optimal_pattern_discovery\" title=\"K-optimal pattern discovery\">K-optimal pattern discovery</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Ensemble Learning</dt>\n",
       " <dd><a href=\"/wiki/Ensemble_learning\" title=\"Ensemble learning\">Ensemble learning</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Ensemble_averaging\" title=\"Ensemble averaging\">Ensemble averaging</a></dd>\n",
       " <dd><a href=\"/wiki/Consensus_clustering\" title=\"Consensus clustering\">Consensus clustering</a></dd>\n",
       " <dd><a href=\"/wiki/AdaBoost\" title=\"AdaBoost\">AdaBoost</a></dd>\n",
       " <dd><a class=\"mw-redirect mw-disambig\" href=\"/wiki/Boosting\" title=\"Boosting\">Boosting</a></dd>\n",
       " <dd><a href=\"/wiki/Bootstrap_aggregating\" title=\"Bootstrap aggregating\">Bootstrap aggregating</a></dd>\n",
       " <dd><a href=\"/wiki/BrownBoost\" title=\"BrownBoost\">BrownBoost</a></dd>\n",
       " <dd><a href=\"/wiki/Cascading_classifiers\" title=\"Cascading classifiers\">Cascading classifiers</a></dd>\n",
       " <dd><a href=\"/wiki/Co-training\" title=\"Co-training\">Co-training</a></dd>\n",
       " <dd><a href=\"/wiki/CoBoosting\" title=\"CoBoosting\">CoBoosting</a></dd>\n",
       " <dd><a href=\"/wiki/Gaussian_process_emulator\" title=\"Gaussian process emulator\">Gaussian process emulator</a></dd>\n",
       " <dd><a href=\"/wiki/Gradient_boosting\" title=\"Gradient boosting\">Gradient boosting</a></dd>\n",
       " <dd><a href=\"/wiki/LogitBoost\" title=\"LogitBoost\">LogitBoost</a></dd>\n",
       " <dd><a href=\"/wiki/LPBoost\" title=\"LPBoost\">LPBoost</a></dd>\n",
       " <dd><a href=\"/wiki/Mixture_model\" title=\"Mixture model\">Mixture model</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Product_of_Experts\" title=\"Product of Experts\">Product of Experts</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Random_multinomial_logit\" title=\"Random multinomial logit\">Random multinomial logit</a></dd>\n",
       " <dd><a href=\"/wiki/Random_subspace_method\" title=\"Random subspace method\">Random subspace method</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Weighted_Majority_Algorithm\" title=\"Weighted Majority Algorithm\">Weighted Majority Algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Randomized_weighted_majority_algorithm\" title=\"Randomized weighted majority algorithm\">Randomized weighted majority algorithm</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Graphical Models</dt>\n",
       " <dd><a href=\"/wiki/Graphical_model\" title=\"Graphical model\">Graphical model</a></dd>\n",
       " <dd><a href=\"/wiki/State_transition_network\" title=\"State transition network\">State transition network</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Bayesian Learning Methods</dt>\n",
       " <dd><a href=\"/wiki/Naive_Bayes_classifier\" title=\"Naive Bayes classifier\">Naive Bayes classifier</a></dd>\n",
       " <dd><a href=\"/wiki/Averaged_one-dependence_estimators\" title=\"Averaged one-dependence estimators\">Averaged one-dependence estimators</a></dd>\n",
       " <dd><a href=\"/wiki/Bayesian_network\" title=\"Bayesian network\">Bayesian network</a></dd>\n",
       " <dd><a href=\"/wiki/Variational_message_passing\" title=\"Variational message passing\">Variational message passing</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Markov Models</dt>\n",
       " <dd><a href=\"/wiki/Markov_model\" title=\"Markov model\">Markov model</a></dd>\n",
       " <dd><a href=\"/wiki/Maximum-entropy_Markov_model\" title=\"Maximum-entropy Markov model\">Maximum-entropy Markov model</a></dd>\n",
       " <dd><a href=\"/wiki/Hidden_Markov_model\" title=\"Hidden Markov model\">Hidden Markov model</a></dd>\n",
       " <dd><a href=\"/wiki/Baum%E2%80%93Welch_algorithm\" title=\"Baum–Welch algorithm\">Baum–Welch algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Forward%E2%80%93backward_algorithm\" title=\"Forward–backward algorithm\">Forward–backward algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Hierarchical_hidden_Markov_model\" title=\"Hierarchical hidden Markov model\">Hierarchical hidden Markov model</a></dd>\n",
       " <dd><a href=\"/wiki/Markov_logic_network\" title=\"Markov logic network\">Markov logic network</a></dd>\n",
       " <dd><a href=\"/wiki/Markov_chain_Monte_Carlo\" title=\"Markov chain Monte Carlo\">Markov chain Monte Carlo</a></dd>\n",
       " <dd><a href=\"/wiki/Markov_random_field\" title=\"Markov random field\">Markov random field</a></dd>\n",
       " <dd><a href=\"/wiki/Conditional_random_field\" title=\"Conditional random field\">Conditional random field</a></dd>\n",
       " <dd><a href=\"/wiki/Predictive_state_representation\" title=\"Predictive state representation\">Predictive state representation</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Learning Theory</dt>\n",
       " <dd><a href=\"/wiki/Computational_learning_theory\" title=\"Computational learning theory\">Computational learning theory</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Version_space\" title=\"Version space\">Version space</a></dd>\n",
       " <dd><a href=\"/wiki/Probably_approximately_correct_learning\" title=\"Probably approximately correct learning\">Probably approximately correct learning</a></dd>\n",
       " <dd><a href=\"/wiki/Vapnik%E2%80%93Chervonenkis_theory\" title=\"Vapnik–Chervonenkis theory\">Vapnik–Chervonenkis theory</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Shattering_(machine_learning)\" title=\"Shattering (machine learning)\">Shattering (machine learning)</a></dd>\n",
       " <dd><a href=\"/wiki/VC_dimension\" title=\"VC dimension\">VC dimension</a></dd>\n",
       " <dd><a href=\"/wiki/Minimum_description_length\" title=\"Minimum description length\">Minimum description length</a></dd>\n",
       " <dd><a href=\"/wiki/Bondy%27s_theorem\" title=\"Bondy's theorem\">Bondy's theorem</a></dd>\n",
       " <dd><a href=\"/wiki/Inferential_theory_of_learning\" title=\"Inferential theory of learning\">Inferential theory of learning</a></dd>\n",
       " <dd><a href=\"/wiki/Rademacher_complexity\" title=\"Rademacher complexity\">Rademacher complexity</a></dd>\n",
       " <dd><a href=\"/wiki/Teaching_dimension\" title=\"Teaching dimension\">Teaching dimension</a></dd>\n",
       " <dd><a href=\"/wiki/Subclass_reachability\" title=\"Subclass reachability\">Subclass reachability</a></dd>\n",
       " <dd><a href=\"/wiki/Sample_exclusion_dimension\" title=\"Sample exclusion dimension\">Sample exclusion dimension</a></dd>\n",
       " <dd><a href=\"/wiki/Unique_negative_dimension\" title=\"Unique negative dimension\">Unique negative dimension</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Uniform_convergence_(combinatorics)\" title=\"Uniform convergence (combinatorics)\">Uniform convergence (combinatorics)</a></dd>\n",
       " <dd><a href=\"/wiki/Witness_set\" title=\"Witness set\">Witness set</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Support Vector Machines</dt>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Kernel_methods\" title=\"Kernel methods\">Kernel methods</a></dd>\n",
       " <dd><a href=\"/wiki/Support_vector_machine\" title=\"Support vector machine\">Support vector machine</a></dd>\n",
       " <dd><a href=\"/wiki/Structural_risk_minimization\" title=\"Structural risk minimization\">Structural risk minimization</a></dd>\n",
       " <dd><a href=\"/wiki/Empirical_risk_minimization\" title=\"Empirical risk minimization\">Empirical risk minimization</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Kernel_trick\" title=\"Kernel trick\">Kernel trick</a></dd>\n",
       " <dd><a href=\"/wiki/Least_squares_support_vector_machine\" title=\"Least squares support vector machine\">Least squares support vector machine</a></dd>\n",
       " <dd><a href=\"/wiki/Relevance_vector_machine\" title=\"Relevance vector machine\">Relevance vector machine</a></dd>\n",
       " <dd><a href=\"/wiki/Sequential_minimal_optimization\" title=\"Sequential minimal optimization\">Sequential minimal optimization</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Structured_SVM\" title=\"Structured SVM\">Structured SVM</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Regression analysis</dt>\n",
       " <dd><a href=\"/wiki/Outline_of_regression_analysis\" title=\"Outline of regression analysis\">Outline of regression analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Regression_analysis\" title=\"Regression analysis\">Regression analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Dependent_and_independent_variables\" title=\"Dependent and independent variables\">Dependent and independent variables</a></dd>\n",
       " <dd><a href=\"/wiki/Linear_model\" title=\"Linear model\">Linear model</a></dd>\n",
       " <dd><a href=\"/wiki/Linear_regression\" title=\"Linear regression\">Linear regression</a></dd>\n",
       " <dd><a href=\"/wiki/Least_squares\" title=\"Least squares\">Least squares</a></dd>\n",
       " <dd><a href=\"/wiki/Linear_least_squares_(mathematics)\" title=\"Linear least squares (mathematics)\">Linear least squares (mathematics)</a></dd>\n",
       " <dd><a href=\"/wiki/Local_regression\" title=\"Local regression\">Local regression</a></dd>\n",
       " <dd><a href=\"/wiki/Additive_model\" title=\"Additive model\">Additive model</a></dd>\n",
       " <dd><a href=\"/wiki/Antecedent_variable\" title=\"Antecedent variable\">Antecedent variable</a></dd>\n",
       " <dd><a href=\"/wiki/Autocorrelation\" title=\"Autocorrelation\">Autocorrelation</a></dd>\n",
       " <dd><a href=\"/wiki/Backfitting_algorithm\" title=\"Backfitting algorithm\">Backfitting algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Bayesian_linear_regression\" title=\"Bayesian linear regression\">Bayesian linear regression</a></dd>\n",
       " <dd><a href=\"/wiki/Bayesian_multivariate_linear_regression\" title=\"Bayesian multivariate linear regression\">Bayesian multivariate linear regression</a></dd>\n",
       " <dd><a href=\"/wiki/Binomial_regression\" title=\"Binomial regression\">Binomial regression</a></dd>\n",
       " <dd><a href=\"/wiki/Canonical_analysis\" title=\"Canonical analysis\">Canonical analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Censored_regression_model\" title=\"Censored regression model\">Censored regression model</a></dd>\n",
       " <dd><a href=\"/wiki/Coefficient_of_determination\" title=\"Coefficient of determination\">Coefficient of determination</a></dd>\n",
       " <dd><a href=\"/wiki/Comparison_of_general_and_generalized_linear_models\" title=\"Comparison of general and generalized linear models\">Comparison of general and generalized linear models</a></dd>\n",
       " <dd><a href=\"/wiki/Compressed_sensing\" title=\"Compressed sensing\">Compressed sensing</a></dd>\n",
       " <dd><a href=\"/wiki/Conditional_change_model\" title=\"Conditional change model\">Conditional change model</a></dd>\n",
       " <dd><a href=\"/wiki/Controlling_for_a_variable\" title=\"Controlling for a variable\">Controlling for a variable</a></dd>\n",
       " <dd><a href=\"/wiki/Cross-sectional_regression\" title=\"Cross-sectional regression\">Cross-sectional regression</a></dd>\n",
       " <dd><a href=\"/wiki/Curve_fitting\" title=\"Curve fitting\">Curve fitting</a></dd>\n",
       " <dd><a href=\"/wiki/Deming_regression\" title=\"Deming regression\">Deming regression</a></dd>\n",
       " <dd><a href=\"/wiki/Design_matrix\" title=\"Design matrix\">Design matrix</a></dd>\n",
       " <dd><a href=\"/wiki/Difference_in_differences\" title=\"Difference in differences\">Difference in differences</a></dd>\n",
       " <dd><a href=\"/wiki/Dummy_variable_(statistics)\" title=\"Dummy variable (statistics)\">Dummy variable (statistics)</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Errors_and_residuals_in_statistics\" title=\"Errors and residuals in statistics\">Errors and residuals in statistics</a></dd>\n",
       " <dd><a href=\"/wiki/Errors-in-variables_models\" title=\"Errors-in-variables models\">Errors-in-variables models</a></dd>\n",
       " <dd><a href=\"/wiki/Explained_sum_of_squares\" title=\"Explained sum of squares\">Explained sum of squares</a></dd>\n",
       " <dd><a href=\"/wiki/Explained_variation\" title=\"Explained variation\">Explained variation</a></dd>\n",
       " <dd><a href=\"/wiki/First-hitting-time_model\" title=\"First-hitting-time model\">First-hitting-time model</a></dd>\n",
       " <dd><a href=\"/wiki/Fixed_effects_model\" title=\"Fixed effects model\">Fixed effects model</a></dd>\n",
       " <dd><a href=\"/wiki/Fraction_of_variance_unexplained\" title=\"Fraction of variance unexplained\">Fraction of variance unexplained</a></dd>\n",
       " <dd><a href=\"/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem\" title=\"Frisch–Waugh–Lovell theorem\">Frisch–Waugh–Lovell theorem</a></dd>\n",
       " <dd><a href=\"/wiki/General_linear_model\" title=\"General linear model\">General linear model</a></dd>\n",
       " <dd><a href=\"/wiki/Generalized_additive_model\" title=\"Generalized additive model\">Generalized additive model</a></dd>\n",
       " <dd><a href=\"/wiki/Generalized_additive_model_for_location,_scale_and_shape\" title=\"Generalized additive model for location, scale and shape\">Generalized additive model for location, scale and shape</a></dd>\n",
       " <dd><a href=\"/wiki/Generalized_estimating_equation\" title=\"Generalized estimating equation\">Generalized estimating equation</a></dd>\n",
       " <dd><a href=\"/wiki/Generalized_least_squares\" title=\"Generalized least squares\">Generalized least squares</a></dd>\n",
       " <dd><a href=\"/wiki/Generalized_linear_array_model\" title=\"Generalized linear array model\">Generalized linear array model</a></dd>\n",
       " <dd><a href=\"/wiki/Generalized_linear_mixed_model\" title=\"Generalized linear mixed model\">Generalized linear mixed model</a></dd>\n",
       " <dd><a href=\"/wiki/Generalized_linear_model\" title=\"Generalized linear model\">Generalized linear model</a></dd>\n",
       " <dd><a class=\"mw-disambig\" href=\"/wiki/Growth_curve\" title=\"Growth curve\">Growth curve</a></dd>\n",
       " <dd><a href=\"/wiki/Guess_value\" title=\"Guess value\">Guess value</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Hat_matrix\" title=\"Hat matrix\">Hat matrix</a></dd>\n",
       " <dd><a href=\"/wiki/Heckman_correction\" title=\"Heckman correction\">Heckman correction</a></dd>\n",
       " <dd><a href=\"/wiki/Heteroscedasticity-consistent_standard_errors\" title=\"Heteroscedasticity-consistent standard errors\">Heteroscedasticity-consistent standard errors</a></dd>\n",
       " <dd><a href=\"/wiki/Hosmer%E2%80%93Lemeshow_test\" title=\"Hosmer–Lemeshow test\">Hosmer–Lemeshow test</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Instrumental_variable\" title=\"Instrumental variable\">Instrumental variable</a></dd>\n",
       " <dd><a href=\"/wiki/Interaction_(statistics)\" title=\"Interaction (statistics)\">Interaction (statistics)</a></dd>\n",
       " <dd><a href=\"/wiki/Isotonic_regression\" title=\"Isotonic regression\">Isotonic regression</a></dd>\n",
       " <dd><a href=\"/wiki/Iteratively_reweighted_least_squares\" title=\"Iteratively reweighted least squares\">Iteratively reweighted least squares</a></dd>\n",
       " <dd><a href=\"/wiki/Kitchen_sink_regression\" title=\"Kitchen sink regression\">Kitchen sink regression</a></dd>\n",
       " <dd><a href=\"/wiki/Lack-of-fit_sum_of_squares\" title=\"Lack-of-fit sum of squares\">Lack-of-fit sum of squares</a></dd>\n",
       " <dd><a href=\"/wiki/Leverage_(statistics)\" title=\"Leverage (statistics)\">Leverage (statistics)</a></dd>\n",
       " <dd><a href=\"/wiki/Limited_dependent_variable\" title=\"Limited dependent variable\">Limited dependent variable</a></dd>\n",
       " <dd><a href=\"/wiki/Linear_probability_model\" title=\"Linear probability model\">Linear probability model</a></dd>\n",
       " <dd><a href=\"/wiki/Mallows%27s_Cp\" title=\"Mallows's Cp\">Mallows's <i>C<sub>p</sub></i></a></dd>\n",
       " <dd><a href=\"/wiki/Mean_and_predicted_response\" title=\"Mean and predicted response\">Mean and predicted response</a></dd>\n",
       " <dd><a href=\"/wiki/Mixed_model\" title=\"Mixed model\">Mixed model</a></dd>\n",
       " <dd><a href=\"/wiki/Moderation_(statistics)\" title=\"Moderation (statistics)\">Moderation (statistics)</a></dd>\n",
       " <dd><a href=\"/wiki/Moving_least_squares\" title=\"Moving least squares\">Moving least squares</a></dd>\n",
       " <dd><a href=\"/wiki/Multicollinearity\" title=\"Multicollinearity\">Multicollinearity</a></dd>\n",
       " <dd><a href=\"/wiki/Multiple_correlation\" title=\"Multiple correlation\">Multiple correlation</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Multivariate_probit\" title=\"Multivariate probit\">Multivariate probit</a></dd>\n",
       " <dd><a href=\"/wiki/Multivariate_adaptive_regression_splines\" title=\"Multivariate adaptive regression splines\">Multivariate adaptive regression splines</a></dd>\n",
       " <dd><a href=\"/wiki/Newey%E2%80%93West_estimator\" title=\"Newey–West estimator\">Newey–West estimator</a></dd>\n",
       " <dd><a href=\"/wiki/Non-linear_least_squares\" title=\"Non-linear least squares\">Non-linear least squares</a></dd>\n",
       " <dd><a href=\"/wiki/Nonlinear_regression\" title=\"Nonlinear regression\">Nonlinear regression</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Logistic Regression</dt>\n",
       " <dd><a href=\"/wiki/Logit\" title=\"Logit\">Logit</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Multinomial_logit\" title=\"Multinomial logit\">Multinomial logit</a></dd>\n",
       " <dd><a href=\"/wiki/Logistic_regression\" title=\"Logistic regression\">Logistic regression</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Bio-inspired Methods</dt>\n",
       " <dd><a href=\"/wiki/Bio-inspired_computing\" title=\"Bio-inspired computing\">Bio-inspired computing</a></dd>\n",
       " <dd><a href=\"/wiki/Metaheuristic\" title=\"Metaheuristic\">Metaheuristic</a> and search algs. there</dd>\n",
       " <dd><a href=\"/wiki/Swarm_intelligence\" title=\"Swarm intelligence\">Swarm intelligence</a> and methods there\n",
       " <dl>\n",
       " <dd><b>Particular algorithms:</b></dd>\n",
       " <dd><a href=\"/wiki/Particle_swarm_optimization\" title=\"Particle swarm optimization\">Particle_swarm_optimization</a></dd>\n",
       " <dd><a href=\"/wiki/Ant_colony_optimization_algorithms\" title=\"Ant colony optimization algorithms\">Ant colony optimization algorithms</a></dd>\n",
       " <dd><a href=\"/wiki/Artificial_immune_system\" title=\"Artificial immune system\">Artificial immune system</a></dd>\n",
       " <dd><a href=\"/wiki/Firefly_algorithm\" title=\"Firefly algorithm\">Firefly algorithm</a>, 2008</dd>\n",
       " <dd><a href=\"/wiki/Cuckoo_search\" title=\"Cuckoo search\">Cuckoo search</a>, 2009</dd>\n",
       " <dd><a href=\"/wiki/Bat_algorithm\" title=\"Bat algorithm\">Bat algorithm</a>, 2010</dd>\n",
       " </dl>\n",
       " </dd>\n",
       " </dl>, <dl>\n",
       " <dd><b>Particular algorithms:</b></dd>\n",
       " <dd><a href=\"/wiki/Particle_swarm_optimization\" title=\"Particle swarm optimization\">Particle_swarm_optimization</a></dd>\n",
       " <dd><a href=\"/wiki/Ant_colony_optimization_algorithms\" title=\"Ant colony optimization algorithms\">Ant colony optimization algorithms</a></dd>\n",
       " <dd><a href=\"/wiki/Artificial_immune_system\" title=\"Artificial immune system\">Artificial immune system</a></dd>\n",
       " <dd><a href=\"/wiki/Firefly_algorithm\" title=\"Firefly algorithm\">Firefly algorithm</a>, 2008</dd>\n",
       " <dd><a href=\"/wiki/Cuckoo_search\" title=\"Cuckoo search\">Cuckoo search</a>, 2009</dd>\n",
       " <dd><a href=\"/wiki/Bat_algorithm\" title=\"Bat algorithm\">Bat algorithm</a>, 2010</dd>\n",
       " </dl>, <dl>\n",
       " <dt>Evolutionary Algorithms</dt>\n",
       " <dd><a href=\"/wiki/Evolvability_(computer_science)\" title=\"Evolvability (computer science)\">Evolvability (computer science)</a></dd>\n",
       " <dd><a href=\"/wiki/Evolutionary_computation\" title=\"Evolutionary computation\">Evolutionary computation</a></dd>\n",
       " <dd><a href=\"/wiki/Evolutionary_algorithm\" title=\"Evolutionary algorithm\">Evolutionary algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Genetic_algorithm\" title=\"Genetic algorithm\">Genetic algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Chromosome_(genetic_algorithm)\" title=\"Chromosome (genetic algorithm)\">Chromosome (genetic algorithm)</a></dd>\n",
       " <dd><a href=\"/wiki/Crossover_(genetic_algorithm)\" title=\"Crossover (genetic algorithm)\">Crossover (genetic algorithm)</a></dd>\n",
       " <dd><a href=\"/wiki/Fitness_function\" title=\"Fitness function\">Fitness function</a></dd>\n",
       " <dd><a href=\"/wiki/Evolutionary_data_mining\" title=\"Evolutionary data mining\">Evolutionary data mining</a></dd>\n",
       " <dd><a href=\"/wiki/Genetic_programming\" title=\"Genetic programming\">Genetic programming</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Learnable_Evolution_Model\" title=\"Learnable Evolution Model\">Learnable Evolution Model</a></dd>\n",
       " <dd><a href=\"/wiki/Stochastic_diffusion_search\" title=\"Stochastic diffusion search\">Stochastic diffusion search</a> (SDS)</dd>\n",
       " </dl>, <dl>\n",
       " <dt>Neural Networks</dt>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Neural_network\" title=\"Neural network\">Neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">Artificial neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Artificial_neuron\" title=\"Artificial neuron\">Artificial neuron</a></dd>\n",
       " <dd><a href=\"/wiki/Types_of_artificial_neural_networks\" title=\"Types of artificial neural networks\">Types of artificial neural networks</a></dd>\n",
       " <dd><a href=\"/wiki/Perceptron\" title=\"Perceptron\">Perceptron</a></dd>\n",
       " <dd><a href=\"/wiki/Multilayer_perceptron\" title=\"Multilayer perceptron\">Multilayer perceptron</a></dd>\n",
       " <dd><a href=\"/wiki/Activation_function\" title=\"Activation function\">Activation function</a></dd>\n",
       " <dd><a href=\"/wiki/Self-organizing_map\" title=\"Self-organizing map\">Self-organizing map</a></dd>\n",
       " <dd><a href=\"/wiki/Attractor_network\" title=\"Attractor network\">Attractor network</a></dd>\n",
       " <dd><a href=\"/wiki/ADALINE\" title=\"ADALINE\">ADALINE</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Adaptive_Neuro_Fuzzy_Inference_System\" title=\"Adaptive Neuro Fuzzy Inference System\">Adaptive Neuro Fuzzy Inference System</a></dd>\n",
       " <dd><a href=\"/wiki/Adaptive_resonance_theory\" title=\"Adaptive resonance theory\">Adaptive resonance theory</a></dd>\n",
       " <dd><a href=\"/wiki/IPO_underpricing_algorithm\" title=\"IPO underpricing algorithm\">IPO underpricing algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/ALOPEX\" title=\"ALOPEX\">ALOPEX</a></dd>\n",
       " <dd><a href=\"/wiki/Artificial_Intelligence_System\" title=\"Artificial Intelligence System\">Artificial Intelligence System</a></dd>\n",
       " <dd><a href=\"/wiki/Autoassociative_memory\" title=\"Autoassociative memory\">Autoassociative memory</a></dd>\n",
       " <dd><a href=\"/wiki/Autoencoder\" title=\"Autoencoder\">Autoencoder</a></dd>\n",
       " <dd><a href=\"/wiki/Backpropagation\" title=\"Backpropagation\">Backpropagation</a></dd>\n",
       " <dd><a href=\"/wiki/Bcpnn\" title=\"Bcpnn\">Bcpnn</a></dd>\n",
       " <dd><a href=\"/wiki/Bidirectional_associative_memory\" title=\"Bidirectional associative memory\">Bidirectional associative memory</a></dd>\n",
       " <dd><a href=\"/wiki/Biological_neural_network\" title=\"Biological neural network\">Biological neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Boltzmann_machine\" title=\"Boltzmann machine\">Boltzmann machine</a></dd>\n",
       " <dd><a href=\"/wiki/Restricted_Boltzmann_machine\" title=\"Restricted Boltzmann machine\">Restricted Boltzmann machine</a></dd>\n",
       " <dd><a href=\"/wiki/Cellular_neural_network\" title=\"Cellular neural network\">Cellular neural network</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Cerebellar_Model_Articulation_Controller\" title=\"Cerebellar Model Articulation Controller\">Cerebellar Model Articulation Controller</a></dd>\n",
       " <dd><a href=\"/wiki/Committee_machine\" title=\"Committee machine\">Committee machine</a></dd>\n",
       " <dd><a href=\"/wiki/Competitive_learning\" title=\"Competitive learning\">Competitive learning</a></dd>\n",
       " <dd><a href=\"/wiki/Compositional_pattern-producing_network\" title=\"Compositional pattern-producing network\">Compositional pattern-producing network</a></dd>\n",
       " <dd><a href=\"/wiki/Computational_cybernetics\" title=\"Computational cybernetics\">Computational cybernetics</a></dd>\n",
       " <dd><a href=\"/wiki/Computational_neurogenetic_modeling\" title=\"Computational neurogenetic modeling\">Computational neurogenetic modeling</a></dd>\n",
       " <dd><a href=\"/wiki/Confabulation_(neural_networks)\" title=\"Confabulation (neural networks)\">Confabulation (neural networks)</a></dd>\n",
       " <dd><a href=\"/wiki/Cortical_column\" title=\"Cortical column\">Cortical column</a></dd>\n",
       " <dd><a class=\"new\" href=\"/w/index.php?title=Counterpropagation_network&amp;action=edit&amp;redlink=1\" title=\"Counterpropagation network (page does not exist)\">Counterpropagation network</a></dd>\n",
       " <dd><a href=\"/wiki/Cover%27s_theorem\" title=\"Cover's theorem\">Cover's theorem</a></dd>\n",
       " <dd><a href=\"/wiki/Cultured_neuronal_network\" title=\"Cultured neuronal network\">Cultured neuronal network</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Dehaene-Changeux_Model\" title=\"Dehaene-Changeux Model\">Dehaene-Changeux Model</a></dd>\n",
       " <dd><a href=\"/wiki/Delta_rule\" title=\"Delta rule\">Delta rule</a></dd>\n",
       " <dd><a href=\"/wiki/Early_stopping\" title=\"Early stopping\">Early stopping</a></dd>\n",
       " <dd><a href=\"/wiki/Echo_state_network\" title=\"Echo state network\">Echo state network</a></dd>\n",
       " <dd><a href=\"/wiki/The_Emotion_Machine\" title=\"The Emotion Machine\">The Emotion Machine</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Evolutionary_Acquisition_of_Neural_Topologies\" title=\"Evolutionary Acquisition of Neural Topologies\">Evolutionary Acquisition of Neural Topologies</a></dd>\n",
       " <dd><a href=\"/wiki/Extension_neural_network\" title=\"Extension neural network\">Extension neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Feed_forward_(control)\" title=\"Feed forward (control)\">Feed-forward</a></dd>\n",
       " <dd><a href=\"/wiki/Feedforward_neural_network\" title=\"Feedforward neural network\">Feedforward neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Generalized_Hebbian_Algorithm\" title=\"Generalized Hebbian Algorithm\">Generalized Hebbian Algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Generative_topographic_map\" title=\"Generative topographic map\">Generative topographic map</a></dd>\n",
       " <dd><a href=\"/wiki/Group_method_of_data_handling\" title=\"Group method of data handling\">Group method of data handling</a></dd>\n",
       " <dd><a href=\"/wiki/Growing_self-organizing_map\" title=\"Growing self-organizing map\">Growing self-organizing map</a></dd>\n",
       " <dd><a href=\"/wiki/Memory-prediction_framework\" title=\"Memory-prediction framework\">Memory-prediction framework</a></dd>\n",
       " <dd><a href=\"/wiki/Helmholtz_machine\" title=\"Helmholtz machine\">Helmholtz machine</a></dd>\n",
       " <dd><a href=\"/wiki/Hierarchical_temporal_memory\" title=\"Hierarchical temporal memory\">Hierarchical temporal memory</a></dd>\n",
       " <dd><a href=\"/wiki/Hopfield_network\" title=\"Hopfield network\">Hopfield network</a></dd>\n",
       " <dd><a href=\"/wiki/Hybrid_neural_network\" title=\"Hybrid neural network\">Hybrid neural network</a></dd>\n",
       " <dd><a href=\"/wiki/HyperNEAT\" title=\"HyperNEAT\">HyperNEAT</a></dd>\n",
       " <dd><a href=\"/wiki/Infomax\" title=\"Infomax\">Infomax</a></dd>\n",
       " <dd><a href=\"/wiki/Instantaneously_trained_neural_networks\" title=\"Instantaneously trained neural networks\">Instantaneously trained neural networks</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Interactive_Activation_and_Competition\" title=\"Interactive Activation and Competition\">Interactive Activation and Competition</a></dd>\n",
       " <dd><a href=\"/wiki/Leabra\" title=\"Leabra\">Leabra</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Learning_Vector_Quantization\" title=\"Learning Vector Quantization\">Learning Vector Quantization</a></dd>\n",
       " <dd><a href=\"/wiki/Lernmatrix\" title=\"Lernmatrix\">Lernmatrix</a></dd>\n",
       " <dd><a href=\"/wiki/Linde%E2%80%93Buzo%E2%80%93Gray_algorithm\" title=\"Linde–Buzo–Gray algorithm\">Linde–Buzo–Gray algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Liquid_state_machine\" title=\"Liquid state machine\">Liquid state machine</a></dd>\n",
       " <dd><a href=\"/wiki/Long_short-term_memory\" title=\"Long short-term memory\">Long short-term memory</a></dd>\n",
       " <dd><a class=\"mw-disambig\" href=\"/wiki/Madaline\" title=\"Madaline\">Madaline</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Modular_neural_networks\" title=\"Modular neural networks\">Modular neural networks</a></dd>\n",
       " <dd><a href=\"/wiki/MoneyBee\" title=\"MoneyBee\">MoneyBee</a></dd>\n",
       " <dd><a href=\"/wiki/Neocognitron\" title=\"Neocognitron\">Neocognitron</a></dd>\n",
       " <dd><a href=\"/wiki/Nervous_system_network_models\" title=\"Nervous system network models\">Nervous system network models</a></dd>\n",
       " <dd><a href=\"/wiki/NETtalk_(artificial_neural_network)\" title=\"NETtalk (artificial neural network)\">NETtalk (artificial neural network)</a></dd>\n",
       " <dd><a href=\"/wiki/Neural_backpropagation\" title=\"Neural backpropagation\">Neural backpropagation</a></dd>\n",
       " <dd><a href=\"/wiki/Neural_coding\" title=\"Neural coding\">Neural coding</a></dd>\n",
       " <dd><a href=\"/wiki/Neural_cryptography\" title=\"Neural cryptography\">Neural cryptography</a></dd>\n",
       " <dd><a href=\"/wiki/Neural_decoding\" title=\"Neural decoding\">Neural decoding</a></dd>\n",
       " <dd><a href=\"/wiki/Neural_gas\" title=\"Neural gas\">Neural gas</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Neural_Information_Processing_Systems\" title=\"Neural Information Processing Systems\">Neural Information Processing Systems</a></dd>\n",
       " <dd><a href=\"/wiki/Neural_modeling_fields\" title=\"Neural modeling fields\">Neural modeling fields</a></dd>\n",
       " <dd><a href=\"/wiki/Neural_oscillation\" title=\"Neural oscillation\">Neural oscillation</a></dd>\n",
       " <dd><a href=\"/wiki/Neurally_controlled_animat\" title=\"Neurally controlled animat\">Neurally controlled animat</a></dd>\n",
       " <dd><a href=\"/wiki/Neuroevolution_of_augmenting_topologies\" title=\"Neuroevolution of augmenting topologies\">Neuroevolution of augmenting topologies</a></dd>\n",
       " <dd><a href=\"/wiki/Neuroplasticity\" title=\"Neuroplasticity\">Neuroplasticity</a></dd>\n",
       " <dd><a href=\"/wiki/Ni1000\" title=\"Ni1000\">Ni1000</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Nonspiking_neurons\" title=\"Nonspiking neurons\">Nonspiking neurons</a></dd>\n",
       " <dd><a href=\"/wiki/Nonsynaptic_plasticity\" title=\"Nonsynaptic plasticity\">Nonsynaptic plasticity</a></dd>\n",
       " <dd><a href=\"/wiki/Oja%27s_rule\" title=\"Oja's rule\">Oja's rule</a></dd>\n",
       " <dd><a href=\"/wiki/Optical_neural_network\" title=\"Optical neural network\">Optical neural network</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Phase-of-firing_code\" title=\"Phase-of-firing code\">Phase-of-firing code</a></dd>\n",
       " <dd><a href=\"/wiki/Promoter_based_genetic_algorithm\" title=\"Promoter based genetic algorithm\">Promoter based genetic algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Pulse-coupled_networks\" title=\"Pulse-coupled networks\">Pulse-coupled networks</a></dd>\n",
       " <dd><a href=\"/wiki/Quantum_neural_network\" title=\"Quantum neural network\">Quantum neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Radial_basis_function\" title=\"Radial basis function\">Radial basis function</a></dd>\n",
       " <dd><a href=\"/wiki/Radial_basis_function_network\" title=\"Radial basis function network\">Radial basis function network</a></dd>\n",
       " <dd><a href=\"/wiki/Random_neural_network\" title=\"Random neural network\">Random neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Recurrent_neural_network\" title=\"Recurrent neural network\">Recurrent neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Reentry_(neural_circuitry)\" title=\"Reentry (neural circuitry)\">Reentry (neural circuitry)</a></dd>\n",
       " <dd><a href=\"/wiki/Reservoir_computing\" title=\"Reservoir computing\">Reservoir computing</a></dd>\n",
       " <dd><a href=\"/wiki/Rprop\" title=\"Rprop\">Rprop</a></dd>\n",
       " <dd><a href=\"/wiki/Semantic_neural_network\" title=\"Semantic neural network\">Semantic neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Sigmoid_function\" title=\"Sigmoid function\">Sigmoid function</a></dd>\n",
       " <dd><a class=\"mw-disambig\" href=\"/wiki/SNARC\" title=\"SNARC\">SNARC</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Softmax_activation_function\" title=\"Softmax activation function\">Softmax activation function</a></dd>\n",
       " <dd><a href=\"/wiki/Spiking_neural_network\" title=\"Spiking neural network\">Spiking neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Stochastic_neural_network\" title=\"Stochastic neural network\">Stochastic neural network</a></dd>\n",
       " <dd><a href=\"/wiki/Synaptic_plasticity\" title=\"Synaptic plasticity\">Synaptic plasticity</a></dd>\n",
       " <dd><a href=\"/wiki/Synaptic_weight\" title=\"Synaptic weight\">Synaptic weight</a></dd>\n",
       " <dd><a href=\"/wiki/Tensor_product_network\" title=\"Tensor product network\">Tensor product network</a></dd>\n",
       " <dd><a href=\"/wiki/Time_delay_neural_network\" title=\"Time delay neural network\">Time delay neural network</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/U-Matrix\" title=\"U-Matrix\">U-Matrix</a></dd>\n",
       " <dd><a href=\"/wiki/Universal_approximation_theorem\" title=\"Universal approximation theorem\">Universal approximation theorem</a></dd>\n",
       " <dd><a href=\"/wiki/Winner-take-all_(computing)\" title=\"Winner-take-all (computing)\">Winner-take-all</a></dd>\n",
       " <dd><a href=\"/wiki/Winnow_(algorithm)\" title=\"Winnow (algorithm)\">Winnow (algorithm)</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Reinforcement learning</dt>\n",
       " <dd><a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">Reinforcement learning</a></dd>\n",
       " <dd><a href=\"/wiki/Markov_decision_process\" title=\"Markov decision process\">Markov decision process</a></dd>\n",
       " <dd><a href=\"/wiki/Bellman_equation\" title=\"Bellman equation\">Bellman equation</a></dd>\n",
       " <dd><a href=\"/wiki/Q-learning\" title=\"Q-learning\">Q-learning</a></dd>\n",
       " <dd><a href=\"/wiki/Temporal_difference_learning\" title=\"Temporal difference learning\">Temporal difference learning</a></dd>\n",
       " <dd><a class=\"mw-redirect mw-disambig\" href=\"/wiki/SARSA\" title=\"SARSA\">SARSA</a></dd>\n",
       " <dd><a href=\"/wiki/Multi-armed_bandit\" title=\"Multi-armed bandit\">Multi-armed bandit</a></dd>\n",
       " <dd><a href=\"/wiki/Apprenticeship_learning\" title=\"Apprenticeship learning\">Apprenticeship learning</a></dd>\n",
       " <dd><a href=\"/wiki/Predictive_learning\" title=\"Predictive learning\">Predictive learning</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Text Mining</dt>\n",
       " <dd><a href=\"/wiki/Text_mining\" title=\"Text mining\">Text mining</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">Natural language processing</a></dd>\n",
       " <dd><a href=\"/wiki/Document_classification\" title=\"Document classification\">Document classification</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Bag_of_words_model\" title=\"Bag of words model\">Bag of words model</a></dd>\n",
       " <dd><a href=\"/wiki/N-gram\" title=\"N-gram\">N-gram</a></dd>\n",
       " <dd><a href=\"/wiki/Part-of-speech_tagging\" title=\"Part-of-speech tagging\">Part-of-speech tagging</a></dd>\n",
       " <dd><a href=\"/wiki/Sentiment_analysis\" title=\"Sentiment analysis\">Sentiment analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Information_extraction\" title=\"Information extraction\">Information extraction</a></dd>\n",
       " <dd><a href=\"/wiki/Topic_model\" title=\"Topic model\">Topic model</a></dd>\n",
       " <dd><a href=\"/wiki/Concept_mining\" title=\"Concept mining\">Concept mining</a></dd>\n",
       " <dd><a href=\"/wiki/Semantic_analysis_(machine_learning)\" title=\"Semantic analysis (machine learning)\">Semantic analysis (machine learning)</a></dd>\n",
       " <dd><a href=\"/wiki/Automatic_summarization\" title=\"Automatic summarization\">Automatic summarization</a></dd>\n",
       " <dd><a href=\"/wiki/String_kernel\" title=\"String kernel\">String kernel</a></dd>\n",
       " <dd><a href=\"/wiki/Biomedical_text_mining\" title=\"Biomedical text mining\">Biomedical text mining</a></dd>\n",
       " <dd><a href=\"/wiki/Never-Ending_Language_Learning\" title=\"Never-Ending Language Learning\">Never-Ending Language Learning</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Structure Mining</dt>\n",
       " <dd><a href=\"/wiki/Structure_mining\" title=\"Structure mining\">Structure mining</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Structured_learning\" title=\"Structured learning\">Structured learning</a></dd>\n",
       " <dd><a href=\"/wiki/Structured_prediction\" title=\"Structured prediction\">Structured prediction</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Sequence_mining\" title=\"Sequence mining\">Sequence mining</a></dd>\n",
       " <dd><a href=\"/wiki/Sequence_labeling\" title=\"Sequence labeling\">Sequence labeling</a></dd>\n",
       " <dd><a href=\"/wiki/Process_mining\" title=\"Process mining\">Process mining</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Advanced Learning Tasks</dt>\n",
       " <dd><a href=\"/wiki/Multi-label_classification\" title=\"Multi-label classification\">Multi-label classification</a></dd>\n",
       " <dd><a href=\"/wiki/Automated_machine_learning\" title=\"Automated machine learning\">Automated machine learning</a> (AutoML)</dd>\n",
       " <dd><a href=\"/wiki/Classifier_chains\" title=\"Classifier chains\">Classifier chains</a></dd>\n",
       " <dd><a href=\"/wiki/Web_mining\" title=\"Web mining\">Web mining</a></dd>\n",
       " <dd><a href=\"/wiki/Anomaly_detection\" title=\"Anomaly detection\">Anomaly detection</a></dd>\n",
       " <dd><a href=\"/wiki/Anomaly_Detection_at_Multiple_Scales\" title=\"Anomaly Detection at Multiple Scales\">Anomaly Detection at Multiple Scales</a></dd>\n",
       " <dd><a href=\"/wiki/Local_outlier_factor\" title=\"Local outlier factor\">Local outlier factor</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Novelty_detection\" title=\"Novelty detection\">Novelty detection</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/GSP_Algorithm\" title=\"GSP Algorithm\">GSP Algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Optimal_matching\" title=\"Optimal matching\">Optimal matching</a></dd>\n",
       " <dd><a href=\"/wiki/Record_linkage\" title=\"Record linkage\">Record linkage</a></dd>\n",
       " <dd><a href=\"/wiki/Meta_learning_(computer_science)\" title=\"Meta learning (computer science)\">Meta learning (computer science)</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Learning_automata\" title=\"Learning automata\">Learning automata</a></dd>\n",
       " <dd><a href=\"/wiki/Learning_to_rank\" title=\"Learning to rank\">Learning to rank</a></dd>\n",
       " <dd><a href=\"/wiki/Multiple-instance_learning\" title=\"Multiple-instance learning\">Multiple-instance learning</a></dd>\n",
       " <dd><a href=\"/wiki/Statistical_relational_learning\" title=\"Statistical relational learning\">Statistical relational learning</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Relational_classification\" title=\"Relational classification\">Relational classification</a></dd>\n",
       " <dd><a href=\"/wiki/Data_stream_mining\" title=\"Data stream mining\">Data stream mining</a></dd>\n",
       " <dd><a href=\"/wiki/Alpha_algorithm\" title=\"Alpha algorithm\">Alpha algorithm</a></dd>\n",
       " <dd><a href=\"/wiki/Syntactic_pattern_recognition\" title=\"Syntactic pattern recognition\">Syntactic pattern recognition</a></dd>\n",
       " <dd><a href=\"/wiki/Multispectral_pattern_recognition\" title=\"Multispectral pattern recognition\">Multispectral pattern recognition</a></dd>\n",
       " <dd><a href=\"/wiki/Algorithmic_learning_theory\" title=\"Algorithmic learning theory\">Algorithmic learning theory</a></dd>\n",
       " <dd><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a></dd>\n",
       " <dd><a href=\"/wiki/Bongard_problem\" title=\"Bongard problem\">Bongard problem</a></dd>\n",
       " <dd><a href=\"/wiki/Learning_with_errors\" title=\"Learning with errors\">Learning with errors</a></dd>\n",
       " <dd><a href=\"/wiki/Parity_learning\" title=\"Parity learning\">Parity learning</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Inductive_transfer\" title=\"Inductive transfer\">Inductive transfer</a></dd>\n",
       " <dd><a href=\"/wiki/Granular_computing\" title=\"Granular computing\">Granular computing</a></dd>\n",
       " <dd><a href=\"/wiki/Conceptual_clustering\" title=\"Conceptual clustering\">Conceptual clustering</a></dd>\n",
       " <dd><a href=\"/wiki/Formal_concept_analysis\" title=\"Formal concept analysis\">Formal concept analysis</a></dd>\n",
       " <dd><a href=\"/wiki/Biclustering\" title=\"Biclustering\">Biclustering</a></dd>\n",
       " <dd><a href=\"/wiki/Information_visualization\" title=\"Information visualization\">Information visualization</a></dd>\n",
       " <dd><a href=\"/wiki/Co-occurrence_networks\" title=\"Co-occurrence networks\">Co-occurrence networks</a></dd>\n",
       " </dl>, <dl>\n",
       " <dt>Applications</dt>\n",
       " <dd><a href=\"/wiki/Problem_domain\" title=\"Problem domain\">Problem domain</a></dd>\n",
       " <dd><a href=\"/wiki/Recommender_system\" title=\"Recommender system\">Recommender system</a></dd>\n",
       " <dd><a href=\"/wiki/Collaborative_filtering\" title=\"Collaborative filtering\">Collaborative filtering</a></dd>\n",
       " <dd><a href=\"/wiki/Profiling_(information_science)\" title=\"Profiling (information science)\">Profiling (information science)</a></dd>\n",
       " <dd><a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">Speech recognition</a></dd>\n",
       " <dd><a class=\"mw-disambig\" href=\"/wiki/Stock_forecast\" title=\"Stock forecast\">Stock forecast</a></dd>\n",
       " <dd><a href=\"/wiki/Activity_recognition\" title=\"Activity recognition\">Activity recognition</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Data_Analysis_Techniques_for_Fraud_Detection\" title=\"Data Analysis Techniques for Fraud Detection\">Data Analysis Techniques for Fraud Detection</a></dd>\n",
       " <dd><a href=\"/wiki/Molecule_mining\" title=\"Molecule mining\">Molecule mining</a></dd>\n",
       " <dd><a href=\"/wiki/Behavioral_targeting\" title=\"Behavioral targeting\">Behavioral targeting</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Proactive_Discovery_of_Insider_Threats_Using_Graph_Analysis_and_Learning\" title=\"Proactive Discovery of Insider Threats Using Graph Analysis and Learning\">Proactive Discovery of Insider Threats Using Graph Analysis and Learning</a></dd>\n",
       " <dd><a href=\"/wiki/Robot_learning\" title=\"Robot learning\">Robot learning</a></dd>\n",
       " <dd><a href=\"/wiki/Computer_vision\" title=\"Computer vision\">Computer vision</a></dd>\n",
       " <dd><a href=\"/wiki/Facial_recognition_system\" title=\"Facial recognition system\">Facial recognition system</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Outlier_detection\" title=\"Outlier detection\">Outlier detection</a></dd>\n",
       " <dd><a href=\"/wiki/Anomaly_detection\" title=\"Anomaly detection\">Anomaly detection</a></dd>\n",
       " <dd><a class=\"mw-redirect\" href=\"/wiki/Novelty_detection\" title=\"Novelty detection\">Novelty detection</a></dd>\n",
       " </dl>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_link = first.select_one('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': '/wiki/Machine_learning', 'title': 'Machine learning'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_link.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sec(sec):\n",
    "    parsed_dds = []\n",
    "    dt = sec.select_one('dt')\n",
    "    if dt is not None:\n",
    "        sec_title = dt.text\n",
    "        dds = sec.select('dd')\n",
    "        if dds is not None:\n",
    "            parsed_dds = [parse_dd(dd, sec_title) for dd in dds]\n",
    "    if len(parsed_dds) > 0:\n",
    "        return parsed_dds\n",
    "\n",
    "def parse_dd(dd, sec_title):\n",
    "    link = dd.select_one('a')\n",
    "    if link is not None:\n",
    "        attrs = link.attrs\n",
    "        link_dict = {\n",
    "            'title': attrs['title'],\n",
    "            'href': attrs['href'],\n",
    "            'section': sec_title\n",
    "        }\n",
    "        return link_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_links = []\n",
    "for sec in sections:\n",
    "    parsed_content = parse_sec(sec)\n",
    "    if parsed_content is not None:\n",
    "        parsed_links.extend(parsed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'href': '/wiki/Machine_learning',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Machine learning'},\n",
       " {'href': '/wiki/Data_analysis',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Data analysis'},\n",
       " {'href': '/wiki/Occam%27s_razor',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': \"Occam's razor\"},\n",
       " {'href': '/wiki/Curse_of_dimensionality',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Curse of dimensionality'},\n",
       " {'href': '/wiki/No_free_lunch_theorem',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'No free lunch theorem'},\n",
       " {'href': '/wiki/Accuracy_paradox',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Accuracy paradox'},\n",
       " {'href': '/wiki/Overfitting',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Overfitting'},\n",
       " {'href': '/wiki/Regularization_(machine_learning)',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Regularization (machine learning)'},\n",
       " {'href': '/wiki/Inductive_bias',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Inductive bias'},\n",
       " {'href': '/wiki/Data_dredging',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Data dredging'},\n",
       " {'href': '/wiki/Ugly_duckling_theorem',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Ugly duckling theorem'},\n",
       " {'href': '/wiki/Uncertain_data',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Uncertain data'},\n",
       " {'href': '/wiki/Knowledge_discovery',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Knowledge discovery'},\n",
       " {'href': '/wiki/Data_mining',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Data mining'},\n",
       " {'href': '/wiki/Predictive_analytics',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Predictive analytics'},\n",
       " {'href': '/wiki/Predictive_modelling',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Predictive modelling'},\n",
       " {'href': '/wiki/Business_intelligence',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Business intelligence'},\n",
       " {'href': '/wiki/Reactive_business_intelligence',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Reactive business intelligence'},\n",
       " {'href': '/wiki/Business_analytics',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Business analytics'},\n",
       " {'href': '/wiki/Reactive_business_intelligence',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Reactive business intelligence'},\n",
       " {'href': '/wiki/Pattern_recognition',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Pattern recognition'},\n",
       " {'href': '/wiki/Abductive_reasoning',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Abductive reasoning'},\n",
       " {'href': '/wiki/Inductive_reasoning',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Inductive reasoning'},\n",
       " {'href': '/wiki/First-order_logic',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'First-order logic'},\n",
       " {'href': '/wiki/Inductive_logic_programming',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Inductive logic programming'},\n",
       " {'href': '/wiki/Reasoning_system',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Reasoning system'},\n",
       " {'href': '/wiki/Case-based_reasoning',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Case-based reasoning'},\n",
       " {'href': '/wiki/Textual_case_based_reasoning',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Textual case based reasoning'},\n",
       " {'href': '/wiki/Causality', 'section': 'Reasoning', 'title': 'Causality'},\n",
       " {'href': '/wiki/Nearest_neighbor_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Nearest neighbor search'},\n",
       " {'href': '/wiki/Stochastic_gradient_descent',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Stochastic gradient descent'},\n",
       " {'href': '/wiki/Beam_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Beam search'},\n",
       " {'href': '/wiki/Best-first_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Best-first search'},\n",
       " {'href': '/wiki/Breadth-first_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Breadth-first search'},\n",
       " {'href': '/wiki/Hill_climbing',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Hill climbing'},\n",
       " {'href': '/wiki/Grid_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Grid search'},\n",
       " {'href': '/wiki/Brute-force_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Brute-force search'},\n",
       " {'href': '/wiki/Depth-first_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Depth-first search'},\n",
       " {'href': '/wiki/Tabu_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Tabu search'},\n",
       " {'href': '/wiki/Anytime_algorithm',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Anytime algorithm'},\n",
       " {'href': '/wiki/Exploratory_data_analysis',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Exploratory data analysis'},\n",
       " {'href': '/wiki/Covariate', 'section': 'Statistics', 'title': 'Covariate'},\n",
       " {'href': '/wiki/Statistical_inference',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Statistical inference'},\n",
       " {'href': '/wiki/Algorithmic_inference',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Algorithmic inference'},\n",
       " {'href': '/wiki/Bayesian_inference',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Bayesian inference'},\n",
       " {'href': '/wiki/Base_rate', 'section': 'Statistics', 'title': 'Base rate'},\n",
       " {'href': '/wiki/Bias_(statistics)',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Bias (statistics)'},\n",
       " {'href': '/wiki/Gibbs_sampling',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Gibbs sampling'},\n",
       " {'href': '/wiki/Cross-entropy_method',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Cross-entropy method'},\n",
       " {'href': '/wiki/Latent_variable',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Latent variable'},\n",
       " {'href': '/wiki/Maximum_likelihood',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Maximum likelihood'},\n",
       " {'href': '/wiki/Maximum_a_posteriori_estimation',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Maximum a posteriori estimation'},\n",
       " {'href': '/wiki/Expectation%E2%80%93maximization_algorithm',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Expectation–maximization algorithm'},\n",
       " {'href': '/wiki/Expectation_propagation',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Expectation propagation'},\n",
       " {'href': '/wiki/Kullback%E2%80%93Leibler_divergence',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Kullback–Leibler divergence'},\n",
       " {'href': '/wiki/Generative_model',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Generative model'},\n",
       " {'href': '/wiki/Supervised_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Supervised learning'},\n",
       " {'href': '/wiki/Unsupervised_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Unsupervised learning'},\n",
       " {'href': '/wiki/Active_learning_(machine_learning)',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Active learning (machine learning)'},\n",
       " {'href': '/wiki/Reinforcement_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Reinforcement learning'},\n",
       " {'href': '/wiki/Multi-task_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Multi-task learning'},\n",
       " {'href': '/wiki/Transduction_(machine_learning)',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Transduction (machine learning)'},\n",
       " {'href': '/wiki/Explanation-based_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Explanation-based learning'},\n",
       " {'href': '/wiki/Offline_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Offline learning'},\n",
       " {'href': '/wiki/Online_learning_model',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Online learning model'},\n",
       " {'href': '/wiki/Online_machine_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Online machine learning'},\n",
       " {'href': '/wiki/Hyperparameter_optimization',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Hyperparameter optimization'},\n",
       " {'href': '/wiki/Classification_in_machine_learning',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Classification in machine learning'},\n",
       " {'href': '/wiki/Concept_class',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Concept class'},\n",
       " {'href': '/wiki/Features_(pattern_recognition)',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Features (pattern recognition)'},\n",
       " {'href': '/wiki/Feature_vector',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Feature vector'},\n",
       " {'href': '/wiki/Feature_space',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Feature space'},\n",
       " {'href': '/wiki/Concept_learning',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Concept learning'},\n",
       " {'href': '/wiki/Binary_classification',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Binary classification'},\n",
       " {'href': '/wiki/Decision_boundary',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Decision boundary'},\n",
       " {'href': '/wiki/Multiclass_classification',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Multiclass classification'},\n",
       " {'href': '/wiki/Class_membership_probabilities',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Class membership probabilities'},\n",
       " {'href': '/wiki/Calibration_(statistics)',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Calibration (statistics)'},\n",
       " {'href': '/wiki/Concept_drift',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Concept drift'},\n",
       " {'href': '/wiki/Prior_knowledge_for_pattern_recognition',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Prior knowledge for pattern recognition'},\n",
       " {'href': '/wiki/Iris_flower_data_set',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Iris flower data set'},\n",
       " {'href': '/wiki/Margin_Infused_Relaxed_Algorithm',\n",
       "  'section': 'Online Learning',\n",
       "  'title': 'Margin Infused Relaxed Algorithm'},\n",
       " {'href': '/wiki/Semi-supervised_learning',\n",
       "  'section': 'Semi-supervised learning',\n",
       "  'title': 'Semi-supervised learning'},\n",
       " {'href': '/wiki/One-class_classification',\n",
       "  'section': 'Semi-supervised learning',\n",
       "  'title': 'One-class classification'},\n",
       " {'href': '/wiki/Coupled_pattern_learner',\n",
       "  'section': 'Semi-supervised learning',\n",
       "  'title': 'Coupled pattern learner'},\n",
       " {'href': '/wiki/Lazy_learning',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Lazy learning'},\n",
       " {'href': '/wiki/Eager_learning',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Eager learning'},\n",
       " {'href': '/wiki/Instance-based_learning',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Instance-based learning'},\n",
       " {'href': '/wiki/Cluster_assumption',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Cluster assumption'},\n",
       " {'href': '/wiki/K-nearest_neighbor_algorithm',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'K-nearest neighbor algorithm'},\n",
       " {'href': '/wiki/IDistance',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'IDistance'},\n",
       " {'href': '/wiki/Large_margin_nearest_neighbor',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Large margin nearest neighbor'},\n",
       " {'href': '/wiki/Decision_tree_learning',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Decision tree learning'},\n",
       " {'href': '/wiki/Decision_stump',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Decision stump'},\n",
       " {'href': '/wiki/Pruning_(decision_trees)',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Pruning (decision trees)'},\n",
       " {'href': '/wiki/Mutual_information',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Mutual information'},\n",
       " {'href': '/wiki/Adjusted_mutual_information',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Adjusted mutual information'},\n",
       " {'href': '/wiki/Information_gain_ratio',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Information gain ratio'},\n",
       " {'href': '/wiki/Information_gain_in_decision_trees',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Information gain in decision trees'},\n",
       " {'href': '/wiki/ID3_algorithm',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'ID3 algorithm'},\n",
       " {'href': '/wiki/C4.5_algorithm',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'C4.5 algorithm'},\n",
       " {'href': '/wiki/CHAID', 'section': 'Decision Trees', 'title': 'CHAID'},\n",
       " {'href': '/wiki/Information_Fuzzy_Networks',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Information Fuzzy Networks'},\n",
       " {'href': '/wiki/Grafting_(decision_trees)',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Grafting (decision trees)'},\n",
       " {'href': '/wiki/Incremental_decision_tree',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Incremental decision tree'},\n",
       " {'href': '/wiki/Alternating_decision_tree',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Alternating decision tree'},\n",
       " {'href': '/wiki/Logistic_model_tree',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Logistic model tree'},\n",
       " {'href': '/wiki/Random_forest',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Random forest'},\n",
       " {'href': '/wiki/Linear_classifier',\n",
       "  'section': 'Linear Classifiers',\n",
       "  'title': 'Linear classifier'},\n",
       " {'href': '/wiki/Margin_(machine_learning)',\n",
       "  'section': 'Linear Classifiers',\n",
       "  'title': 'Margin (machine learning)'},\n",
       " {'href': '/wiki/Margin_classifier',\n",
       "  'section': 'Linear Classifiers',\n",
       "  'title': 'Margin classifier'},\n",
       " {'href': '/wiki/Soft_independent_modelling_of_class_analogies',\n",
       "  'section': 'Linear Classifiers',\n",
       "  'title': 'Soft independent modelling of class analogies'},\n",
       " {'href': '/wiki/Statistical_classification',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Statistical classification'},\n",
       " {'href': '/wiki/Probability_matching',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Probability matching'},\n",
       " {'href': '/wiki/Discriminative_model',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Discriminative model'},\n",
       " {'href': '/wiki/Linear_discriminant_analysis',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Linear discriminant analysis'},\n",
       " {'href': '/wiki/Multiclass_LDA',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Multiclass LDA'},\n",
       " {'href': '/wiki/Multiple_discriminant_analysis',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Multiple discriminant analysis'},\n",
       " {'href': '/wiki/Optimal_discriminant_analysis',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Optimal discriminant analysis'},\n",
       " {'href': '/wiki/Fisher_kernel',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Fisher kernel'},\n",
       " {'href': '/wiki/Discriminant_function_analysis',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Discriminant function analysis'},\n",
       " {'href': '/wiki/Multilinear_subspace_learning',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Multilinear subspace learning'},\n",
       " {'href': '/wiki/Quadratic_classifier',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Quadratic classifier'},\n",
       " {'href': '/wiki/Variable_kernel_density_estimation',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Variable kernel density estimation'},\n",
       " {'href': '/wiki/Category_utility',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Category utility'},\n",
       " {'href': '/wiki/Data_classification_(business_intelligence)',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Data classification (business intelligence)'},\n",
       " {'href': '/wiki/Training_set',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Training set'},\n",
       " {'href': '/wiki/Test_set',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Test set'},\n",
       " {'href': '/wiki/Synthetic_data',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Synthetic data'},\n",
       " {'href': '/wiki/Cross-validation_(statistics)',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Cross-validation (statistics)'},\n",
       " {'href': '/wiki/Loss_function',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Loss function'},\n",
       " {'href': '/wiki/Hinge_loss',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Hinge loss'},\n",
       " {'href': '/wiki/Generalization_error',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Generalization error'},\n",
       " {'href': '/wiki/Type_I_and_type_II_errors',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Type I and type II errors'},\n",
       " {'href': '/wiki/Sensitivity_and_specificity',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Sensitivity and specificity'},\n",
       " {'href': '/wiki/Precision_and_recall',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Precision and recall'},\n",
       " {'href': '/wiki/F1_score',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'F1 score'},\n",
       " {'href': '/wiki/Confusion_matrix',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Confusion matrix'},\n",
       " {'href': '/wiki/Matthews_correlation_coefficient',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Matthews correlation coefficient'},\n",
       " {'href': '/wiki/Receiver_operating_characteristic',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Receiver operating characteristic'},\n",
       " {'href': '/wiki/Lift_(data_mining)',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Lift (data mining)'},\n",
       " {'href': '/wiki/Stability_in_learning',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Stability in learning'},\n",
       " {'href': '/wiki/Data_Pre-processing',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Data Pre-processing'},\n",
       " {'href': '/wiki/Discretization_of_continuous_features',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Discretization of continuous features'},\n",
       " {'href': '/wiki/Feature_engineering',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Feature engineering'},\n",
       " {'href': '/wiki/Feature_selection',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Feature selection'},\n",
       " {'href': '/wiki/Feature_extraction',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Feature extraction'},\n",
       " {'href': '/wiki/Dimension_reduction',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Dimension reduction'},\n",
       " {'href': '/wiki/Principal_component_analysis',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Principal component analysis'},\n",
       " {'href': '/wiki/Multilinear_principal-component_analysis',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Multilinear principal-component analysis'},\n",
       " {'href': '/wiki/Multifactor_dimensionality_reduction',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Multifactor dimensionality reduction'},\n",
       " {'href': '/wiki/Targeted_projection_pursuit',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Targeted projection pursuit'},\n",
       " {'href': '/wiki/Multidimensional_scaling',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Multidimensional scaling'},\n",
       " {'href': '/wiki/Nonlinear_dimensionality_reduction',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Nonlinear dimensionality reduction'},\n",
       " {'href': '/wiki/Kernel_principal_component_analysis',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Kernel principal component analysis'},\n",
       " {'href': '/wiki/Kernel_eigenvoice',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Kernel eigenvoice'},\n",
       " {'href': '/wiki/Gramian_matrix',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Gramian matrix'},\n",
       " {'href': '/wiki/Gaussian_process',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Gaussian process'},\n",
       " {'href': '/wiki/Kernel_adaptive_filter',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Kernel adaptive filter'},\n",
       " {'href': '/wiki/Isomap',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Isomap'},\n",
       " {'href': '/wiki/Manifold_alignment',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Manifold alignment'},\n",
       " {'href': '/wiki/Diffusion_map',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Diffusion map'},\n",
       " {'href': '/wiki/Elastic_map',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Elastic map'},\n",
       " {'href': '/wiki/Locality-sensitive_hashing',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Locality-sensitive hashing'},\n",
       " {'href': '/wiki/Spectral_clustering',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Spectral clustering'},\n",
       " {'href': '/wiki/Minimum_redundancy_feature_selection',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Minimum redundancy feature selection'},\n",
       " {'href': '/wiki/Cluster_analysis',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Cluster analysis'},\n",
       " {'href': '/wiki/K-means_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'K-means clustering'},\n",
       " {'href': '/wiki/K-means%2B%2B',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'K-means++'},\n",
       " {'href': '/wiki/K-medians_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'K-medians clustering'},\n",
       " {'href': '/wiki/K-medoids', 'section': 'Clustering', 'title': 'K-medoids'},\n",
       " {'href': '/wiki/DBSCAN', 'section': 'Clustering', 'title': 'DBSCAN'},\n",
       " {'href': '/wiki/Fuzzy_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Fuzzy clustering'},\n",
       " {'href': '/wiki/BIRCH_(data_clustering)',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'BIRCH (data clustering)'},\n",
       " {'href': '/wiki/Canopy_clustering_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Canopy clustering algorithm'},\n",
       " {'href': '/wiki/Cluster-weighted_modeling',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Cluster-weighted modeling'},\n",
       " {'href': '/wiki/Clustering_high-dimensional_data',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Clustering high-dimensional data'},\n",
       " {'href': '/wiki/Cobweb_(clustering)',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Cobweb (clustering)'},\n",
       " {'href': '/wiki/Complete-linkage_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Complete-linkage clustering'},\n",
       " {'href': '/wiki/Constrained_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Constrained clustering'},\n",
       " {'href': '/wiki/Correlation_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Correlation clustering'},\n",
       " {'href': '/wiki/CURE_data_clustering_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'CURE data clustering algorithm'},\n",
       " {'href': '/wiki/Data_stream_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Data stream clustering'},\n",
       " {'href': '/wiki/Dendrogram', 'section': 'Clustering', 'title': 'Dendrogram'},\n",
       " {'href': '/wiki/Determining_the_number_of_clusters_in_a_data_set',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Determining the number of clusters in a data set'},\n",
       " {'href': '/wiki/FLAME_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'FLAME clustering'},\n",
       " {'href': '/wiki/Hierarchical_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Hierarchical clustering'},\n",
       " {'href': '/wiki/Information_bottleneck_method',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Information bottleneck method'},\n",
       " {'href': '/wiki/Lloyd%27s_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': \"Lloyd's algorithm\"},\n",
       " {'href': '/wiki/Nearest-neighbor_chain_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Nearest-neighbor chain algorithm'},\n",
       " {'href': '/wiki/Neighbor_joining',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Neighbor joining'},\n",
       " {'href': '/wiki/OPTICS_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'OPTICS algorithm'},\n",
       " {'href': '/wiki/Pitman%E2%80%93Yor_process',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Pitman–Yor process'},\n",
       " {'href': '/wiki/Single-linkage_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Single-linkage clustering'},\n",
       " {'href': '/wiki/SUBCLU', 'section': 'Clustering', 'title': 'SUBCLU'},\n",
       " {'href': '/wiki/Thresholding_(image_processing)',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Thresholding (image processing)'},\n",
       " {'href': '/wiki/UPGMA', 'section': 'Clustering', 'title': 'UPGMA'},\n",
       " {'href': '/wiki/Rand_index',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'Rand index'},\n",
       " {'href': '/wiki/Dunn_index',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'Dunn index'},\n",
       " {'href': '/wiki/Davies%E2%80%93Bouldin_index',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'Davies–Bouldin index'},\n",
       " {'href': '/wiki/Jaccard_index',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'Jaccard index'},\n",
       " {'href': '/wiki/MinHash',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'MinHash'},\n",
       " {'href': '/wiki/K_q-flats',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'K q-flats'},\n",
       " {'href': '/wiki/Decision_rules',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'Decision rules'},\n",
       " {'href': '/wiki/Rule_induction',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'Rule induction'},\n",
       " {'href': '/wiki/Classification_rule',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'Classification rule'},\n",
       " {'href': '/wiki/CN2_algorithm',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'CN2 algorithm'},\n",
       " {'href': '/wiki/Decision_list',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'Decision list'},\n",
       " {'href': '/wiki/First_Order_Inductive_Learner',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'First Order Inductive Learner'},\n",
       " {'href': '/wiki/Association_rule_learning',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'Association rule learning'},\n",
       " {'href': '/wiki/Apriori_algorithm',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'Apriori algorithm'},\n",
       " {'href': '/wiki/Contrast_set_learning',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'Contrast set learning'},\n",
       " {'href': '/wiki/Affinity_analysis',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'Affinity analysis'},\n",
       " {'href': '/wiki/K-optimal_pattern_discovery',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'K-optimal pattern discovery'},\n",
       " {'href': '/wiki/Ensemble_learning',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Ensemble learning'},\n",
       " {'href': '/wiki/Ensemble_averaging',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Ensemble averaging'},\n",
       " {'href': '/wiki/Consensus_clustering',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Consensus clustering'},\n",
       " {'href': '/wiki/AdaBoost',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'AdaBoost'},\n",
       " {'href': '/wiki/Boosting',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Boosting'},\n",
       " {'href': '/wiki/Bootstrap_aggregating',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Bootstrap aggregating'},\n",
       " {'href': '/wiki/BrownBoost',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'BrownBoost'},\n",
       " {'href': '/wiki/Cascading_classifiers',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Cascading classifiers'},\n",
       " {'href': '/wiki/Co-training',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Co-training'},\n",
       " {'href': '/wiki/CoBoosting',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'CoBoosting'},\n",
       " {'href': '/wiki/Gaussian_process_emulator',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Gaussian process emulator'},\n",
       " {'href': '/wiki/Gradient_boosting',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Gradient boosting'},\n",
       " {'href': '/wiki/LogitBoost',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'LogitBoost'},\n",
       " {'href': '/wiki/LPBoost', 'section': 'Ensemble Learning', 'title': 'LPBoost'},\n",
       " {'href': '/wiki/Mixture_model',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Mixture model'},\n",
       " {'href': '/wiki/Product_of_Experts',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Product of Experts'},\n",
       " {'href': '/wiki/Random_multinomial_logit',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Random multinomial logit'},\n",
       " {'href': '/wiki/Random_subspace_method',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Random subspace method'},\n",
       " {'href': '/wiki/Weighted_Majority_Algorithm',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Weighted Majority Algorithm'},\n",
       " {'href': '/wiki/Randomized_weighted_majority_algorithm',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Randomized weighted majority algorithm'},\n",
       " {'href': '/wiki/Graphical_model',\n",
       "  'section': 'Graphical Models',\n",
       "  'title': 'Graphical model'},\n",
       " {'href': '/wiki/State_transition_network',\n",
       "  'section': 'Graphical Models',\n",
       "  'title': 'State transition network'},\n",
       " {'href': '/wiki/Naive_Bayes_classifier',\n",
       "  'section': 'Bayesian Learning Methods',\n",
       "  'title': 'Naive Bayes classifier'},\n",
       " {'href': '/wiki/Averaged_one-dependence_estimators',\n",
       "  'section': 'Bayesian Learning Methods',\n",
       "  'title': 'Averaged one-dependence estimators'},\n",
       " {'href': '/wiki/Bayesian_network',\n",
       "  'section': 'Bayesian Learning Methods',\n",
       "  'title': 'Bayesian network'},\n",
       " {'href': '/wiki/Variational_message_passing',\n",
       "  'section': 'Bayesian Learning Methods',\n",
       "  'title': 'Variational message passing'},\n",
       " {'href': '/wiki/Markov_model',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Markov model'},\n",
       " {'href': '/wiki/Maximum-entropy_Markov_model',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Maximum-entropy Markov model'},\n",
       " {'href': '/wiki/Hidden_Markov_model',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Hidden Markov model'},\n",
       " {'href': '/wiki/Baum%E2%80%93Welch_algorithm',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Baum–Welch algorithm'},\n",
       " {'href': '/wiki/Forward%E2%80%93backward_algorithm',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Forward–backward algorithm'},\n",
       " {'href': '/wiki/Hierarchical_hidden_Markov_model',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Hierarchical hidden Markov model'},\n",
       " {'href': '/wiki/Markov_logic_network',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Markov logic network'},\n",
       " {'href': '/wiki/Markov_chain_Monte_Carlo',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Markov chain Monte Carlo'},\n",
       " {'href': '/wiki/Markov_random_field',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Markov random field'},\n",
       " {'href': '/wiki/Conditional_random_field',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Conditional random field'},\n",
       " {'href': '/wiki/Predictive_state_representation',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Predictive state representation'},\n",
       " {'href': '/wiki/Computational_learning_theory',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Computational learning theory'},\n",
       " {'href': '/wiki/Version_space',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Version space'},\n",
       " {'href': '/wiki/Probably_approximately_correct_learning',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Probably approximately correct learning'},\n",
       " {'href': '/wiki/Vapnik%E2%80%93Chervonenkis_theory',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Vapnik–Chervonenkis theory'},\n",
       " {'href': '/wiki/Shattering_(machine_learning)',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Shattering (machine learning)'},\n",
       " {'href': '/wiki/VC_dimension',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'VC dimension'},\n",
       " {'href': '/wiki/Minimum_description_length',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Minimum description length'},\n",
       " {'href': '/wiki/Bondy%27s_theorem',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': \"Bondy's theorem\"},\n",
       " {'href': '/wiki/Inferential_theory_of_learning',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Inferential theory of learning'},\n",
       " {'href': '/wiki/Rademacher_complexity',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Rademacher complexity'},\n",
       " {'href': '/wiki/Teaching_dimension',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Teaching dimension'},\n",
       " {'href': '/wiki/Subclass_reachability',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Subclass reachability'},\n",
       " {'href': '/wiki/Sample_exclusion_dimension',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Sample exclusion dimension'},\n",
       " {'href': '/wiki/Unique_negative_dimension',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Unique negative dimension'},\n",
       " {'href': '/wiki/Uniform_convergence_(combinatorics)',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Uniform convergence (combinatorics)'},\n",
       " {'href': '/wiki/Witness_set',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Witness set'},\n",
       " {'href': '/wiki/Kernel_methods',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Kernel methods'},\n",
       " {'href': '/wiki/Support_vector_machine',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Support vector machine'},\n",
       " {'href': '/wiki/Structural_risk_minimization',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Structural risk minimization'},\n",
       " {'href': '/wiki/Empirical_risk_minimization',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Empirical risk minimization'},\n",
       " {'href': '/wiki/Kernel_trick',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Kernel trick'},\n",
       " {'href': '/wiki/Least_squares_support_vector_machine',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Least squares support vector machine'},\n",
       " {'href': '/wiki/Relevance_vector_machine',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Relevance vector machine'},\n",
       " {'href': '/wiki/Sequential_minimal_optimization',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Sequential minimal optimization'},\n",
       " {'href': '/wiki/Structured_SVM',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Structured SVM'},\n",
       " {'href': '/wiki/Outline_of_regression_analysis',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Outline of regression analysis'},\n",
       " {'href': '/wiki/Regression_analysis',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Regression analysis'},\n",
       " {'href': '/wiki/Dependent_and_independent_variables',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Dependent and independent variables'},\n",
       " {'href': '/wiki/Linear_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Linear model'},\n",
       " {'href': '/wiki/Linear_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Linear regression'},\n",
       " {'href': '/wiki/Least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Least squares'},\n",
       " {'href': '/wiki/Linear_least_squares_(mathematics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Linear least squares (mathematics)'},\n",
       " {'href': '/wiki/Local_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Local regression'},\n",
       " {'href': '/wiki/Additive_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Additive model'},\n",
       " {'href': '/wiki/Antecedent_variable',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Antecedent variable'},\n",
       " {'href': '/wiki/Autocorrelation',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Autocorrelation'},\n",
       " {'href': '/wiki/Backfitting_algorithm',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Backfitting algorithm'},\n",
       " {'href': '/wiki/Bayesian_linear_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Bayesian linear regression'},\n",
       " {'href': '/wiki/Bayesian_multivariate_linear_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Bayesian multivariate linear regression'},\n",
       " {'href': '/wiki/Binomial_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Binomial regression'},\n",
       " {'href': '/wiki/Canonical_analysis',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Canonical analysis'},\n",
       " {'href': '/wiki/Censored_regression_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Censored regression model'},\n",
       " {'href': '/wiki/Coefficient_of_determination',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Coefficient of determination'},\n",
       " {'href': '/wiki/Comparison_of_general_and_generalized_linear_models',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Comparison of general and generalized linear models'},\n",
       " {'href': '/wiki/Compressed_sensing',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Compressed sensing'},\n",
       " {'href': '/wiki/Conditional_change_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Conditional change model'},\n",
       " {'href': '/wiki/Controlling_for_a_variable',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Controlling for a variable'},\n",
       " {'href': '/wiki/Cross-sectional_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Cross-sectional regression'},\n",
       " {'href': '/wiki/Curve_fitting',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Curve fitting'},\n",
       " {'href': '/wiki/Deming_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Deming regression'},\n",
       " {'href': '/wiki/Design_matrix',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Design matrix'},\n",
       " {'href': '/wiki/Difference_in_differences',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Difference in differences'},\n",
       " {'href': '/wiki/Dummy_variable_(statistics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Dummy variable (statistics)'},\n",
       " {'href': '/wiki/Errors_and_residuals_in_statistics',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Errors and residuals in statistics'},\n",
       " {'href': '/wiki/Errors-in-variables_models',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Errors-in-variables models'},\n",
       " {'href': '/wiki/Explained_sum_of_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Explained sum of squares'},\n",
       " {'href': '/wiki/Explained_variation',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Explained variation'},\n",
       " {'href': '/wiki/First-hitting-time_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'First-hitting-time model'},\n",
       " {'href': '/wiki/Fixed_effects_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Fixed effects model'},\n",
       " {'href': '/wiki/Fraction_of_variance_unexplained',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Fraction of variance unexplained'},\n",
       " {'href': '/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Frisch–Waugh–Lovell theorem'},\n",
       " {'href': '/wiki/General_linear_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'General linear model'},\n",
       " {'href': '/wiki/Generalized_additive_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized additive model'},\n",
       " {'href': '/wiki/Generalized_additive_model_for_location,_scale_and_shape',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized additive model for location, scale and shape'},\n",
       " {'href': '/wiki/Generalized_estimating_equation',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized estimating equation'},\n",
       " {'href': '/wiki/Generalized_least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized least squares'},\n",
       " {'href': '/wiki/Generalized_linear_array_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized linear array model'},\n",
       " {'href': '/wiki/Generalized_linear_mixed_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized linear mixed model'},\n",
       " {'href': '/wiki/Generalized_linear_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized linear model'},\n",
       " {'href': '/wiki/Growth_curve',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Growth curve'},\n",
       " {'href': '/wiki/Guess_value',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Guess value'},\n",
       " {'href': '/wiki/Hat_matrix',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Hat matrix'},\n",
       " {'href': '/wiki/Heckman_correction',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Heckman correction'},\n",
       " {'href': '/wiki/Heteroscedasticity-consistent_standard_errors',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Heteroscedasticity-consistent standard errors'},\n",
       " {'href': '/wiki/Hosmer%E2%80%93Lemeshow_test',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Hosmer–Lemeshow test'},\n",
       " {'href': '/wiki/Instrumental_variable',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Instrumental variable'},\n",
       " {'href': '/wiki/Interaction_(statistics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Interaction (statistics)'},\n",
       " {'href': '/wiki/Isotonic_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Isotonic regression'},\n",
       " {'href': '/wiki/Iteratively_reweighted_least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Iteratively reweighted least squares'},\n",
       " {'href': '/wiki/Kitchen_sink_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Kitchen sink regression'},\n",
       " {'href': '/wiki/Lack-of-fit_sum_of_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Lack-of-fit sum of squares'},\n",
       " {'href': '/wiki/Leverage_(statistics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Leverage (statistics)'},\n",
       " {'href': '/wiki/Limited_dependent_variable',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Limited dependent variable'},\n",
       " {'href': '/wiki/Linear_probability_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Linear probability model'},\n",
       " {'href': '/wiki/Mallows%27s_Cp',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': \"Mallows's Cp\"},\n",
       " {'href': '/wiki/Mean_and_predicted_response',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Mean and predicted response'},\n",
       " {'href': '/wiki/Mixed_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Mixed model'},\n",
       " {'href': '/wiki/Moderation_(statistics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Moderation (statistics)'},\n",
       " {'href': '/wiki/Moving_least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Moving least squares'},\n",
       " {'href': '/wiki/Multicollinearity',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Multicollinearity'},\n",
       " {'href': '/wiki/Multiple_correlation',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Multiple correlation'},\n",
       " {'href': '/wiki/Multivariate_probit',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Multivariate probit'},\n",
       " {'href': '/wiki/Multivariate_adaptive_regression_splines',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Multivariate adaptive regression splines'},\n",
       " {'href': '/wiki/Newey%E2%80%93West_estimator',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Newey–West estimator'},\n",
       " {'href': '/wiki/Non-linear_least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Non-linear least squares'},\n",
       " {'href': '/wiki/Nonlinear_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Nonlinear regression'},\n",
       " {'href': '/wiki/Logit', 'section': 'Logistic Regression', 'title': 'Logit'},\n",
       " {'href': '/wiki/Multinomial_logit',\n",
       "  'section': 'Logistic Regression',\n",
       "  'title': 'Multinomial logit'},\n",
       " {'href': '/wiki/Logistic_regression',\n",
       "  'section': 'Logistic Regression',\n",
       "  'title': 'Logistic regression'},\n",
       " {'href': '/wiki/Bio-inspired_computing',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Bio-inspired computing'},\n",
       " {'href': '/wiki/Metaheuristic',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Metaheuristic'},\n",
       " {'href': '/wiki/Swarm_intelligence',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Swarm intelligence'},\n",
       " None,\n",
       " {'href': '/wiki/Particle_swarm_optimization',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Particle swarm optimization'},\n",
       " {'href': '/wiki/Ant_colony_optimization_algorithms',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Ant colony optimization algorithms'},\n",
       " {'href': '/wiki/Artificial_immune_system',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Artificial immune system'},\n",
       " {'href': '/wiki/Firefly_algorithm',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Firefly algorithm'},\n",
       " {'href': '/wiki/Cuckoo_search',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Cuckoo search'},\n",
       " {'href': '/wiki/Bat_algorithm',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Bat algorithm'},\n",
       " {'href': '/wiki/Evolvability_(computer_science)',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Evolvability (computer science)'},\n",
       " {'href': '/wiki/Evolutionary_computation',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Evolutionary computation'},\n",
       " {'href': '/wiki/Evolutionary_algorithm',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Evolutionary algorithm'},\n",
       " {'href': '/wiki/Genetic_algorithm',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Genetic algorithm'},\n",
       " {'href': '/wiki/Chromosome_(genetic_algorithm)',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Chromosome (genetic algorithm)'},\n",
       " {'href': '/wiki/Crossover_(genetic_algorithm)',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Crossover (genetic algorithm)'},\n",
       " {'href': '/wiki/Fitness_function',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Fitness function'},\n",
       " {'href': '/wiki/Evolutionary_data_mining',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Evolutionary data mining'},\n",
       " {'href': '/wiki/Genetic_programming',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Genetic programming'},\n",
       " {'href': '/wiki/Learnable_Evolution_Model',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Learnable Evolution Model'},\n",
       " {'href': '/wiki/Stochastic_diffusion_search',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Stochastic diffusion search'},\n",
       " {'href': '/wiki/Neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural network'},\n",
       " {'href': '/wiki/Artificial_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Artificial neural network'},\n",
       " {'href': '/wiki/Artificial_neuron',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Artificial neuron'},\n",
       " {'href': '/wiki/Types_of_artificial_neural_networks',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Types of artificial neural networks'},\n",
       " {'href': '/wiki/Perceptron',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Perceptron'},\n",
       " {'href': '/wiki/Multilayer_perceptron',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Multilayer perceptron'},\n",
       " {'href': '/wiki/Activation_function',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Activation function'},\n",
       " {'href': '/wiki/Self-organizing_map',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Self-organizing map'},\n",
       " {'href': '/wiki/Attractor_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Attractor network'},\n",
       " {'href': '/wiki/ADALINE', 'section': 'Neural Networks', 'title': 'ADALINE'},\n",
       " {'href': '/wiki/Adaptive_Neuro_Fuzzy_Inference_System',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Adaptive Neuro Fuzzy Inference System'},\n",
       " {'href': '/wiki/Adaptive_resonance_theory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Adaptive resonance theory'},\n",
       " {'href': '/wiki/IPO_underpricing_algorithm',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'IPO underpricing algorithm'},\n",
       " {'href': '/wiki/ALOPEX', 'section': 'Neural Networks', 'title': 'ALOPEX'},\n",
       " {'href': '/wiki/Artificial_Intelligence_System',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Artificial Intelligence System'},\n",
       " {'href': '/wiki/Autoassociative_memory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Autoassociative memory'},\n",
       " {'href': '/wiki/Autoencoder',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Autoencoder'},\n",
       " {'href': '/wiki/Backpropagation',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Backpropagation'},\n",
       " {'href': '/wiki/Bcpnn', 'section': 'Neural Networks', 'title': 'Bcpnn'},\n",
       " {'href': '/wiki/Bidirectional_associative_memory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Bidirectional associative memory'},\n",
       " {'href': '/wiki/Biological_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Biological neural network'},\n",
       " {'href': '/wiki/Boltzmann_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Boltzmann machine'},\n",
       " {'href': '/wiki/Restricted_Boltzmann_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Restricted Boltzmann machine'},\n",
       " {'href': '/wiki/Cellular_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Cellular neural network'},\n",
       " {'href': '/wiki/Cerebellar_Model_Articulation_Controller',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Cerebellar Model Articulation Controller'},\n",
       " {'href': '/wiki/Committee_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Committee machine'},\n",
       " {'href': '/wiki/Competitive_learning',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Competitive learning'},\n",
       " {'href': '/wiki/Compositional_pattern-producing_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Compositional pattern-producing network'},\n",
       " {'href': '/wiki/Computational_cybernetics',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Computational cybernetics'},\n",
       " {'href': '/wiki/Computational_neurogenetic_modeling',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Computational neurogenetic modeling'},\n",
       " {'href': '/wiki/Confabulation_(neural_networks)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Confabulation (neural networks)'},\n",
       " {'href': '/wiki/Cortical_column',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Cortical column'},\n",
       " {'href': '/w/index.php?title=Counterpropagation_network&action=edit&redlink=1',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Counterpropagation network (page does not exist)'},\n",
       " {'href': '/wiki/Cover%27s_theorem',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': \"Cover's theorem\"},\n",
       " {'href': '/wiki/Cultured_neuronal_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Cultured neuronal network'},\n",
       " {'href': '/wiki/Dehaene-Changeux_Model',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Dehaene-Changeux Model'},\n",
       " {'href': '/wiki/Delta_rule',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Delta rule'},\n",
       " {'href': '/wiki/Early_stopping',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Early stopping'},\n",
       " {'href': '/wiki/Echo_state_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Echo state network'},\n",
       " {'href': '/wiki/The_Emotion_Machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'The Emotion Machine'},\n",
       " {'href': '/wiki/Evolutionary_Acquisition_of_Neural_Topologies',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Evolutionary Acquisition of Neural Topologies'},\n",
       " {'href': '/wiki/Extension_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Extension neural network'},\n",
       " {'href': '/wiki/Feed_forward_(control)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Feed forward (control)'},\n",
       " {'href': '/wiki/Feedforward_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Feedforward neural network'},\n",
       " {'href': '/wiki/Generalized_Hebbian_Algorithm',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Generalized Hebbian Algorithm'},\n",
       " {'href': '/wiki/Generative_topographic_map',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Generative topographic map'},\n",
       " {'href': '/wiki/Group_method_of_data_handling',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Group method of data handling'},\n",
       " {'href': '/wiki/Growing_self-organizing_map',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Growing self-organizing map'},\n",
       " {'href': '/wiki/Memory-prediction_framework',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Memory-prediction framework'},\n",
       " {'href': '/wiki/Helmholtz_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Helmholtz machine'},\n",
       " {'href': '/wiki/Hierarchical_temporal_memory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Hierarchical temporal memory'},\n",
       " {'href': '/wiki/Hopfield_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Hopfield network'},\n",
       " {'href': '/wiki/Hybrid_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Hybrid neural network'},\n",
       " {'href': '/wiki/HyperNEAT',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'HyperNEAT'},\n",
       " {'href': '/wiki/Infomax', 'section': 'Neural Networks', 'title': 'Infomax'},\n",
       " {'href': '/wiki/Instantaneously_trained_neural_networks',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Instantaneously trained neural networks'},\n",
       " {'href': '/wiki/Interactive_Activation_and_Competition',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Interactive Activation and Competition'},\n",
       " {'href': '/wiki/Leabra', 'section': 'Neural Networks', 'title': 'Leabra'},\n",
       " {'href': '/wiki/Learning_Vector_Quantization',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Learning Vector Quantization'},\n",
       " {'href': '/wiki/Lernmatrix',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Lernmatrix'},\n",
       " {'href': '/wiki/Linde%E2%80%93Buzo%E2%80%93Gray_algorithm',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Linde–Buzo–Gray algorithm'},\n",
       " {'href': '/wiki/Liquid_state_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Liquid state machine'},\n",
       " {'href': '/wiki/Long_short-term_memory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Long short-term memory'},\n",
       " {'href': '/wiki/Madaline', 'section': 'Neural Networks', 'title': 'Madaline'},\n",
       " {'href': '/wiki/Modular_neural_networks',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Modular neural networks'},\n",
       " {'href': '/wiki/MoneyBee', 'section': 'Neural Networks', 'title': 'MoneyBee'},\n",
       " {'href': '/wiki/Neocognitron',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neocognitron'},\n",
       " {'href': '/wiki/Nervous_system_network_models',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Nervous system network models'},\n",
       " {'href': '/wiki/NETtalk_(artificial_neural_network)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'NETtalk (artificial neural network)'},\n",
       " {'href': '/wiki/Neural_backpropagation',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural backpropagation'},\n",
       " {'href': '/wiki/Neural_coding',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural coding'},\n",
       " {'href': '/wiki/Neural_cryptography',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural cryptography'},\n",
       " {'href': '/wiki/Neural_decoding',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural decoding'},\n",
       " {'href': '/wiki/Neural_gas',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural gas'},\n",
       " {'href': '/wiki/Neural_Information_Processing_Systems',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural Information Processing Systems'},\n",
       " {'href': '/wiki/Neural_modeling_fields',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural modeling fields'},\n",
       " {'href': '/wiki/Neural_oscillation',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural oscillation'},\n",
       " {'href': '/wiki/Neurally_controlled_animat',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neurally controlled animat'},\n",
       " {'href': '/wiki/Neuroevolution_of_augmenting_topologies',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neuroevolution of augmenting topologies'},\n",
       " {'href': '/wiki/Neuroplasticity',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neuroplasticity'},\n",
       " {'href': '/wiki/Ni1000', 'section': 'Neural Networks', 'title': 'Ni1000'},\n",
       " {'href': '/wiki/Nonspiking_neurons',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Nonspiking neurons'},\n",
       " {'href': '/wiki/Nonsynaptic_plasticity',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Nonsynaptic plasticity'},\n",
       " {'href': '/wiki/Oja%27s_rule',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': \"Oja's rule\"},\n",
       " {'href': '/wiki/Optical_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Optical neural network'},\n",
       " {'href': '/wiki/Phase-of-firing_code',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Phase-of-firing code'},\n",
       " {'href': '/wiki/Promoter_based_genetic_algorithm',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Promoter based genetic algorithm'},\n",
       " {'href': '/wiki/Pulse-coupled_networks',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Pulse-coupled networks'},\n",
       " {'href': '/wiki/Quantum_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Quantum neural network'},\n",
       " {'href': '/wiki/Radial_basis_function',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Radial basis function'},\n",
       " {'href': '/wiki/Radial_basis_function_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Radial basis function network'},\n",
       " {'href': '/wiki/Random_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Random neural network'},\n",
       " {'href': '/wiki/Recurrent_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Recurrent neural network'},\n",
       " {'href': '/wiki/Reentry_(neural_circuitry)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Reentry (neural circuitry)'},\n",
       " {'href': '/wiki/Reservoir_computing',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Reservoir computing'},\n",
       " {'href': '/wiki/Rprop', 'section': 'Neural Networks', 'title': 'Rprop'},\n",
       " {'href': '/wiki/Semantic_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Semantic neural network'},\n",
       " {'href': '/wiki/Sigmoid_function',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Sigmoid function'},\n",
       " {'href': '/wiki/SNARC', 'section': 'Neural Networks', 'title': 'SNARC'},\n",
       " {'href': '/wiki/Softmax_activation_function',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Softmax activation function'},\n",
       " {'href': '/wiki/Spiking_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Spiking neural network'},\n",
       " {'href': '/wiki/Stochastic_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Stochastic neural network'},\n",
       " {'href': '/wiki/Synaptic_plasticity',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Synaptic plasticity'},\n",
       " {'href': '/wiki/Synaptic_weight',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Synaptic weight'},\n",
       " {'href': '/wiki/Tensor_product_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Tensor product network'},\n",
       " {'href': '/wiki/Time_delay_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Time delay neural network'},\n",
       " {'href': '/wiki/U-Matrix', 'section': 'Neural Networks', 'title': 'U-Matrix'},\n",
       " {'href': '/wiki/Universal_approximation_theorem',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Universal approximation theorem'},\n",
       " {'href': '/wiki/Winner-take-all_(computing)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Winner-take-all (computing)'},\n",
       " {'href': '/wiki/Winnow_(algorithm)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Winnow (algorithm)'},\n",
       " {'href': '/wiki/Reinforcement_learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Reinforcement learning'},\n",
       " {'href': '/wiki/Markov_decision_process',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Markov decision process'},\n",
       " {'href': '/wiki/Bellman_equation',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Bellman equation'},\n",
       " {'href': '/wiki/Q-learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Q-learning'},\n",
       " {'href': '/wiki/Temporal_difference_learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Temporal difference learning'},\n",
       " {'href': '/wiki/SARSA',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'SARSA'},\n",
       " {'href': '/wiki/Multi-armed_bandit',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Multi-armed bandit'},\n",
       " {'href': '/wiki/Apprenticeship_learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Apprenticeship learning'},\n",
       " {'href': '/wiki/Predictive_learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Predictive learning'},\n",
       " {'href': '/wiki/Text_mining',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Text mining'},\n",
       " {'href': '/wiki/Natural_language_processing',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Natural language processing'},\n",
       " {'href': '/wiki/Document_classification',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Document classification'},\n",
       " {'href': '/wiki/Bag_of_words_model',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Bag of words model'},\n",
       " {'href': '/wiki/N-gram', 'section': 'Text Mining', 'title': 'N-gram'},\n",
       " {'href': '/wiki/Part-of-speech_tagging',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Part-of-speech tagging'},\n",
       " {'href': '/wiki/Sentiment_analysis',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Sentiment analysis'},\n",
       " {'href': '/wiki/Information_extraction',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Information extraction'},\n",
       " {'href': '/wiki/Topic_model',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Topic model'},\n",
       " {'href': '/wiki/Concept_mining',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Concept mining'},\n",
       " {'href': '/wiki/Semantic_analysis_(machine_learning)',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Semantic analysis (machine learning)'},\n",
       " {'href': '/wiki/Automatic_summarization',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Automatic summarization'},\n",
       " {'href': '/wiki/String_kernel',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'String kernel'},\n",
       " {'href': '/wiki/Biomedical_text_mining',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Biomedical text mining'},\n",
       " {'href': '/wiki/Never-Ending_Language_Learning',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Never-Ending Language Learning'},\n",
       " {'href': '/wiki/Structure_mining',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Structure mining'},\n",
       " {'href': '/wiki/Structured_learning',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Structured learning'},\n",
       " {'href': '/wiki/Structured_prediction',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Structured prediction'},\n",
       " {'href': '/wiki/Sequence_mining',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Sequence mining'},\n",
       " {'href': '/wiki/Sequence_labeling',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Sequence labeling'},\n",
       " {'href': '/wiki/Process_mining',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Process mining'},\n",
       " {'href': '/wiki/Multi-label_classification',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Multi-label classification'},\n",
       " {'href': '/wiki/Automated_machine_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Automated machine learning'},\n",
       " {'href': '/wiki/Classifier_chains',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Classifier chains'},\n",
       " {'href': '/wiki/Web_mining',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Web mining'},\n",
       " {'href': '/wiki/Anomaly_detection',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Anomaly detection'},\n",
       " {'href': '/wiki/Anomaly_Detection_at_Multiple_Scales',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Anomaly Detection at Multiple Scales'},\n",
       " {'href': '/wiki/Local_outlier_factor',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Local outlier factor'},\n",
       " {'href': '/wiki/Novelty_detection',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Novelty detection'},\n",
       " {'href': '/wiki/GSP_Algorithm',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'GSP Algorithm'},\n",
       " {'href': '/wiki/Optimal_matching',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Optimal matching'},\n",
       " {'href': '/wiki/Record_linkage',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Record linkage'},\n",
       " {'href': '/wiki/Meta_learning_(computer_science)',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Meta learning (computer science)'},\n",
       " {'href': '/wiki/Learning_automata',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Learning automata'},\n",
       " {'href': '/wiki/Learning_to_rank',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Learning to rank'},\n",
       " {'href': '/wiki/Multiple-instance_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Multiple-instance learning'},\n",
       " {'href': '/wiki/Statistical_relational_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Statistical relational learning'},\n",
       " {'href': '/wiki/Relational_classification',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Relational classification'},\n",
       " {'href': '/wiki/Data_stream_mining',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Data stream mining'},\n",
       " {'href': '/wiki/Alpha_algorithm',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Alpha algorithm'},\n",
       " {'href': '/wiki/Syntactic_pattern_recognition',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Syntactic pattern recognition'},\n",
       " {'href': '/wiki/Multispectral_pattern_recognition',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Multispectral pattern recognition'},\n",
       " {'href': '/wiki/Algorithmic_learning_theory',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Algorithmic learning theory'},\n",
       " {'href': '/wiki/Deep_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Deep learning'},\n",
       " {'href': '/wiki/Bongard_problem',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Bongard problem'},\n",
       " {'href': '/wiki/Learning_with_errors',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Learning with errors'},\n",
       " {'href': '/wiki/Parity_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Parity learning'},\n",
       " {'href': '/wiki/Inductive_transfer',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Inductive transfer'},\n",
       " {'href': '/wiki/Granular_computing',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Granular computing'},\n",
       " {'href': '/wiki/Conceptual_clustering',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Conceptual clustering'},\n",
       " {'href': '/wiki/Formal_concept_analysis',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Formal concept analysis'},\n",
       " {'href': '/wiki/Biclustering',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Biclustering'},\n",
       " {'href': '/wiki/Information_visualization',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Information visualization'},\n",
       " {'href': '/wiki/Co-occurrence_networks',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Co-occurrence networks'},\n",
       " {'href': '/wiki/Problem_domain',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Problem domain'},\n",
       " {'href': '/wiki/Recommender_system',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Recommender system'},\n",
       " {'href': '/wiki/Collaborative_filtering',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Collaborative filtering'},\n",
       " {'href': '/wiki/Profiling_(information_science)',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Profiling (information science)'},\n",
       " {'href': '/wiki/Speech_recognition',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Speech recognition'},\n",
       " {'href': '/wiki/Stock_forecast',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Stock forecast'},\n",
       " {'href': '/wiki/Activity_recognition',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Activity recognition'},\n",
       " {'href': '/wiki/Data_Analysis_Techniques_for_Fraud_Detection',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Data Analysis Techniques for Fraud Detection'},\n",
       " {'href': '/wiki/Molecule_mining',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Molecule mining'},\n",
       " {'href': '/wiki/Behavioral_targeting',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Behavioral targeting'},\n",
       " {'href': '/wiki/Proactive_Discovery_of_Insider_Threats_Using_Graph_Analysis_and_Learning',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Proactive Discovery of Insider Threats Using Graph Analysis and Learning'},\n",
       " {'href': '/wiki/Robot_learning',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Robot learning'},\n",
       " {'href': '/wiki/Computer_vision',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Computer vision'},\n",
       " {'href': '/wiki/Facial_recognition_system',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Facial recognition system'},\n",
       " {'href': '/wiki/Outlier_detection',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Outlier detection'},\n",
       " {'href': '/wiki/Anomaly_detection',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Anomaly detection'},\n",
       " {'href': '/wiki/Novelty_detection',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Novelty detection'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = 'https://en.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_definition(link_dict):\n",
    "    if link_dict is not None:\n",
    "        url = baseurl + link_dict['href']\n",
    "        print(url)\n",
    "        response = requests.get(url, params = my_attrs)\n",
    "        if response.status_code == 200:\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            div = soup.select_one('div.mw-parser-output')\n",
    "#             print(div)\n",
    "            p_definition = div.select_one('p')\n",
    "#             print(p_definition)\n",
    "            definition = p_definition.text\n",
    "            link_dict['definition'] = definition\n",
    "            print(\"Got definition for {}.\".format(link_dict['title']))\n",
    "#             print(definition)\n",
    "    return link_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Got definition for Machine learning.\n",
      "https://en.wikipedia.org/wiki/Data_analysis\n",
      "Got definition for Data analysis.\n",
      "https://en.wikipedia.org/wiki/Occam%27s_razor\n",
      "Got definition for Occam's razor.\n",
      "https://en.wikipedia.org/wiki/Curse_of_dimensionality\n",
      "Got definition for Curse of dimensionality.\n",
      "https://en.wikipedia.org/wiki/No_free_lunch_theorem\n",
      "Got definition for No free lunch theorem.\n",
      "https://en.wikipedia.org/wiki/Accuracy_paradox\n",
      "Got definition for Accuracy paradox.\n",
      "https://en.wikipedia.org/wiki/Overfitting\n",
      "Got definition for Overfitting.\n",
      "https://en.wikipedia.org/wiki/Regularization_(machine_learning)\n",
      "Got definition for Regularization (machine learning).\n",
      "https://en.wikipedia.org/wiki/Inductive_bias\n",
      "Got definition for Inductive bias.\n",
      "https://en.wikipedia.org/wiki/Data_dredging\n",
      "Got definition for Data dredging.\n",
      "https://en.wikipedia.org/wiki/Ugly_duckling_theorem\n",
      "Got definition for Ugly duckling theorem.\n",
      "https://en.wikipedia.org/wiki/Uncertain_data\n",
      "Got definition for Uncertain data.\n",
      "https://en.wikipedia.org/wiki/Knowledge_discovery\n",
      "Got definition for Knowledge discovery.\n",
      "https://en.wikipedia.org/wiki/Data_mining\n",
      "Got definition for Data mining.\n",
      "https://en.wikipedia.org/wiki/Predictive_analytics\n",
      "Got definition for Predictive analytics.\n",
      "https://en.wikipedia.org/wiki/Predictive_modelling\n",
      "Got definition for Predictive modelling.\n",
      "https://en.wikipedia.org/wiki/Business_intelligence\n",
      "Got definition for Business intelligence.\n",
      "https://en.wikipedia.org/wiki/Reactive_business_intelligence\n",
      "Got definition for Reactive business intelligence.\n",
      "https://en.wikipedia.org/wiki/Business_analytics\n",
      "Got definition for Business analytics.\n",
      "https://en.wikipedia.org/wiki/Reactive_business_intelligence\n",
      "Got definition for Reactive business intelligence.\n",
      "https://en.wikipedia.org/wiki/Pattern_recognition\n",
      "Got definition for Pattern recognition.\n",
      "https://en.wikipedia.org/wiki/Abductive_reasoning\n",
      "Got definition for Abductive reasoning.\n",
      "https://en.wikipedia.org/wiki/Inductive_reasoning\n",
      "Got definition for Inductive reasoning.\n",
      "https://en.wikipedia.org/wiki/First-order_logic\n",
      "Got definition for First-order logic.\n",
      "https://en.wikipedia.org/wiki/Inductive_logic_programming\n",
      "Got definition for Inductive logic programming.\n",
      "https://en.wikipedia.org/wiki/Reasoning_system\n",
      "Got definition for Reasoning system.\n",
      "https://en.wikipedia.org/wiki/Case-based_reasoning\n",
      "Got definition for Case-based reasoning.\n",
      "https://en.wikipedia.org/wiki/Textual_case_based_reasoning\n",
      "Got definition for Textual case based reasoning.\n",
      "https://en.wikipedia.org/wiki/Causality\n",
      "Got definition for Causality.\n",
      "https://en.wikipedia.org/wiki/Nearest_neighbor_search\n",
      "Got definition for Nearest neighbor search.\n",
      "https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
      "Got definition for Stochastic gradient descent.\n",
      "https://en.wikipedia.org/wiki/Beam_search\n",
      "Got definition for Beam search.\n",
      "https://en.wikipedia.org/wiki/Best-first_search\n",
      "Got definition for Best-first search.\n",
      "https://en.wikipedia.org/wiki/Breadth-first_search\n",
      "Got definition for Breadth-first search.\n",
      "https://en.wikipedia.org/wiki/Hill_climbing\n",
      "Got definition for Hill climbing.\n",
      "https://en.wikipedia.org/wiki/Grid_search\n",
      "Got definition for Grid search.\n",
      "https://en.wikipedia.org/wiki/Brute-force_search\n",
      "Got definition for Brute-force search.\n",
      "https://en.wikipedia.org/wiki/Depth-first_search\n",
      "Got definition for Depth-first search.\n",
      "https://en.wikipedia.org/wiki/Tabu_search\n",
      "Got definition for Tabu search.\n",
      "https://en.wikipedia.org/wiki/Anytime_algorithm\n",
      "Got definition for Anytime algorithm.\n",
      "https://en.wikipedia.org/wiki/Exploratory_data_analysis\n",
      "Got definition for Exploratory data analysis.\n",
      "https://en.wikipedia.org/wiki/Covariate\n",
      "Got definition for Covariate.\n",
      "https://en.wikipedia.org/wiki/Statistical_inference\n",
      "Got definition for Statistical inference.\n",
      "https://en.wikipedia.org/wiki/Algorithmic_inference\n",
      "Got definition for Algorithmic inference.\n",
      "https://en.wikipedia.org/wiki/Bayesian_inference\n",
      "Got definition for Bayesian inference.\n",
      "https://en.wikipedia.org/wiki/Base_rate\n",
      "Got definition for Base rate.\n",
      "https://en.wikipedia.org/wiki/Bias_(statistics)\n",
      "Got definition for Bias (statistics).\n",
      "https://en.wikipedia.org/wiki/Gibbs_sampling\n",
      "Got definition for Gibbs sampling.\n",
      "https://en.wikipedia.org/wiki/Cross-entropy_method\n",
      "Got definition for Cross-entropy method.\n",
      "https://en.wikipedia.org/wiki/Latent_variable\n",
      "Got definition for Latent variable.\n",
      "https://en.wikipedia.org/wiki/Maximum_likelihood\n",
      "Got definition for Maximum likelihood.\n",
      "https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation\n",
      "Got definition for Maximum a posteriori estimation.\n",
      "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\n",
      "Got definition for Expectation–maximization algorithm.\n",
      "https://en.wikipedia.org/wiki/Expectation_propagation\n",
      "Got definition for Expectation propagation.\n",
      "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
      "Got definition for Kullback–Leibler divergence.\n",
      "https://en.wikipedia.org/wiki/Generative_model\n",
      "Got definition for Generative model.\n",
      "https://en.wikipedia.org/wiki/Supervised_learning\n",
      "Got definition for Supervised learning.\n",
      "https://en.wikipedia.org/wiki/Unsupervised_learning\n",
      "Got definition for Unsupervised learning.\n",
      "https://en.wikipedia.org/wiki/Active_learning_(machine_learning)\n",
      "Got definition for Active learning (machine learning).\n",
      "https://en.wikipedia.org/wiki/Reinforcement_learning\n",
      "Got definition for Reinforcement learning.\n",
      "https://en.wikipedia.org/wiki/Multi-task_learning\n",
      "Got definition for Multi-task learning.\n",
      "https://en.wikipedia.org/wiki/Transduction_(machine_learning)\n",
      "Got definition for Transduction (machine learning).\n",
      "https://en.wikipedia.org/wiki/Explanation-based_learning\n",
      "Got definition for Explanation-based learning.\n",
      "https://en.wikipedia.org/wiki/Offline_learning\n",
      "Got definition for Offline learning.\n",
      "https://en.wikipedia.org/wiki/Online_learning_model\n",
      "Got definition for Online learning model.\n",
      "https://en.wikipedia.org/wiki/Online_machine_learning\n",
      "Got definition for Online machine learning.\n",
      "https://en.wikipedia.org/wiki/Hyperparameter_optimization\n",
      "Got definition for Hyperparameter optimization.\n",
      "https://en.wikipedia.org/wiki/Classification_in_machine_learning\n",
      "Got definition for Classification in machine learning.\n",
      "https://en.wikipedia.org/wiki/Concept_class\n",
      "Got definition for Concept class.\n",
      "https://en.wikipedia.org/wiki/Features_(pattern_recognition)\n",
      "Got definition for Features (pattern recognition).\n",
      "https://en.wikipedia.org/wiki/Feature_vector\n",
      "Got definition for Feature vector.\n",
      "https://en.wikipedia.org/wiki/Feature_space\n",
      "Got definition for Feature space.\n",
      "https://en.wikipedia.org/wiki/Concept_learning\n",
      "Got definition for Concept learning.\n",
      "https://en.wikipedia.org/wiki/Binary_classification\n",
      "Got definition for Binary classification.\n",
      "https://en.wikipedia.org/wiki/Decision_boundary\n",
      "Got definition for Decision boundary.\n",
      "https://en.wikipedia.org/wiki/Multiclass_classification\n",
      "Got definition for Multiclass classification.\n",
      "https://en.wikipedia.org/wiki/Class_membership_probabilities\n",
      "Got definition for Class membership probabilities.\n",
      "https://en.wikipedia.org/wiki/Calibration_(statistics)\n",
      "Got definition for Calibration (statistics).\n",
      "https://en.wikipedia.org/wiki/Concept_drift\n",
      "Got definition for Concept drift.\n",
      "https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition\n",
      "Got definition for Prior knowledge for pattern recognition.\n",
      "https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
      "Got definition for Iris flower data set.\n",
      "https://en.wikipedia.org/wiki/Margin_Infused_Relaxed_Algorithm\n",
      "Got definition for Margin Infused Relaxed Algorithm.\n",
      "https://en.wikipedia.org/wiki/Semi-supervised_learning\n",
      "Got definition for Semi-supervised learning.\n",
      "https://en.wikipedia.org/wiki/One-class_classification\n",
      "Got definition for One-class classification.\n",
      "https://en.wikipedia.org/wiki/Coupled_pattern_learner\n",
      "Got definition for Coupled pattern learner.\n",
      "https://en.wikipedia.org/wiki/Lazy_learning\n",
      "Got definition for Lazy learning.\n",
      "https://en.wikipedia.org/wiki/Eager_learning\n",
      "Got definition for Eager learning.\n",
      "https://en.wikipedia.org/wiki/Instance-based_learning\n",
      "Got definition for Instance-based learning.\n",
      "https://en.wikipedia.org/wiki/Cluster_assumption\n",
      "Got definition for Cluster assumption.\n",
      "https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "Got definition for K-nearest neighbor algorithm.\n",
      "https://en.wikipedia.org/wiki/IDistance\n",
      "Got definition for IDistance.\n",
      "https://en.wikipedia.org/wiki/Large_margin_nearest_neighbor\n",
      "Got definition for Large margin nearest neighbor.\n",
      "https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "Got definition for Decision tree learning.\n",
      "https://en.wikipedia.org/wiki/Decision_stump\n",
      "Got definition for Decision stump.\n",
      "https://en.wikipedia.org/wiki/Pruning_(decision_trees)\n",
      "Got definition for Pruning (decision trees).\n",
      "https://en.wikipedia.org/wiki/Mutual_information\n",
      "Got definition for Mutual information.\n",
      "https://en.wikipedia.org/wiki/Adjusted_mutual_information\n",
      "Got definition for Adjusted mutual information.\n",
      "https://en.wikipedia.org/wiki/Information_gain_ratio\n",
      "Got definition for Information gain ratio.\n",
      "https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\n",
      "Got definition for Information gain in decision trees.\n",
      "https://en.wikipedia.org/wiki/ID3_algorithm\n",
      "Got definition for ID3 algorithm.\n",
      "https://en.wikipedia.org/wiki/C4.5_algorithm\n",
      "Got definition for C4.5 algorithm.\n",
      "https://en.wikipedia.org/wiki/CHAID\n",
      "Got definition for CHAID.\n",
      "https://en.wikipedia.org/wiki/Information_Fuzzy_Networks\n",
      "Got definition for Information Fuzzy Networks.\n",
      "https://en.wikipedia.org/wiki/Grafting_(decision_trees)\n",
      "Got definition for Grafting (decision trees).\n",
      "https://en.wikipedia.org/wiki/Incremental_decision_tree\n",
      "Got definition for Incremental decision tree.\n",
      "https://en.wikipedia.org/wiki/Alternating_decision_tree\n",
      "Got definition for Alternating decision tree.\n",
      "https://en.wikipedia.org/wiki/Logistic_model_tree\n",
      "Got definition for Logistic model tree.\n",
      "https://en.wikipedia.org/wiki/Random_forest\n",
      "Got definition for Random forest.\n",
      "https://en.wikipedia.org/wiki/Linear_classifier\n",
      "Got definition for Linear classifier.\n",
      "https://en.wikipedia.org/wiki/Margin_(machine_learning)\n",
      "Got definition for Margin (machine learning).\n",
      "https://en.wikipedia.org/wiki/Margin_classifier\n",
      "Got definition for Margin classifier.\n",
      "https://en.wikipedia.org/wiki/Soft_independent_modelling_of_class_analogies\n",
      "Got definition for Soft independent modelling of class analogies.\n",
      "https://en.wikipedia.org/wiki/Statistical_classification\n",
      "Got definition for Statistical classification.\n",
      "https://en.wikipedia.org/wiki/Probability_matching\n",
      "Got definition for Probability matching.\n",
      "https://en.wikipedia.org/wiki/Discriminative_model\n",
      "Got definition for Discriminative model.\n",
      "https://en.wikipedia.org/wiki/Linear_discriminant_analysis\n",
      "Got definition for Linear discriminant analysis.\n",
      "https://en.wikipedia.org/wiki/Multiclass_LDA\n",
      "Got definition for Multiclass LDA.\n",
      "https://en.wikipedia.org/wiki/Multiple_discriminant_analysis\n",
      "Got definition for Multiple discriminant analysis.\n",
      "https://en.wikipedia.org/wiki/Optimal_discriminant_analysis\n",
      "Got definition for Optimal discriminant analysis.\n",
      "https://en.wikipedia.org/wiki/Fisher_kernel\n",
      "Got definition for Fisher kernel.\n",
      "https://en.wikipedia.org/wiki/Discriminant_function_analysis\n",
      "Got definition for Discriminant function analysis.\n",
      "https://en.wikipedia.org/wiki/Multilinear_subspace_learning\n",
      "Got definition for Multilinear subspace learning.\n",
      "https://en.wikipedia.org/wiki/Quadratic_classifier\n",
      "Got definition for Quadratic classifier.\n",
      "https://en.wikipedia.org/wiki/Variable_kernel_density_estimation\n",
      "Got definition for Variable kernel density estimation.\n",
      "https://en.wikipedia.org/wiki/Category_utility\n",
      "Got definition for Category utility.\n",
      "https://en.wikipedia.org/wiki/Data_classification_(business_intelligence)\n",
      "Got definition for Data classification (business intelligence).\n",
      "https://en.wikipedia.org/wiki/Training_set\n",
      "Got definition for Training set.\n",
      "https://en.wikipedia.org/wiki/Test_set\n",
      "Got definition for Test set.\n",
      "https://en.wikipedia.org/wiki/Synthetic_data\n",
      "Got definition for Synthetic data.\n",
      "https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
      "Got definition for Cross-validation (statistics).\n",
      "https://en.wikipedia.org/wiki/Loss_function\n",
      "Got definition for Loss function.\n",
      "https://en.wikipedia.org/wiki/Hinge_loss\n",
      "Got definition for Hinge loss.\n",
      "https://en.wikipedia.org/wiki/Generalization_error\n",
      "Got definition for Generalization error.\n",
      "https://en.wikipedia.org/wiki/Type_I_and_type_II_errors\n",
      "Got definition for Type I and type II errors.\n",
      "https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
      "Got definition for Sensitivity and specificity.\n",
      "https://en.wikipedia.org/wiki/Precision_and_recall\n",
      "Got definition for Precision and recall.\n",
      "https://en.wikipedia.org/wiki/F1_score\n",
      "Got definition for F1 score.\n",
      "https://en.wikipedia.org/wiki/Confusion_matrix\n",
      "Got definition for Confusion matrix.\n",
      "https://en.wikipedia.org/wiki/Matthews_correlation_coefficient\n",
      "Got definition for Matthews correlation coefficient.\n",
      "https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
      "Got definition for Receiver operating characteristic.\n",
      "https://en.wikipedia.org/wiki/Lift_(data_mining)\n",
      "Got definition for Lift (data mining).\n",
      "https://en.wikipedia.org/wiki/Stability_in_learning\n",
      "Got definition for Stability in learning.\n",
      "https://en.wikipedia.org/wiki/Data_Pre-processing\n",
      "Got definition for Data Pre-processing.\n",
      "https://en.wikipedia.org/wiki/Discretization_of_continuous_features\n",
      "Got definition for Discretization of continuous features.\n",
      "https://en.wikipedia.org/wiki/Feature_engineering\n",
      "Got definition for Feature engineering.\n",
      "https://en.wikipedia.org/wiki/Feature_selection\n",
      "Got definition for Feature selection.\n",
      "https://en.wikipedia.org/wiki/Feature_extraction\n",
      "Got definition for Feature extraction.\n",
      "https://en.wikipedia.org/wiki/Dimension_reduction\n",
      "Got definition for Dimension reduction.\n",
      "https://en.wikipedia.org/wiki/Principal_component_analysis\n",
      "Got definition for Principal component analysis.\n",
      "https://en.wikipedia.org/wiki/Multilinear_principal-component_analysis\n",
      "Got definition for Multilinear principal-component analysis.\n",
      "https://en.wikipedia.org/wiki/Multifactor_dimensionality_reduction\n",
      "Got definition for Multifactor dimensionality reduction.\n",
      "https://en.wikipedia.org/wiki/Targeted_projection_pursuit\n",
      "Got definition for Targeted projection pursuit.\n",
      "https://en.wikipedia.org/wiki/Multidimensional_scaling\n",
      "Got definition for Multidimensional scaling.\n",
      "https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction\n",
      "Got definition for Nonlinear dimensionality reduction.\n",
      "https://en.wikipedia.org/wiki/Kernel_principal_component_analysis\n",
      "Got definition for Kernel principal component analysis.\n",
      "https://en.wikipedia.org/wiki/Kernel_eigenvoice\n",
      "Got definition for Kernel eigenvoice.\n",
      "https://en.wikipedia.org/wiki/Gramian_matrix\n",
      "Got definition for Gramian matrix.\n",
      "https://en.wikipedia.org/wiki/Gaussian_process\n",
      "Got definition for Gaussian process.\n",
      "https://en.wikipedia.org/wiki/Kernel_adaptive_filter\n",
      "Got definition for Kernel adaptive filter.\n",
      "https://en.wikipedia.org/wiki/Isomap\n",
      "Got definition for Isomap.\n",
      "https://en.wikipedia.org/wiki/Manifold_alignment\n",
      "Got definition for Manifold alignment.\n",
      "https://en.wikipedia.org/wiki/Diffusion_map\n",
      "Got definition for Diffusion map.\n",
      "https://en.wikipedia.org/wiki/Elastic_map\n",
      "Got definition for Elastic map.\n",
      "https://en.wikipedia.org/wiki/Locality-sensitive_hashing\n",
      "Got definition for Locality-sensitive hashing.\n",
      "https://en.wikipedia.org/wiki/Spectral_clustering\n",
      "Got definition for Spectral clustering.\n",
      "https://en.wikipedia.org/wiki/Minimum_redundancy_feature_selection\n",
      "Got definition for Minimum redundancy feature selection.\n",
      "https://en.wikipedia.org/wiki/Cluster_analysis\n",
      "Got definition for Cluster analysis.\n",
      "https://en.wikipedia.org/wiki/K-means_clustering\n",
      "Got definition for K-means clustering.\n",
      "https://en.wikipedia.org/wiki/K-means%2B%2B\n",
      "Got definition for K-means++.\n",
      "https://en.wikipedia.org/wiki/K-medians_clustering\n",
      "Got definition for K-medians clustering.\n",
      "https://en.wikipedia.org/wiki/K-medoids\n",
      "Got definition for K-medoids.\n",
      "https://en.wikipedia.org/wiki/DBSCAN\n",
      "Got definition for DBSCAN.\n",
      "https://en.wikipedia.org/wiki/Fuzzy_clustering\n",
      "Got definition for Fuzzy clustering.\n",
      "https://en.wikipedia.org/wiki/BIRCH_(data_clustering)\n",
      "Got definition for BIRCH (data clustering).\n",
      "https://en.wikipedia.org/wiki/Canopy_clustering_algorithm\n",
      "Got definition for Canopy clustering algorithm.\n",
      "https://en.wikipedia.org/wiki/Cluster-weighted_modeling\n",
      "Got definition for Cluster-weighted modeling.\n",
      "https://en.wikipedia.org/wiki/Clustering_high-dimensional_data\n",
      "Got definition for Clustering high-dimensional data.\n",
      "https://en.wikipedia.org/wiki/Cobweb_(clustering)\n",
      "Got definition for Cobweb (clustering).\n",
      "https://en.wikipedia.org/wiki/Complete-linkage_clustering\n",
      "Got definition for Complete-linkage clustering.\n",
      "https://en.wikipedia.org/wiki/Constrained_clustering\n",
      "Got definition for Constrained clustering.\n",
      "https://en.wikipedia.org/wiki/Correlation_clustering\n",
      "Got definition for Correlation clustering.\n",
      "https://en.wikipedia.org/wiki/CURE_data_clustering_algorithm\n",
      "Got definition for CURE data clustering algorithm.\n",
      "https://en.wikipedia.org/wiki/Data_stream_clustering\n",
      "Got definition for Data stream clustering.\n",
      "https://en.wikipedia.org/wiki/Dendrogram\n",
      "Got definition for Dendrogram.\n",
      "https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set\n",
      "Got definition for Determining the number of clusters in a data set.\n",
      "https://en.wikipedia.org/wiki/FLAME_clustering\n",
      "Got definition for FLAME clustering.\n",
      "https://en.wikipedia.org/wiki/Hierarchical_clustering\n",
      "Got definition for Hierarchical clustering.\n",
      "https://en.wikipedia.org/wiki/Information_bottleneck_method\n",
      "Got definition for Information bottleneck method.\n",
      "https://en.wikipedia.org/wiki/Lloyd%27s_algorithm\n",
      "Got definition for Lloyd's algorithm.\n",
      "https://en.wikipedia.org/wiki/Nearest-neighbor_chain_algorithm\n",
      "Got definition for Nearest-neighbor chain algorithm.\n",
      "https://en.wikipedia.org/wiki/Neighbor_joining\n",
      "Got definition for Neighbor joining.\n",
      "https://en.wikipedia.org/wiki/OPTICS_algorithm\n",
      "Got definition for OPTICS algorithm.\n",
      "https://en.wikipedia.org/wiki/Pitman%E2%80%93Yor_process\n",
      "Got definition for Pitman–Yor process.\n",
      "https://en.wikipedia.org/wiki/Single-linkage_clustering\n",
      "Got definition for Single-linkage clustering.\n",
      "https://en.wikipedia.org/wiki/SUBCLU\n",
      "Got definition for SUBCLU.\n",
      "https://en.wikipedia.org/wiki/Thresholding_(image_processing)\n",
      "Got definition for Thresholding (image processing).\n",
      "https://en.wikipedia.org/wiki/UPGMA\n",
      "Got definition for UPGMA.\n",
      "https://en.wikipedia.org/wiki/Rand_index\n",
      "Got definition for Rand index.\n",
      "https://en.wikipedia.org/wiki/Dunn_index\n",
      "Got definition for Dunn index.\n",
      "https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index\n",
      "Got definition for Davies–Bouldin index.\n",
      "https://en.wikipedia.org/wiki/Jaccard_index\n",
      "Got definition for Jaccard index.\n",
      "https://en.wikipedia.org/wiki/MinHash\n",
      "Got definition for MinHash.\n",
      "https://en.wikipedia.org/wiki/K_q-flats\n",
      "Got definition for K q-flats.\n",
      "https://en.wikipedia.org/wiki/Decision_rules\n",
      "Got definition for Decision rules.\n",
      "https://en.wikipedia.org/wiki/Rule_induction\n",
      "Got definition for Rule induction.\n",
      "https://en.wikipedia.org/wiki/Classification_rule\n",
      "Got definition for Classification rule.\n",
      "https://en.wikipedia.org/wiki/CN2_algorithm\n",
      "Got definition for CN2 algorithm.\n",
      "https://en.wikipedia.org/wiki/Decision_list\n",
      "Got definition for Decision list.\n",
      "https://en.wikipedia.org/wiki/First_Order_Inductive_Learner\n",
      "Got definition for First Order Inductive Learner.\n",
      "https://en.wikipedia.org/wiki/Association_rule_learning\n",
      "Got definition for Association rule learning.\n",
      "https://en.wikipedia.org/wiki/Apriori_algorithm\n",
      "Got definition for Apriori algorithm.\n",
      "https://en.wikipedia.org/wiki/Contrast_set_learning\n",
      "Got definition for Contrast set learning.\n",
      "https://en.wikipedia.org/wiki/Affinity_analysis\n",
      "Got definition for Affinity analysis.\n",
      "https://en.wikipedia.org/wiki/K-optimal_pattern_discovery\n",
      "Got definition for K-optimal pattern discovery.\n",
      "https://en.wikipedia.org/wiki/Ensemble_learning\n",
      "Got definition for Ensemble learning.\n",
      "https://en.wikipedia.org/wiki/Ensemble_averaging\n",
      "Got definition for Ensemble averaging.\n",
      "https://en.wikipedia.org/wiki/Consensus_clustering\n",
      "Got definition for Consensus clustering.\n",
      "https://en.wikipedia.org/wiki/AdaBoost\n",
      "Got definition for AdaBoost.\n",
      "https://en.wikipedia.org/wiki/Boosting\n",
      "Got definition for Boosting.\n",
      "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
      "Got definition for Bootstrap aggregating.\n",
      "https://en.wikipedia.org/wiki/BrownBoost\n",
      "Got definition for BrownBoost.\n",
      "https://en.wikipedia.org/wiki/Cascading_classifiers\n",
      "Got definition for Cascading classifiers.\n",
      "https://en.wikipedia.org/wiki/Co-training\n",
      "Got definition for Co-training.\n",
      "https://en.wikipedia.org/wiki/CoBoosting\n",
      "Got definition for CoBoosting.\n",
      "https://en.wikipedia.org/wiki/Gaussian_process_emulator\n",
      "Got definition for Gaussian process emulator.\n",
      "https://en.wikipedia.org/wiki/Gradient_boosting\n",
      "Got definition for Gradient boosting.\n",
      "https://en.wikipedia.org/wiki/LogitBoost\n",
      "Got definition for LogitBoost.\n",
      "https://en.wikipedia.org/wiki/LPBoost\n",
      "Got definition for LPBoost.\n",
      "https://en.wikipedia.org/wiki/Mixture_model\n",
      "Got definition for Mixture model.\n",
      "https://en.wikipedia.org/wiki/Product_of_Experts\n",
      "Got definition for Product of Experts.\n",
      "https://en.wikipedia.org/wiki/Random_multinomial_logit\n",
      "Got definition for Random multinomial logit.\n",
      "https://en.wikipedia.org/wiki/Random_subspace_method\n",
      "Got definition for Random subspace method.\n",
      "https://en.wikipedia.org/wiki/Weighted_Majority_Algorithm\n",
      "Got definition for Weighted Majority Algorithm.\n",
      "https://en.wikipedia.org/wiki/Randomized_weighted_majority_algorithm\n",
      "Got definition for Randomized weighted majority algorithm.\n",
      "https://en.wikipedia.org/wiki/Graphical_model\n",
      "Got definition for Graphical model.\n",
      "https://en.wikipedia.org/wiki/State_transition_network\n",
      "Got definition for State transition network.\n",
      "https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
      "Got definition for Naive Bayes classifier.\n",
      "https://en.wikipedia.org/wiki/Averaged_one-dependence_estimators\n",
      "Got definition for Averaged one-dependence estimators.\n",
      "https://en.wikipedia.org/wiki/Bayesian_network\n",
      "Got definition for Bayesian network.\n",
      "https://en.wikipedia.org/wiki/Variational_message_passing\n",
      "Got definition for Variational message passing.\n",
      "https://en.wikipedia.org/wiki/Markov_model\n",
      "Got definition for Markov model.\n",
      "https://en.wikipedia.org/wiki/Maximum-entropy_Markov_model\n",
      "Got definition for Maximum-entropy Markov model.\n",
      "https://en.wikipedia.org/wiki/Hidden_Markov_model\n",
      "Got definition for Hidden Markov model.\n",
      "https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm\n",
      "Got definition for Baum–Welch algorithm.\n",
      "https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm\n",
      "Got definition for Forward–backward algorithm.\n",
      "https://en.wikipedia.org/wiki/Hierarchical_hidden_Markov_model\n",
      "Got definition for Hierarchical hidden Markov model.\n",
      "https://en.wikipedia.org/wiki/Markov_logic_network\n",
      "Got definition for Markov logic network.\n",
      "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\n",
      "Got definition for Markov chain Monte Carlo.\n",
      "https://en.wikipedia.org/wiki/Markov_random_field\n",
      "Got definition for Markov random field.\n",
      "https://en.wikipedia.org/wiki/Conditional_random_field\n",
      "Got definition for Conditional random field.\n",
      "https://en.wikipedia.org/wiki/Predictive_state_representation\n",
      "Got definition for Predictive state representation.\n",
      "https://en.wikipedia.org/wiki/Computational_learning_theory\n",
      "Got definition for Computational learning theory.\n",
      "https://en.wikipedia.org/wiki/Version_space\n",
      "Got definition for Version space.\n",
      "https://en.wikipedia.org/wiki/Probably_approximately_correct_learning\n",
      "Got definition for Probably approximately correct learning.\n",
      "https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory\n",
      "Got definition for Vapnik–Chervonenkis theory.\n",
      "https://en.wikipedia.org/wiki/Shattering_(machine_learning)\n",
      "Got definition for Shattering (machine learning).\n",
      "https://en.wikipedia.org/wiki/VC_dimension\n",
      "Got definition for VC dimension.\n",
      "https://en.wikipedia.org/wiki/Minimum_description_length\n",
      "Got definition for Minimum description length.\n",
      "https://en.wikipedia.org/wiki/Bondy%27s_theorem\n",
      "Got definition for Bondy's theorem.\n",
      "https://en.wikipedia.org/wiki/Inferential_theory_of_learning\n",
      "Got definition for Inferential theory of learning.\n",
      "https://en.wikipedia.org/wiki/Rademacher_complexity\n",
      "Got definition for Rademacher complexity.\n",
      "https://en.wikipedia.org/wiki/Teaching_dimension\n",
      "Got definition for Teaching dimension.\n",
      "https://en.wikipedia.org/wiki/Subclass_reachability\n",
      "Got definition for Subclass reachability.\n",
      "https://en.wikipedia.org/wiki/Sample_exclusion_dimension\n",
      "Got definition for Sample exclusion dimension.\n",
      "https://en.wikipedia.org/wiki/Unique_negative_dimension\n",
      "Got definition for Unique negative dimension.\n",
      "https://en.wikipedia.org/wiki/Uniform_convergence_(combinatorics)\n",
      "Got definition for Uniform convergence (combinatorics).\n",
      "https://en.wikipedia.org/wiki/Witness_set\n",
      "Got definition for Witness set.\n",
      "https://en.wikipedia.org/wiki/Kernel_methods\n",
      "Got definition for Kernel methods.\n",
      "https://en.wikipedia.org/wiki/Support_vector_machine\n",
      "Got definition for Support vector machine.\n",
      "https://en.wikipedia.org/wiki/Structural_risk_minimization\n",
      "Got definition for Structural risk minimization.\n",
      "https://en.wikipedia.org/wiki/Empirical_risk_minimization\n",
      "Got definition for Empirical risk minimization.\n",
      "https://en.wikipedia.org/wiki/Kernel_trick\n",
      "Got definition for Kernel trick.\n",
      "https://en.wikipedia.org/wiki/Least_squares_support_vector_machine\n",
      "Got definition for Least squares support vector machine.\n",
      "https://en.wikipedia.org/wiki/Relevance_vector_machine\n",
      "Got definition for Relevance vector machine.\n",
      "https://en.wikipedia.org/wiki/Sequential_minimal_optimization\n",
      "Got definition for Sequential minimal optimization.\n",
      "https://en.wikipedia.org/wiki/Structured_SVM\n",
      "Got definition for Structured SVM.\n",
      "https://en.wikipedia.org/wiki/Outline_of_regression_analysis\n",
      "Got definition for Outline of regression analysis.\n",
      "https://en.wikipedia.org/wiki/Regression_analysis\n",
      "Got definition for Regression analysis.\n",
      "https://en.wikipedia.org/wiki/Dependent_and_independent_variables\n",
      "Got definition for Dependent and independent variables.\n",
      "https://en.wikipedia.org/wiki/Linear_model\n",
      "Got definition for Linear model.\n",
      "https://en.wikipedia.org/wiki/Linear_regression\n",
      "Got definition for Linear regression.\n",
      "https://en.wikipedia.org/wiki/Least_squares\n",
      "Got definition for Least squares.\n",
      "https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)\n",
      "Got definition for Linear least squares (mathematics).\n",
      "https://en.wikipedia.org/wiki/Local_regression\n",
      "Got definition for Local regression.\n",
      "https://en.wikipedia.org/wiki/Additive_model\n",
      "Got definition for Additive model.\n",
      "https://en.wikipedia.org/wiki/Antecedent_variable\n",
      "Got definition for Antecedent variable.\n",
      "https://en.wikipedia.org/wiki/Autocorrelation\n",
      "Got definition for Autocorrelation.\n",
      "https://en.wikipedia.org/wiki/Backfitting_algorithm\n",
      "Got definition for Backfitting algorithm.\n",
      "https://en.wikipedia.org/wiki/Bayesian_linear_regression\n",
      "Got definition for Bayesian linear regression.\n",
      "https://en.wikipedia.org/wiki/Bayesian_multivariate_linear_regression\n",
      "Got definition for Bayesian multivariate linear regression.\n",
      "https://en.wikipedia.org/wiki/Binomial_regression\n",
      "Got definition for Binomial regression.\n",
      "https://en.wikipedia.org/wiki/Canonical_analysis\n",
      "Got definition for Canonical analysis.\n",
      "https://en.wikipedia.org/wiki/Censored_regression_model\n",
      "Got definition for Censored regression model.\n",
      "https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
      "Got definition for Coefficient of determination.\n",
      "https://en.wikipedia.org/wiki/Comparison_of_general_and_generalized_linear_models\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a85e17f3035e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed_links\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlink_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_definition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-ff4d655467df>\u001b[0m in \u001b[0;36madd_definition\u001b[0;34m(link_dict)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mp_definition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#             print(p_definition)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mdefinition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_definition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mlink_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'definition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefinition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got definition for {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "for link_dict in parsed_links:\n",
    "    link_dict = add_definition(link_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'definition': 'Machine learning is a field of computer science that often uses statistical techniques to give computers the ability to \"learn\" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.[1]',\n",
       "  'href': '/wiki/Machine_learning',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Machine learning'},\n",
       " {'definition': 'Numerical analysis\\xa0· Simulation',\n",
       "  'href': '/wiki/Data_analysis',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Data analysis'},\n",
       " {'definition': 'Occam\\'s razor (also Ockham\\'s razor or Ocham\\'s razor; Latin: lex parsimoniae \"law of parsimony\") is the problem-solving principle that, when presented with competing hypothetical answers to a problem, one should select the answer that makes the fewest assumptions. The idea is attributed to William of Ockham (c. 1287–1347), who was an English Franciscan friar, scholastic philosopher, and theologian.',\n",
       "  'href': '/wiki/Occam%27s_razor',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': \"Occam's razor\"},\n",
       " {'definition': 'The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The expression was coined by Richard E. Bellman when considering problems in dynamic optimization.[1][2]',\n",
       "  'href': '/wiki/Curse_of_dimensionality',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Curse of dimensionality'},\n",
       " {'definition': 'In mathematical folklore, the \"no free lunch\" (NFL) theorem (sometimes pluralized) of David Wolpert and William Macready appears in the 1997 \"No Free Lunch Theorems for Optimization\".[1] Wolpert had previously derived no free lunch theorems for machine learning (statistical inference).[2]',\n",
       "  'href': '/wiki/No_free_lunch_theorem',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'No free lunch theorem'},\n",
       " {'definition': 'The accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as precision and recall.',\n",
       "  'href': '/wiki/Accuracy_paradox',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Accuracy paradox'},\n",
       " {'definition': 'In statistics, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably\".[1] An overfitted model is a statistical model that contains more parameters than can be justified by the data.[2] The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the noise) as if that variation represented underlying model structure.[3]:45',\n",
       "  'href': '/wiki/Overfitting',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Overfitting'},\n",
       " {'definition': 'In mathematics, statistics, and computer science, particularly in the fields of machine learning and inverse problems, regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.[1]',\n",
       "  'href': '/wiki/Regularization_(machine_learning)',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Regularization (machine learning)'},\n",
       " {'definition': 'The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered.[1]',\n",
       "  'href': '/wiki/Inductive_bias',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Inductive bias'},\n",
       " {'definition': 'Data dredging (also data fishing, data snooping, and p-hacking) is the use of data mining to uncover patterns in data that can be presented as statistically significant, without first devising a specific hypothesis as to the underlying causality.',\n",
       "  'href': '/wiki/Data_dredging',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Data dredging'},\n",
       " {'definition': 'The Ugly Duckling theorem is an argument asserting that classification is impossible without some sort of bias. More particularly, it assumes finitely many properties combinable by logical connectives, and finitely many objects; it asserts that any two different objects share the same number of (extensional) properties. The theorem is named after Hans Christian Andersen\\'s story \"The Ugly Duckling\", because it shows that a duckling is just as similar to a swan as two duckling are to each other. It was proposed by Satosi Watanabe in 1969.[1]:376–377',\n",
       "  'href': '/wiki/Ugly_duckling_theorem',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Ugly duckling theorem'},\n",
       " {'definition': 'In computer science, uncertain data is data that contains noise that makes it deviate from the correct, intended or original values. In the age of big data, uncertainty or data veracity is one of the defining characteristics of data. Data is constantly growing in volume, variety, velocity and uncertainty (1/veracity). Uncertain data is found in abundance today on the web, in sensor networks, within enterprises both in their structured and unstructured sources. For example, there may be uncertainty regarding the address of a customer in an enterprise dataset, or the temperature readings captured by a sensor due to aging of the sensor. In 2012 IBM called out managing uncertain data at scale in its global technology outlook report[1] that presents a comprehensive analysis looking three to ten years into the future seeking to identify significant, disruptive technologies that will change the world. In order to make confident business decisions based on real-world data, analyses must necessarily account for many different kinds of uncertainty present in very large amounts of data. Analyses based on uncertain data will have an effect on the quality of subsequent decisions, so the degree and types of inaccuracies in this uncertain data cannot be ignored.',\n",
       "  'href': '/wiki/Uncertain_data',\n",
       "  'section': 'Introduction and Main Principles',\n",
       "  'title': 'Uncertain data'},\n",
       " {'definition': 'Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criteria is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.',\n",
       "  'href': '/wiki/Knowledge_discovery',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Knowledge discovery'},\n",
       " {'definition': 'Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] It is an essential process where intelligent methods are applied to extract data patterns.[1][2] It is an interdisciplinary subfield of computer science.[1][3][4] The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.[1] Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5]',\n",
       "  'href': '/wiki/Data_mining',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Data mining'},\n",
       " {'definition': 'Predictive analytics encompasses a variety of statistical techniques from predictive modelling, machine learning, and data mining that analyze current and historical facts to make predictions about future or otherwise unknown events.[1][2]',\n",
       "  'href': '/wiki/Predictive_analytics',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Predictive analytics'},\n",
       " {'definition': 'Predictive modelling uses statistics to predict outcomes.[1] Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place.[2]',\n",
       "  'href': '/wiki/Predictive_modelling',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Predictive modelling'},\n",
       " {'definition': 'Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]',\n",
       "  'href': '/wiki/Business_intelligence',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Business intelligence'},\n",
       " {'definition': 'LIONsolver is an integrated software for data mining, business intelligence, analytics, and modeling Learning and Intelligent OptimizatioN[1] and reactive business intelligence approach.[2] A non-profit version is available as LIONoso.',\n",
       "  'href': '/wiki/Reactive_business_intelligence',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Reactive business intelligence'},\n",
       " {'definition': 'Business analytics (BA) refers to the skills, technologies, practices for continuous iterative exploration and investigation of past business performance to gain insight and drive business planning.[1] Business analytics focuses on developing new insights and understanding of business performance based on data and statistical methods. In contrast, business intelligence traditionally focuses on using a consistent set of metrics to both measure past performance and guide business planning, which is also based on data and statistical methods.[citation needed]',\n",
       "  'href': '/wiki/Business_analytics',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Business analytics'},\n",
       " {'definition': 'LIONsolver is an integrated software for data mining, business intelligence, analytics, and modeling Learning and Intelligent OptimizatioN[1] and reactive business intelligence approach.[2] A non-profit version is available as LIONoso.',\n",
       "  'href': '/wiki/Reactive_business_intelligence',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Reactive business intelligence'},\n",
       " {'definition': 'Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning.[1] Pattern recognition systems are in many cases trained from labeled \"training\" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning).',\n",
       "  'href': '/wiki/Pattern_recognition',\n",
       "  'section': 'Background and Preliminaries',\n",
       "  'title': 'Pattern recognition'},\n",
       " {'definition': 'Abductive reasoning (also called abduction,[1] abductive inference,[1] or retroduction[2]) is a form of logical inference which starts with an observation or set of observations then seeks to find the simplest and most likely explanation. In abductive reasoning, unlike in deductive reasoning, the premises do not guarantee the conclusion. One can understand abductive reasoning as inference to the best explanation,[3] although not all uses of the terms abduction and inference to the best explanation are exactly equivalent.[4][5]',\n",
       "  'href': '/wiki/Abductive_reasoning',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Abductive reasoning'},\n",
       " {'definition': 'Inductive reasoning (as opposed to deductive reasoning or abductive reasoning) is a method of reasoning in which the premises are viewed as supplying some evidence for the truth of the conclusion. While the conclusion of a deductive argument is certain, the truth of the conclusion of an inductive argument may be probable, based upon the evidence given.[1]',\n",
       "  'href': '/wiki/Inductive_reasoning',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Inductive reasoning'},\n",
       " {'definition': 'First-order logic—also known as first-order predicate calculus and predicate logic—is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form \"there exists X such that X is Socrates and X is a man\" and there exists is a quantifier while X is a variable.[1] This distinguishes it from propositional logic, which does not use quantifiers or relations.[2]',\n",
       "  'href': '/wiki/First-order_logic',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'First-order logic'},\n",
       " {'definition': 'Inductive logic programming (ILP) is a subfield of machine learning which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.',\n",
       "  'href': '/wiki/Inductive_logic_programming',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Inductive logic programming'},\n",
       " {'definition': 'In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems.',\n",
       "  'href': '/wiki/Reasoning_system',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Reasoning system'},\n",
       " {'definition': 'Case-based reasoning (CBR), broadly construed, is the process of solving new problems based on the solutions of similar past problems. An auto mechanic who fixes an engine by recalling another car that exhibited similar symptoms is using case-based reasoning. A lawyer who advocates a particular outcome in a trial based on legal precedents or a judge who creates case law is using case-based reasoning. So, too, an engineer copying working elements of nature (practicing biomimicry), is treating nature as a database of solutions to problems. Case-based reasoning is a prominent type of analogy solution making.',\n",
       "  'href': '/wiki/Case-based_reasoning',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Case-based reasoning'},\n",
       " {'definition': 'Textual case-based reasoning is a subtopic of case-based reasoning, in short CBR, a popular area in artificial intelligence. CBR suggests the ways to use past experiences to solve future similar problems, requiring that past experiences be structured in a form similar to attribute - value pairs. This leads to the investigation of textual descriptions for knowledge exploration whose output will be, in turn, used to solve similar problems.[1]',\n",
       "  'href': '/wiki/Textual_case_based_reasoning',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Textual case based reasoning'},\n",
       " {'definition': 'Causality (also referred to as causation,[1] or cause and effect) is what connects one process (the cause) with another process or state (the effect),[citation needed] where the first is partly responsible for the second, and the second is partly dependent on the first. In general, a process has many causes,[2] which are said to be causal factors for it, and all lie in its past. An effect can in turn be a cause of, or causal factor for, many other effects, which all lie in its future. Causality is metaphysically prior to notions of time and space.[3][4]',\n",
       "  'href': '/wiki/Causality',\n",
       "  'section': 'Reasoning',\n",
       "  'title': 'Causality'},\n",
       " {'definition': 'Nearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q\\xa0∈\\xa0M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points.',\n",
       "  'href': '/wiki/Nearest_neighbor_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Nearest neighbor search'},\n",
       " {'definition': 'Stochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, is a stochastic approximation of the gradient descent optimization and iterative method for minimizing an objective function that is written as a sum of differentiable functions. In essence, SGD tries to find minima or maxima by iteration.',\n",
       "  'href': '/wiki/Stochastic_gradient_descent',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Stochastic gradient descent'},\n",
       " {'definition': 'In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic which attempts to predict how close a partial solution is to a complete solution (goal state). But in beam search, only a predetermined number of best partial solutions are kept as candidates.[1] It is thus a greedy algorithm.',\n",
       "  'href': '/wiki/Beam_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Beam search'},\n",
       " {'definition': 'Best-first search is a search algorithm which explores a graph by expanding the most promising node chosen according to a specified rule.',\n",
       "  'href': '/wiki/Best-first_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Best-first search'},\n",
       " {'definition': \"Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'[1]) and explores the neighbor nodes first, before moving to the next level neighbors.\",\n",
       "  'href': '/wiki/Breadth-first_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Breadth-first search'},\n",
       " {'definition': 'In numerical analysis, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by making an incremental change to the solution. If the change produces a better solution, another incremental change is made to the new solution, and so on until no further improvements can be found.',\n",
       "  'href': '/wiki/Hill_climbing',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Hill climbing'},\n",
       " {'definition': 'In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.',\n",
       "  'href': '/wiki/Grid_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Grid search'},\n",
       " {'definition': \"In computer science, brute-force search or exhaustive search, also known as generate and test, is a very general problem-solving technique that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement.\",\n",
       "  'href': '/wiki/Brute-force_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Brute-force search'},\n",
       " {'definition': 'Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. One starts at the root (selecting some arbitrary node as the root in the case of a graph) and explores as far as possible along each branch before backtracking.',\n",
       "  'href': '/wiki/Depth-first_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Depth-first search'},\n",
       " {'definition': 'Tabu search, created by Fred W. Glover in 1986[1] and formalized in 1989,[2][3] is a metaheuristic search method employing local search methods used for mathematical optimization.',\n",
       "  'href': '/wiki/Tabu_search',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Tabu search'},\n",
       " {'definition': 'In computer science, an anytime algorithm is an algorithm that can return a valid solution to a problem even if it is interrupted before it ends. The algorithm is expected to find better and better solutions the more time it keeps running.',\n",
       "  'href': '/wiki/Anytime_algorithm',\n",
       "  'section': 'Search Methods',\n",
       "  'title': 'Anytime algorithm'},\n",
       " {'definition': 'In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA),[1] which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.',\n",
       "  'href': '/wiki/Exploratory_data_analysis',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Exploratory data analysis'},\n",
       " {'definition': 'In mathematical modeling, statistical modeling and experimental sciences, the values of dependent variables depend on the values of independent variables. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e., potential reasons for variation or, in the experimental setting, the variable controlled by the experimenter. Models and experiments test or determine the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly.',\n",
       "  'href': '/wiki/Covariate',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Covariate'},\n",
       " {'definition': 'Statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution.[1] Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.',\n",
       "  'href': '/wiki/Statistical_inference',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Statistical inference'},\n",
       " {'definition': 'Algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability (Fraser 1966). The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics, and the interest of computer scientists from the algorithms for processing data to the information they process.',\n",
       "  'href': '/wiki/Algorithmic_inference',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Algorithmic inference'},\n",
       " {'definition': 'Bayesian inference is a method of statistical inference in which Bayes\\' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called \"Bayesian probability\".',\n",
       "  'href': '/wiki/Bayesian_inference',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Bayesian inference'},\n",
       " {'definition': 'In probability and statistics, base rate generally refers to the (base) class probabilities unconditioned on featural evidence, frequently also known as prior probabilities. For example, if it were the case that 1% of the public were \"medical professionals\", and 99% of the public were not \"medical professionals\", then the base rate of medical professionals is simply 1%.',\n",
       "  'href': '/wiki/Base_rate',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Base rate'},\n",
       " {'definition': 'Statistical bias is a feature of a statistical technique or of its results whereby the expected value of the results differs from the true underlying quantitative parameter being estimated.',\n",
       "  'href': '/wiki/Bias_(statistics)',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Bias (statistics)'},\n",
       " {'definition': 'In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult. This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables). Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.',\n",
       "  'href': '/wiki/Gibbs_sampling',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Gibbs sampling'},\n",
       " {'definition': 'The cross-entropy (CE) method developed by Reuven Rubinstein is a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance sampling. The method originated from the field of rare event simulation, where very small probabilities need to be accurately estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The CE method can be applied to static and noisy combinatorial optimization problems such as the traveling salesman problem, the quadratic assignment problem, DNA sequence alignment, the max-cut problem and the buffer allocation problem, as well as continuous global optimization problems with many local extrema.',\n",
       "  'href': '/wiki/Cross-entropy_method',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Cross-entropy method'},\n",
       " {'definition': 'In statistics, latent variables (from Latin: present participle of lateo (“lie hidden”), as opposed to observable variables), are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured). Mathematical models that aim to explain observed variables in terms of latent variables are called latent variable models. Latent variable models are used in many disciplines, including psychology, demography, economics, engineering, medicine, physics, machine learning/artificial intelligence, bioinformatics, natural language processing, econometrics, management and the social sciences.',\n",
       "  'href': '/wiki/Latent_variable',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Latent variable'},\n",
       " {'definition': 'In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model, given observations. MLE attempts to find the parameter values that maximize the likelihood function, given the observations. The resulting estimate is called a maximum likelihood estimate, which is also abbreviated as MLE.',\n",
       "  'href': '/wiki/Maximum_likelihood',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Maximum likelihood'},\n",
       " {'definition': 'In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to the method of maximum likelihood (ML) estimation, but employs an augmented optimization objective which incorporates a prior distribution (that quantifies the additional information available through prior knowledge of a related event) over the quantity one wants to estimate. MAP estimation can therefore be seen as a regularization of ML estimation.',\n",
       "  'href': '/wiki/Maximum_a_posteriori_estimation',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Maximum a posteriori estimation'},\n",
       " {'definition': 'In statistics, an expectation–maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.',\n",
       "  'href': '/wiki/Expectation%E2%80%93maximization_algorithm',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Expectation–maximization algorithm'},\n",
       " {'definition': 'Expectation propagation (EP) is a technique in Bayesian machine learning.',\n",
       "  'href': '/wiki/Expectation_propagation',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Expectation propagation'},\n",
       " {'definition': 'In mathematical statistics, the Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution diverges from a second, expected probability distribution.[1][2] Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. In contrast to variation of information, it is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread. In the simple case, a Kullback–Leibler divergence of 0 indicates that we can expect similar, if not the same, behavior of two different distributions, while a Kullback–Leibler divergence of 1 indicates that the two distributions behave in such a different manner that the expectation given the first distribution approaches zero. In simplified terms, it is a measure of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning.',\n",
       "  'href': '/wiki/Kullback%E2%80%93Leibler_divergence',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Kullback–Leibler divergence'},\n",
       " {'definition': 'In statistical classification, including machine learning, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent,[a] but three major types can be distinguished, following (Jebara 2004). Given an observable variable X and a target variable Y, a generative model is a statistical model of the joint probability distribution on X\\xa0×\\xa0Y, \\n\\n\\n\\nP\\n(\\nX\\n,\\nY\\n)\\n\\n\\n{\\\\displaystyle P(X,Y)}\\n\\n;[1] a discriminative model is a model of the conditional probability of the target Y, given an observation x, symbolically, \\n\\n\\n\\nP\\n(\\nY\\n\\n|\\n\\nX\\n=\\nx\\n)\\n\\n\\n{\\\\displaystyle P(Y|X=x)}\\n\\n; and classifiers computed without using a probability model are also referred to loosely as \"discriminative\". The distinction between these last two classes is not consistently made;[2] (Jebara 2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but (Ng & Jordan 2002) only distinguishes two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes.[3] Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model. Standard examples of each, all of which are linear classifiers, are: generative classifiers: naive Bayes classifier and linear discriminant analysis; discriminative model: logistic regression; non-model classifier: perceptron and support vector machine.',\n",
       "  'href': '/wiki/Generative_model',\n",
       "  'section': 'Statistics',\n",
       "  'title': 'Generative model'},\n",
       " {'definition': 'Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias).',\n",
       "  'href': '/wiki/Supervised_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Supervised learning'},\n",
       " {'definition': 'Unsupervised machine learning is the machine learning task of inferring a function to describe hidden structure from \"unlabeled\" data (a classification or categorization is not included in the observations). Since the examples given to the learner are unlabeled, there is no evaluation of the accuracy of the structure that is output by the relevant algorithm—which is one way of distinguishing unsupervised learning from supervised learning and reinforcement learning.',\n",
       "  'href': '/wiki/Unsupervised_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Unsupervised learning'},\n",
       " {'definition': 'Active learning is a special case of semi-supervised machine learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.[1] [2] [3] In statistics literature it is sometimes also called optimal experimental design. [4]',\n",
       "  'href': '/wiki/Active_learning_(machine_learning)',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Active learning (machine learning)'},\n",
       " {'definition': 'Reinforcement learning (RL) is an area of machine learning inspired by behaviourist psychology[citation needed], concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming.[1][2] The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.[citation needed]',\n",
       "  'href': '/wiki/Reinforcement_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Reinforcement learning'},\n",
       " {'definition': 'Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[1][2][3] Early versions of MTL were called \"hints\"[4][5]',\n",
       "  'href': '/wiki/Multi-task_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Multi-task learning'},\n",
       " {'definition': 'In logic, statistical inference, and supervised learning, transduction or transductive inference is reasoning from observed, specific (training) cases to specific (test) cases. In contrast, induction is reasoning from observed training cases to general rules, which are then applied to the test cases. The distinction is most interesting in cases where the predictions of the transductive model are not achievable by any inductive model. Note that this is caused by transductive inference on different test sets producing mutually inconsistent predictions.',\n",
       "  'href': '/wiki/Transduction_(machine_learning)',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Transduction (machine learning)'},\n",
       " {'definition': 'Explanation-based learning (EBL) is a form of machine learning that exploits a very strong, or even perfect, domain theory in order to make generalizations or form concepts from training examples.[1]',\n",
       "  'href': '/wiki/Explanation-based_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Explanation-based learning'},\n",
       " {'definition': 'In machine learning, systems which employ offline learning do not change their approximation of the target function when the initial training phase has been completed.[citation needed] These systems are also typically examples of eager learning.[citation needed]',\n",
       "  'href': '/wiki/Offline_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Offline learning'},\n",
       " {'definition': 'In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g. stock price prediction. Online learning algorithms may be prone to catastrophic interference. This problem is tackled by incremental learning approaches.',\n",
       "  'href': '/wiki/Online_learning_model',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Online learning model'},\n",
       " {'definition': 'In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g. stock price prediction. Online learning algorithms may be prone to catastrophic interference. This problem is tackled by incremental learning approaches.',\n",
       "  'href': '/wiki/Online_machine_learning',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Online machine learning'},\n",
       " {'definition': 'In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.',\n",
       "  'href': '/wiki/Hyperparameter_optimization',\n",
       "  'section': 'Main Learning Paradigms',\n",
       "  'title': 'Hyperparameter optimization'},\n",
       " {'definition': 'In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition.',\n",
       "  'href': '/wiki/Classification_in_machine_learning',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Classification in machine learning'},\n",
       " {'definition': 'A concept over a domain X is a total Boolean function over X. A concept class is a class of concepts. Concept class is a subject of computational learning theory.',\n",
       "  'href': '/wiki/Concept_class',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Concept class'},\n",
       " {'definition': 'In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed.[1] Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.',\n",
       "  'href': '/wiki/Features_(pattern_recognition)',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Features (pattern recognition)'},\n",
       " {'definition': 'In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed.[1] Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.',\n",
       "  'href': '/wiki/Feature_vector',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Feature vector'},\n",
       " {'definition': 'In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed.[1] Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.',\n",
       "  'href': '/wiki/Feature_space',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Feature space'},\n",
       " {'definition': 'Concept learning, also known as category learning, concept attainment, and concept formation, is defined by Bruner, Goodnow, & Austin (1967) as \"the search for and listing of attributes that can be used to distinguish exemplars from non exemplars of various categories\". More simply put, concepts are the mental categories that help us classify objects, events, or ideas, building on the understanding that each object, event, or idea has a set of common relevant features. Thus, concept learning is a strategy which requires a learner to compare and contrast groups or categories that contain concept-relevant features with groups or categories that do not contain concept-relevant features.',\n",
       "  'href': '/wiki/Concept_learning',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Concept learning'},\n",
       " {'definition': 'Binary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule. Contexts requiring a decision as to whether or not an item has some qualitative property, some specified characteristic, or some typical binary classification include:',\n",
       "  'href': '/wiki/Binary_classification',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Binary classification'},\n",
       " {'definition': 'In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.',\n",
       "  'href': '/wiki/Decision_boundary',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Decision boundary'},\n",
       " {'definition': 'Not to be confused with multi-label classification.',\n",
       "  'href': '/wiki/Multiclass_classification',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Multiclass classification'},\n",
       " {'definition': 'In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right[1] or when combining classifiers into ensembles.',\n",
       "  'href': '/wiki/Class_membership_probabilities',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Class membership probabilities'},\n",
       " {'definition': 'There are two main uses of the term calibration in statistics that denote special types of statistical inference problems. Thus \"calibration\" can mean',\n",
       "  'href': '/wiki/Calibration_(statistics)',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Calibration (statistics)'},\n",
       " {'definition': 'In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.',\n",
       "  'href': '/wiki/Concept_drift',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Concept drift'},\n",
       " {'definition': 'Pattern recognition is a very active field of research intimately bound to machine learning. Also known as classification or statistical classification, pattern recognition aims at building a classifier that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs \\n\\n\\n\\n(\\n\\n\\nx\\n\\n\\ni\\n\\n\\n,\\n\\ny\\n\\ni\\n\\n\\n)\\n\\n\\n{\\\\displaystyle ({\\\\boldsymbol {x}}_{i},y_{i})}\\n\\n that form the training data (or training set). Nonetheless, in real world applications such as character recognition, a certain amount of information on the problem is usually known beforehand. The incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications.',\n",
       "  'href': '/wiki/Prior_knowledge_for_pattern_recognition',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Prior knowledge for pattern recognition'},\n",
       " {'definition': 'The Iris flower data set or Fisher\\'s Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.[1] It is sometimes called Anderson\\'s Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species.[2] Two of the three species were collected in the Gaspé Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".[3]',\n",
       "  'href': '/wiki/Iris_flower_data_set',\n",
       "  'section': 'Classification Tasks',\n",
       "  'title': 'Iris flower data set'},\n",
       " {'definition': 'Margin-infused relaxed algorithm (MIRA)[1] is a machine learning algorithm, an online algorithm for multiclass classification problems. It is designed to learn a set of parameters (vector or matrix) by processing all the given training examples one-by-one and updating the parameters according to each training example, so that the current training example is classified correctly with a margin against incorrect classifications at least as large as their loss.[2] The change of the parameters is kept as small as possible.',\n",
       "  'href': '/wiki/Margin_Infused_Relaxed_Algorithm',\n",
       "  'section': 'Online Learning',\n",
       "  'title': 'Margin Infused Relaxed Algorithm'},\n",
       " {'definition': 'Semi-supervised learning is a class of supervised learning tasks and techniques that also make use of unlabeled data for training – typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.',\n",
       "  'href': '/wiki/Semi-supervised_learning',\n",
       "  'section': 'Semi-supervised learning',\n",
       "  'title': 'Semi-supervised learning'},\n",
       " {'definition': \"In machine learning, one-class classification, also known as unary classification or class-modelling, tries to identify objects of a specific class amongst all objects, by learning from a training set containing only the objects of that class[1]. This is different from and more difficult than the traditional classification problem, which tries to distinguish between two or more classes with the training set containing objects from all the classes. An example is the classification of the operational status of a nuclear plant as 'normal':[2] In this scenario, there are few, if any, examples of catastrophic system states; only the statistics of normal operation are known. The term one-class classification was coined by Moya & Hush (1996)[3] and many applications can be found in scientific literature, for example outlier detection, anomaly detection, novelty detection. A feature of one-class classification is that it uses only sample points from the assigned class, so that a representative sampling is not strictly required for non-target classes.[4]\",\n",
       "  'href': '/wiki/One-class_classification',\n",
       "  'section': 'Semi-supervised learning',\n",
       "  'title': 'One-class classification'},\n",
       " {'definition': 'Coupled Pattern Learner (CPL) is a machine learning algorithm which couples the semi-supervised learning of categories and relations to forestall the problem of semantic drift associated with boot-strap learning methods.',\n",
       "  'href': '/wiki/Coupled_pattern_learner',\n",
       "  'section': 'Semi-supervised learning',\n",
       "  'title': 'Coupled pattern learner'},\n",
       " {'definition': 'In machine learning, lazy learning is a learning method in which generalization of the training data is delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries.',\n",
       "  'href': '/wiki/Lazy_learning',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Lazy learning'},\n",
       " {'definition': 'In artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system. [1] The main advantage gained in employing an eager learning method, such as an artificial neural network, is that the target function will be approximated globally during training, thus requiring much less space than using a lazy learning system. Eager learning systems also deal much better with noise in the training data. Eager learning is an example of offline learning, in which post-training queries to the system have no effect on the system itself, and thus the same query to the system will always produce the same result.',\n",
       "  'href': '/wiki/Eager_learning',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Eager learning'},\n",
       " {'definition': 'In machine learning, instance-based learning (sometimes called memory-based learning[1]) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.',\n",
       "  'href': '/wiki/Instance-based_learning',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Instance-based learning'},\n",
       " {'definition': 'In machine learning and information retrieval, the cluster hypothesis is an assumption about the nature of the data handled in those fields, which takes various forms. In information retrieval, it states that documents that are clustered together \"behave similarly with respect to relevance to information needs\".[1] In terms of classification, it states that if points are in the same cluster, they are likely to be of the same class.[2] There may be multiple clusters forming a single class.',\n",
       "  'href': '/wiki/Cluster_assumption',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Cluster assumption'},\n",
       " {'definition': 'In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:',\n",
       "  'href': '/wiki/K-nearest_neighbor_algorithm',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'K-nearest neighbor algorithm'},\n",
       " {'definition': 'In pattern recognition, the iDistance is an indexing and query processing technique for k-nearest neighbor queries on point data in multi-dimensional metric spaces. The kNN query is one of the hardest problems on multi-dimensional data, especially when the dimensionality of the data is high. The iDistance is designed to process kNN queries in high-dimensional spaces efficiently and it is especially good for skewed data distributions, which usually occur in real-life data sets.',\n",
       "  'href': '/wiki/IDistance',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'IDistance'},\n",
       " {'definition': 'Large margin nearest neighbor (LMNN)[1] classification is a statistical machine learning algorithm for metric learning. It learns a pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization.',\n",
       "  'href': '/wiki/Large_margin_nearest_neighbor',\n",
       "  'section': 'Lazy learning and nearest neighbors',\n",
       "  'title': 'Large margin nearest neighbor'},\n",
       " {'definition': \"Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\",\n",
       "  'href': '/wiki/Decision_tree_learning',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Decision tree learning'},\n",
       " {'definition': 'A decision stump is a machine learning model consisting of a one-level decision tree.[1] That is, it is a decision tree with one internal node (the root) which is immediately connected to the terminal nodes (its leaves). A decision stump makes a prediction based on the value of just a single input feature. Sometimes they are also called 1-rules.[2]',\n",
       "  'href': '/wiki/Decision_stump',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Decision stump'},\n",
       " {'definition': 'Pruning is a technique in machine learning that reduces the size of decision trees by removing sections of the tree that provide little power to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.',\n",
       "  'href': '/wiki/Pruning_(decision_trees)',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Pruning (decision trees)'},\n",
       " {'definition': 'In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the \"amount of information\" (in units such as shannons, more commonly called bits) obtained about one random variable, through the other random variable. The concept of mutual information is intricately linked to that of entropy of a random variable, a fundamental notion in information theory, that defines the \"amount of information\" held in a random variable.',\n",
       "  'href': '/wiki/Mutual_information',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Mutual information'},\n",
       " {'definition': 'In probability theory and information theory, adjusted mutual information, a variation of mutual information may be used for comparing clusterings.[1] It corrects the effect of agreement solely due to chance between clusterings, similar to the way the adjusted rand index corrects the Rand index. It is closely related to variation of information:[2] when a similar adjustment is made to the VI index, it becomes equivalent to the AMI.[1] The adjusted measure however is no longer metrical.[3]',\n",
       "  'href': '/wiki/Adjusted_mutual_information',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Adjusted mutual information'},\n",
       " {'definition': 'In decision tree learning, Information gain ratio is a ratio of information gain to the intrinsic information. It is used to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute.[1]',\n",
       "  'href': '/wiki/Information_gain_ratio',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Information gain ratio'},\n",
       " {'definition': 'In information theory and machine learning, information gain is a synonym for Kullback–Leibler divergence. However, in the context of decision trees, the term is sometimes used synonymously with mutual information, which is the expected value of the Kullback–Leibler divergence of the univariate probability distribution of one variable from the conditional distribution of this variable given the other one.',\n",
       "  'href': '/wiki/Information_gain_in_decision_trees',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Information gain in decision trees'},\n",
       " {'definition': 'In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan[1] used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.',\n",
       "  'href': '/wiki/ID3_algorithm',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'ID3 algorithm'},\n",
       " {'definition': 'C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan.[1] C4.5 is an extension of Quinlan\\'s earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. Authors of the Weka machine learning software described the C4.5 algorithm as \"a landmark decision tree program that is probably the machine learning workhorse most widely used in practice to date\".[2]',\n",
       "  'href': '/wiki/C4.5_algorithm',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'C4.5 algorithm'},\n",
       " {'definition': 'Chi-square automatic interaction detection (CHAID) is a decision tree technique, based on adjusted significance testing (Bonferroni testing). The technique was developed in South Africa and was published in 1980 by Gordon V. Kass, who had completed a PhD thesis on this topic. CHAID can be used for prediction (in a similar fashion to regression analysis, this version of CHAID being originally known as XAID) as well as classification, and for detection of interaction between variables. CHAID is based on a formal extension of the US AID (Automatic Interaction Detection) and THAID (THeta Automatic Interaction Detection) procedures of the 1960s and 1970s, which in turn were extensions of earlier research, including that performed in the UK in the 1950s.',\n",
       "  'href': '/wiki/CHAID',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'CHAID'},\n",
       " {'definition': \"Information fuzzy networks (IFN) is a greedy machine learning algorithm for supervised learning. The data structure produced by the learning algorithm is also called Info Fuzzy Network. IFN construction is quite similar to decision trees' construction. However, IFN constructs a directed graph and not a tree. IFN also uses the conditional mutual information metric in order to choose features during the construction stage while decision trees usually use other metrics like entropy or gini.\",\n",
       "  'href': '/wiki/Information_Fuzzy_Networks',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Information Fuzzy Networks'},\n",
       " {'definition': 'Grafting is the process of adding nodes to inferred decision trees to improve the predictive accuracy.[clarification needed] A decision tree is a graphical model that is used as a support tool for decision process.',\n",
       "  'href': '/wiki/Grafting_(decision_trees)',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Grafting (decision trees)'},\n",
       " {'definition': 'An incremental decision tree algorithm is an online machine learning algorithm that outputs a decision tree. Many decision tree methods, such as C4.5, construct a tree using a complete dataset. Incremental decision tree methods allow an existing tree to be updated using only new individual data instances, without having to re-process past instances. This may be useful in situations where the entire dataset is not available when the tree is updated (i.e. the data was not stored), the original data set is too large to process or the characteristics of the data change over time.',\n",
       "  'href': '/wiki/Incremental_decision_tree',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Incremental decision tree'},\n",
       " {'definition': 'An alternating decision tree (ADTree) is a machine learning method for classification. It generalizes decision trees and has connections to boosting.',\n",
       "  'href': '/wiki/Alternating_decision_tree',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Alternating decision tree'},\n",
       " {'definition': 'In computer science, a logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning.[1][2]',\n",
       "  'href': '/wiki/Logistic_model_tree',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Logistic model tree'},\n",
       " {'definition': \"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]:587–588\",\n",
       "  'href': '/wiki/Random_forest',\n",
       "  'section': 'Decision Trees',\n",
       "  'title': 'Random forest'},\n",
       " {'definition': \"In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to. A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics. An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.[1]\",\n",
       "  'href': '/wiki/Linear_classifier',\n",
       "  'section': 'Linear Classifiers',\n",
       "  'title': 'Linear classifier'},\n",
       " {'definition': 'In machine learning the margin of a single data point is defined to be the distance from the data point to a decision boundary. Note that there are many distances and decision boundaries that may be appropriate for certain datasets and goals. A margin classifier is a classifier that explicitly utilizes the margin of each example while learning a classifier. There are theoretical justifications (based on the VC dimension) as to why maximizing the margin (under some suitable constraints) may be beneficial for machine learning and statistical inferences algorithms.',\n",
       "  'href': '/wiki/Margin_(machine_learning)',\n",
       "  'section': 'Linear Classifiers',\n",
       "  'title': 'Margin (machine learning)'},\n",
       " {'definition': 'In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. For instance, if a linear classifier (e.g. perceptron or linear discriminant analysis) is used, the distance (typically euclidean distance, though others may be used) of an example from the separating hyperplane is the margin of that example.',\n",
       "  'href': '/wiki/Margin_classifier',\n",
       "  'section': 'Linear Classifiers',\n",
       "  'title': 'Margin classifier'},\n",
       " {'definition': 'Soft independent modelling by class analogy (SIMCA) is a statistical method for supervised classification of data. The method requires a training data set consisting of samples (or objects) with a set of attributes and their class membership. The term soft refers to the fact the classifier can identify samples as belonging to multiple classes and not necessarily producing a classification of samples into non-overlapping classes.',\n",
       "  'href': '/wiki/Soft_independent_modelling_of_class_analogies',\n",
       "  'section': 'Linear Classifiers',\n",
       "  'title': 'Soft independent modelling of class analogies'},\n",
       " {'definition': 'In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition.',\n",
       "  'href': '/wiki/Statistical_classification',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Statistical classification'},\n",
       " {'definition': 'Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, then the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of \"positive\" on 60% of instances, and a class label of \"negative\" on 40% of instances.',\n",
       "  'href': '/wiki/Probability_matching',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Probability matching'},\n",
       " {'definition': 'Discriminative models, also called conditional models, are a class of models used in machine learning for modeling the dependence of unobserved (target) variables \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n on observed variables \\n\\n\\n\\nx\\n\\n\\n{\\\\displaystyle x}\\n\\n. Within a probabilistic framework, this is done by modeling the conditional probability distribution \\n\\n\\n\\nP\\n(\\ny\\n\\n|\\n\\nx\\n)\\n\\n\\n{\\\\displaystyle P(y|x)}\\n\\n, which can be used for predicting \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n from \\n\\n\\n\\nx\\n\\n\\n{\\\\displaystyle x}\\n\\n.',\n",
       "  'href': '/wiki/Discriminative_model',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Discriminative model'},\n",
       " {'definition': \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\",\n",
       "  'href': '/wiki/Linear_discriminant_analysis',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Linear discriminant analysis'},\n",
       " {'definition': \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\",\n",
       "  'href': '/wiki/Multiclass_LDA',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Multiclass LDA'},\n",
       " {'definition': 'Multiple Discriminant Analysis (MDA) is a multivariate technique used to predict corporate failure.[1]',\n",
       "  'href': '/wiki/Multiple_discriminant_analysis',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Multiple discriminant analysis'},\n",
       " {'definition': 'Optimal Discriminant Analysis (ODA) [1] and the related classification tree analysis (CTA) are exact statistical methods that maximize predictive accuracy. For any specific sample and exploratory or confirmatory hypothesis, optimal discriminant analysis (ODA) identifies the statistical model that yields maximum predictive accuracy, assesses the exact Type I error rate, and evaluates potential cross-generalizability. Optimal discriminant analysis may be applied to >\\xa00 dimensions, with the one-dimensional case being referred to as UniODA and the multidimensional case being referred to as MultiODA. Classification tree analysis is a generalization of optimal discriminant analysis to non-orthogonal trees. Classification tree analysis has more recently been called \"hierarchical optimal discriminant analysis\". Optimal discriminant analysis and classification tree analysis may be used to find the combination of variables and cut points that best separate classes of objects or events. These variables and cut points may then be used to reduce dimensions and to then build a statistical model that optimally describes the data.',\n",
       "  'href': '/wiki/Optimal_discriminant_analysis',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Optimal discriminant analysis'},\n",
       " {'definition': 'In statistical classification, the Fisher kernel, named after Ronald Fisher, is a function that measures the similarity of two objects on the basis of sets of measurements for each object and a statistical model. In a classification procedure, the class for a new object (whose real class is unknown) can be estimated by minimising, across classes, an average of the Fisher kernel distance from the new object to each known member of the given class.',\n",
       "  'href': '/wiki/Fisher_kernel',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Fisher kernel'},\n",
       " {'definition': \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\",\n",
       "  'href': '/wiki/Discriminant_function_analysis',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Discriminant function analysis'},\n",
       " {'definition': 'Multilinear subspace learning is an approach to dimensionality reduction.[1][2][3][4][5] Dimensionality reduction can be performed on a data tensor whose observations have been vectorized[1] and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor.[6][7] Here are some examples of data tensors whose observations are vectorized or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).',\n",
       "  'href': '/wiki/Multilinear_subspace_learning',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Multilinear subspace learning'},\n",
       " {'definition': 'A quadratic classifier is used in machine learning and statistical classification to separate measurements of two or more classes of objects or events by a quadric surface. It is a more general version of the linear classifier.',\n",
       "  'href': '/wiki/Quadratic_classifier',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Quadratic classifier'},\n",
       " {'definition': 'In statistics, adaptive or \"variable-bandwidth\" kernel density estimation is a form of kernel density estimation in which the size of the kernels used in the estimate are varied depending upon either the location of the samples or the location of the test point. It is a particularly effective technique when the sample space is multi-dimensional. [1]',\n",
       "  'href': '/wiki/Variable_kernel_density_estimation',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Variable kernel density estimation'},\n",
       " {'definition': 'Category utility is a measure of \"category goodness\" defined in Gluck & Corter (1985) and Corter & Gluck (1992). It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as \"cue validity\" (Reed 1972; Rosch & Mervis 1975) and \"collocation index\" (Jones 1983). It provides a normative information-theoretic measure of the predictive advantage gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does not possess knowledge of the category structure. In this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of category utility in its probabilistic incarnation, with applications to machine learning, is provided in Witten & Frank (2005, pp.\\xa0260–262).',\n",
       "  'href': '/wiki/Category_utility',\n",
       "  'section': 'Statistical classification',\n",
       "  'title': 'Category utility'},\n",
       " {'definition': 'In business intelligence, data classification has close ties to data clustering, but where data clustering is descriptive, data classification is predictive.[1][2] In essence data classification consists of using variables with known values to predict the unknown or future values of other variables. It can be used in e.g. direct marketing, insurance fraud detection or medical diagnosis.[2]',\n",
       "  'href': '/wiki/Data_classification_(business_intelligence)',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Data classification (business intelligence)'},\n",
       " {'definition': 'In machine learning, the study and construction of algorithms that can learn from and make predictions on data[1] is a common task. Such algorithms work by making data-driven predictions or decisions,[2]:2 through building a mathematical model from input data.',\n",
       "  'href': '/wiki/Training_set',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Training set'},\n",
       " {'definition': 'In machine learning, the study and construction of algorithms that can learn from and make predictions on data[1] is a common task. Such algorithms work by making data-driven predictions or decisions,[2]:2 through building a mathematical model from input data.',\n",
       "  'href': '/wiki/Test_set',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Test set'},\n",
       " {'definition': 'Synthetic data is \"any production data applicable to a given situation that are not obtained by direct measurement\" according to the McGraw-Hill Dictionary of Scientific and Technical Terms;[1] where Craig S. Mullins, an expert in data management, defines production data as \"information that is persistently stored and used by professionals to conduct business processes.\".[2]',\n",
       "  'href': '/wiki/Synthetic_data',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Synthetic data'},\n",
       " {'definition': 'Cross-validation, sometimes called rotation estimation,[1][2][3] or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).[4] The goal of cross-validation is to test the model’s ability to predict new data that were not used in estimating it, in order to flag problems like overfitting[citation needed] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).',\n",
       "  'href': '/wiki/Cross-validation_(statistics)',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Cross-validation (statistics)'},\n",
       " {'definition': 'In mathematical optimization, statistics, econometrics, decision theory, machine learning and computational neuroscience, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its negative (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized.',\n",
       "  'href': '/wiki/Loss_function',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Loss function'},\n",
       " {'definition': 'In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs).[1] For an intended output t = ±1 and a classifier score y, the hinge loss of the prediction y is defined as',\n",
       "  'href': '/wiki/Hinge_loss',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Hinge loss'},\n",
       " {'definition': 'In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error[1]) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data. Because learning algorithms are evaluated on finite samples, the evaluation of a learning algorithm may be sensitive to sampling error. As a result, measurements of prediction error on the current data may not provide much information about predictive ability on new data. Generalization error can be minimized by avoiding overfitting in the learning algorithm. The performance of a machine learning algorithm is measured by plots of the generalization error values through the learning process, which are called learning curves.',\n",
       "  'href': '/wiki/Generalization_error',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Generalization error'},\n",
       " {'definition': 'In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis (also known as a \"false positive\" finding), while a type II error is retaining a false null hypothesis (also known as a \"false negative\" finding).[1] More simply stated, a type I error is to falsely infer the existence of something that is not there, while a type II error is to falsely infer the absence of something that is.',\n",
       "  'href': '/wiki/Type_I_and_type_II_errors',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Type I and type II errors'},\n",
       " {'definition': 'Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as a classification function:',\n",
       "  'href': '/wiki/Sensitivity_and_specificity',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Sensitivity and specificity'},\n",
       " {'definition': 'In pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Both precision and recall are therefore based on an understanding and measure of relevance.',\n",
       "  'href': '/wiki/Precision_and_recall',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Precision and recall'},\n",
       " {'definition': \"In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\",\n",
       "  'href': '/wiki/F1_score',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'F1 score'},\n",
       " {'definition': 'Sources: Fawcett (2006), Powers (2011), and Ting (2011) [1] [2] [3]',\n",
       "  'href': '/wiki/Confusion_matrix',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Confusion matrix'},\n",
       " {'definition': 'The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975.[1] It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes.[2] The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation. The statistic is also known as the phi coefficient. MCC is related to the chi-square statistic for a 2×2 contingency table',\n",
       "  'href': '/wiki/Matthews_correlation_coefficient',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Matthews correlation coefficient'},\n",
       " {'definition': 'In statistics, a receiver operating characteristic curve, i.e. ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.',\n",
       "  'href': '/wiki/Receiver_operating_characteristic',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Receiver operating characteristic'},\n",
       " {'definition': 'In data mining and association rule learning, lift is a measure of the performance of a targeting model (association rule) at predicting or classifying cases as having an enhanced response (with respect to the population as a whole), measured against a random choice targeting model. A targeting model is doing a good job if the response within the target is much better than the average for the population as a whole. Lift is simply the ratio of these values: target response divided by average response.',\n",
       "  'href': '/wiki/Lift_(data_mining)',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Lift (data mining)'},\n",
       " {'definition': 'Stability, also known as algorithmic stability, is a notion in computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs. A stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly. For instance, consider a machine learning algorithm that is being trained to recognize handwritten letters of the alphabet, using 1000 examples of handwritten letters and their labels (\"A\" to \"Z\") as a training set. One way to modify this training set is to leave out an example, so that only 999 examples of handwritten letters and their labels are available. A stable learning algorithm would produce a similar classifier with both the 1000-element and 999-element training sets.',\n",
       "  'href': '/wiki/Stability_in_learning',\n",
       "  'section': 'Evaluation of Classification Models',\n",
       "  'title': 'Stability in learning'},\n",
       " {'definition': 'Data pre-processing is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis.[1] Often, data pre-processing is the most important phase of a machine learning project, especially in computational biology.[2]',\n",
       "  'href': '/wiki/Data_Pre-processing',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Data Pre-processing'},\n",
       " {'definition': 'In statistics and machine learning, discretization refers to the process of converting or partitioning continuous attributes, features or variables to discretized or nominal attributes/features/variables/intervals. This can be useful when creating probability mass functions – formally, in density estimation. It is a form of discretization in general and also of binning, as in making a histogram. Whenever continuous data is discretized, there is always some amount of discretization error. The goal is to reduce the amount to a level considered negligible for the modeling purposes at hand.',\n",
       "  'href': '/wiki/Discretization_of_continuous_features',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Discretization of continuous features'},\n",
       " {'definition': 'Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated feature learning.',\n",
       "  'href': '/wiki/Feature_engineering',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Feature engineering'},\n",
       " {'definition': 'In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:',\n",
       "  'href': '/wiki/Feature_selection',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Feature selection'},\n",
       " {'definition': 'In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.',\n",
       "  'href': '/wiki/Feature_extraction',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Feature extraction'},\n",
       " {'definition': 'In statistics, machine learning, and information theory, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration[1] by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.[2]',\n",
       "  'href': '/wiki/Dimension_reduction',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Dimension reduction'},\n",
       " {'definition': 'Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables[clarification needed] into a set of values of linearly uncorrelated variables called principal components. If there are \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n observations with \\n\\n\\n\\np\\n\\n\\n{\\\\displaystyle p}\\n\\n variables, then the number of distinct principal components is \\n\\n\\n\\nmin\\n(\\nn\\n−\\n1\\n,\\np\\n)\\n\\n\\n{\\\\displaystyle \\\\min(n-1,p)}\\n\\n. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors[clarification needed] are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.',\n",
       "  'href': '/wiki/Principal_component_analysis',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Principal component analysis'},\n",
       " {'definition': 'Multilinear principal component analysis (MPCA) is a multilinear extension of principal component analysis (PCA). MPCA is employed in the analysis of n-way arrays, i.e. a cube or hyper-cube of numbers, also informally referred to as a \"data tensor\". N-way arrays may be decomposed, analyzed, or modeled by',\n",
       "  'href': '/wiki/Multilinear_principal-component_analysis',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Multilinear principal-component analysis'},\n",
       " {'definition': 'Multifactor dimensionality reduction (MDR) is a statistical approach, also used in machine learning automatic approaches[1], for detecting and characterizing combinations of attributes or independent variables that interact to influence a dependent or class variable.[2][3][4][5][6][7][8] MDR was designed specifically to identify nonadditive interactions among discrete variables that influence a binary outcome and is considered a nonparametric and model-free alternative to traditional statistical methods such as logistic regression.',\n",
       "  'href': '/wiki/Multifactor_dimensionality_reduction',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Multifactor dimensionality reduction'},\n",
       " {'definition': 'Targeted projection pursuit is a type of statistical technique used for exploratory data analysis, information visualization, and feature selection. It allows the user to interactively explore very complex data (typically having tens to hundreds of attributes) to find features or patterns of potential interest.',\n",
       "  'href': '/wiki/Targeted_projection_pursuit',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Targeted projection pursuit'},\n",
       " {'definition': 'Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset. It refers to a set of related ordination techniques used in information visualization, in particular to display the information contained in a distance matrix. It is a form of non-linear dimensionality reduction. An MDS algorithm aims to place each object in N-dimensional space such that the between-object distances are preserved as well as possible. Each object is then assigned coordinates in each of the N dimensions. The number of dimensions of an MDS plot N can exceed 2 and is specified a priori. Choosing N=2 optimizes the object locations for a two-dimensional scatterplot.[1]',\n",
       "  'href': '/wiki/Multidimensional_scaling',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Multidimensional scaling'},\n",
       " {'definition': 'High-dimensional data, meaning data that requires more than two or three dimensions to represent, can be difficult to interpret. One approach to simplification is to assume that the data of interest lie on an embedded non-linear manifold within the higher-dimensional space. If the manifold is of low enough dimension, the data can be visualised in the low-dimensional space.',\n",
       "  'href': '/wiki/Nonlinear_dimensionality_reduction',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Nonlinear dimensionality reduction'},\n",
       " {'definition': 'In the field of multivariate statistics, kernel principal component analysis (kernel PCA) [1] is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.',\n",
       "  'href': '/wiki/Kernel_principal_component_analysis',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Kernel principal component analysis'},\n",
       " {'definition': 'Speaker adaptation is an important technology to fine-tune either features or speech models for mis-match due to inter-speaker variation. In the last decade, eigenvoice (EV) speaker adaptation has been developed. It makes use of the prior knowledge of training speakers to provide a fast adaptation algorithm (in other words, only a small amount of adaptation data is needed). Inspired by the kernel eigenface idea in face recognition, kernel eigenvoice (KEV) is proposed.[1] KEV is a non-linear generalization to EV. This incorporates Kernel principal component analysis, a non-linear version of Principal Component Analysis, to capture the higher order correlations in order to further explore the speaker space and enhance recognition performance.',\n",
       "  'href': '/wiki/Kernel_eigenvoice',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Kernel eigenvoice'},\n",
       " {'definition': 'In linear algebra, the Gram matrix (Gramian matrix or Gramian) of a set of vectors \\n\\n\\n\\n\\nv\\n\\n1\\n\\n\\n,\\n…\\n,\\n\\nv\\n\\nn\\n\\n\\n\\n\\n{\\\\displaystyle v_{1},\\\\dots ,v_{n}}\\n\\n in an inner product space is the Hermitian matrix of inner products, whose entries are given by \\n\\n\\n\\n\\nG\\n\\ni\\nj\\n\\n\\n=\\n⟨\\n\\nv\\n\\ni\\n\\n\\n,\\n\\nv\\n\\nj\\n\\n\\n⟩\\n\\n\\n{\\\\displaystyle G_{ij}=\\\\langle v_{i},v_{j}\\\\rangle }\\n\\n.[1]',\n",
       "  'href': '/wiki/Gramian_matrix',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Gramian matrix'},\n",
       " {'definition': 'In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.',\n",
       "  'href': '/wiki/Gaussian_process',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Gaussian process'},\n",
       " {'definition': 'In signal processing, a kernel adaptive filter is a type of nonlinear adaptive filter.[1] An adaptive filter is a filter that adapts its transfer function to changes in signal properties over time by minimizing an error or loss function that characterizes how far the filter deviates from ideal behavior. The adaptation process is based on learning from a sequence of signal samples and is thus an online algorithm. A nonlinear adaptive filter is one in which the transfer function is nonlinear.',\n",
       "  'href': '/wiki/Kernel_adaptive_filter',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Kernel adaptive filter'},\n",
       " {'definition': 'Isomap is a nonlinear dimensionality reduction method. It is one of several widely used low-dimensional embedding methods.[1] Isomap is used for computing a quasi-isometric, low-dimensional embedding of a set of high-dimensional data points. The algorithm provides a simple method for estimating the intrinsic geometry of a data manifold based on a rough estimate of each data point’s neighbors on the manifold. Isomap is highly efficient and generally applicable to a broad range of data sources and dimensionalities.',\n",
       "  'href': '/wiki/Isomap',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Isomap'},\n",
       " {'definition': 'Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003,[1] adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors.[2]',\n",
       "  'href': '/wiki/Manifold_alignment',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Manifold alignment'},\n",
       " {'definition': 'Diffusion maps is a dimensionality reduction or feature extraction algorithm introduced by Coifman and Lafon[1][2][3][4] which computes a family of embeddings of a data set into Euclidean space (often low-dimensional) whose coordinates can be computed from the eigenvectors and eigenvalues of a diffusion operator on the data. The Euclidean distance between points in the embedded space is equal to the \"diffusion distance\" between probability distributions centered at those points. Different from linear dimensionality reduction methods such as principal component analysis (PCA) and multi-dimensional scaling (MDS), diffusion maps is part of the family of nonlinear dimensionality reduction methods which focus on discovering the underlying manifold that the data has been sampled from. By integrating local similarities at different scales, diffusion maps gives a global description of the data-set. Compared with other methods, the diffusion maps algorithm is robust to noise perturbation and is computationally inexpensive.',\n",
       "  'href': '/wiki/Diffusion_map',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Diffusion map'},\n",
       " {'definition': 'Elastic maps provide a tool for nonlinear dimensionality reduction. By their construction, they are a system of elastic springs embedded in the data space.[1] This system approximates a low-dimensional manifold. The elastic coefficients of this system allow the switch from completely unstructured k-means clustering (zero elasticity) to the estimators located closely to linear PCA manifolds (for high bending and low stretching modules). With some intermediate values of the elasticity coefficients, this system effectively approximates non-linear principal manifolds. This approach is based on a mechanical analogy between principal manifolds, that are passing through \"the middle\" of the data distribution, and elastic membranes and plates. The method was developed by A.N. Gorban, A.Y. Zinovyev and A.A. Pitenko in 1996–1998.',\n",
       "  'href': '/wiki/Elastic_map',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Elastic map'},\n",
       " {'definition': 'Locality-sensitive hashing (LSH) reduces the dimensionality of high-dimensional data. LSH hashes input items so that similar items map to the same “buckets” with high probability (the number of buckets being much smaller than the universe of possible input items). LSH differs from conventional and cryptographic hash functions because it aims to maximize the probability of a “collision” for similar items.[1] Locality-sensitive hashing has much in common with data clustering and nearest neighbor search.',\n",
       "  'href': '/wiki/Locality-sensitive_hashing',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Locality-sensitive hashing'},\n",
       " {'definition': 'In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.',\n",
       "  'href': '/wiki/Spectral_clustering',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Spectral clustering'},\n",
       " {'definition': 'Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes and narrow down their relevance and is usually described in its pairing with relevant feature selection as Minimum Redundancy Maximum Relevance (mRMR).',\n",
       "  'href': '/wiki/Minimum_redundancy_feature_selection',\n",
       "  'section': 'Feature Creation and Optimization',\n",
       "  'title': 'Minimum redundancy feature selection'},\n",
       " {'definition': 'Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.',\n",
       "  'href': '/wiki/Cluster_analysis',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Cluster analysis'},\n",
       " {'definition': 'k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.',\n",
       "  'href': '/wiki/K-means_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'K-means clustering'},\n",
       " {'definition': 'In data mining, k-means++[1][2] is an algorithm for choosing the initial values (or \"seeds\") for the k-means clustering algorithm. It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem—a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm. It is similar to the first of three seeding methods proposed, in independent work, in 2006[3] by Rafail Ostrovsky, Yuval Rabani, Leonard Schulman and Chaitanya Swamy. (The distribution of the first seed is different.)',\n",
       "  'href': '/wiki/K-means%2B%2B',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'K-means++'},\n",
       " {'definition': 'In statistics and data mining, k-medians clustering[1][2] is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the square of the 2-norm distance metric (which k-means does.)',\n",
       "  'href': '/wiki/K-medians_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'K-medians clustering'},\n",
       " {'definition': '\\nThe k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with a generalization of the Manhattan Norm to define distance between datapoints instead of \\n\\n\\n\\n\\nl\\n\\n2\\n\\n\\n\\n\\n{\\\\displaystyle l_{2}}\\n\\n. This method was proposed in 1987[1] for the work with \\n\\n\\n\\n\\nl\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle l_{1}}\\n\\n norm and other distances.',\n",
       "  'href': '/wiki/K-medoids',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'K-medoids'},\n",
       " {'definition': 'Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.[1] It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.[2]',\n",
       "  'href': '/wiki/DBSCAN',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'DBSCAN'},\n",
       " {'definition': 'Fuzzy clustering (also referred to as soft clustering) is a form of clustering in which each data point can belong to more than one cluster.',\n",
       "  'href': '/wiki/Fuzzy_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Fuzzy clustering'},\n",
       " {'definition': 'BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets.[1] An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.',\n",
       "  'href': '/wiki/BIRCH_(data_clustering)',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'BIRCH (data clustering)'},\n",
       " {'definition': 'The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000.[1] It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set.',\n",
       "  'href': '/wiki/Canopy_clustering_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Canopy clustering algorithm'},\n",
       " {'definition': 'In data mining, cluster-weighted modeling (CWM) is an algorithm-based approach to non-linear prediction of outputs (dependent variables) from inputs (independent variables) based on density estimation using a set of models (clusters) that are each notionally appropriate in a sub-region of the input space. The overall approach works in jointly input-output space and an initial version was proposed by Neil Gershenfeld.[1][2]',\n",
       "  'href': '/wiki/Cluster-weighted_modeling',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Cluster-weighted modeling'},\n",
       " {'definition': 'Clustering high-dimensional data is the cluster analysis of data with anywhere from a few dozen to many thousands of dimensions. Such high-dimensional spaces of data are often encountered in areas such as medicine, where DNA microarray technology can produce a large number of measurements at once, and the clustering of text documents, where, if a word-frequency vector is used, the number of dimensions equals the size of the vocabulary.',\n",
       "  'href': '/wiki/Clustering_high-dimensional_data',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Clustering high-dimensional data'},\n",
       " {'definition': 'COBWEB is an incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University.[1][2]',\n",
       "  'href': '/wiki/Cobweb_(clustering)',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Cobweb (clustering)'},\n",
       " {'definition': \"Complete-linkage clustering is one of several methods of agglomerative hierarchical clustering. At the beginning of the process, each element is in a cluster of its own. The clusters are then sequentially combined into larger clusters until all elements end up being in the same cluster. At each step, the two clusters separated by the shortest distance are combined. The definition of 'shortest distance' is what differentiates between the different agglomerative clustering methods. In complete-linkage clustering, the link between two clusters contains all element pairs, and the distance between clusters equals the distance between those two elements (one in each cluster) that are farthest away from each other. The shortest of these links that remains at any step causes the fusion of the two clusters whose elements are involved. The method is also known as farthest neighbour clustering. The result of the clustering can be visualized as a dendrogram, which shows the sequence of cluster fusion and the distance at which each fusion took place.[1][2][3]\",\n",
       "  'href': '/wiki/Complete-linkage_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Complete-linkage clustering'},\n",
       " {'definition': 'In computer science, constrained clustering is a class of semi-supervised learning algorithms. Typically, constrained clustering incorporates either a set of must-link constraints, cannot-link constraints, or both, with a Data clustering algorithm. Both a must-link and a cannot-link constraint define a relationship between two data instances. A must-link constraint is used to specify that the two instances in the must-link relation should be associated with the same cluster. A cannot-link constraint is used to specify that the two instances in the cannot-link relation should not be associated with the same cluster. These sets of constraints acts as a guide for which a constrained clustering algorithm will attempt to find clusters in a data set which satisfy the specified must-link and cannot-link constraints. Some constrained clustering algorithms will abort if no such clustering exists which satisfies the specified constraints. Others will try to minimize the amount of constraint violation should it be impossible to find a clustering which satisfies the constraints. Constraints could also be used to guide the selection of a clustering model among several possible solutions. [1]',\n",
       "  'href': '/wiki/Constrained_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Constrained clustering'},\n",
       " {'definition': 'Clustering is the problem of partitioning data points into groups based on their similarity. Correlation clustering provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance.[1]',\n",
       "  'href': '/wiki/Correlation_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Correlation clustering'},\n",
       " {'definition': 'CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases[citation needed]. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.',\n",
       "  'href': '/wiki/CURE_data_clustering_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'CURE data clustering algorithm'},\n",
       " {'definition': 'In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time.',\n",
       "  'href': '/wiki/Data_stream_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Data stream clustering'},\n",
       " {'definition': 'A dendrogram (from Greek dendro \"tree\" and gramma \"drawing\") is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering.[1] Dendrograms are often used in computational biology to illustrate the clustering of genes or samples, sometimes on top of heatmaps.',\n",
       "  'href': '/wiki/Dendrogram',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Dendrogram'},\n",
       " {'definition': 'Determining the number of clusters in a data set, a quantity often labelled k as in the k-means algorithm, is a frequent problem in data clustering, and is a distinct issue from the process of actually solving the clustering problem.',\n",
       "  'href': '/wiki/Determining_the_number_of_clusters_in_a_data_set',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Determining the number of clusters in a data set'},\n",
       " {'definition': 'Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset and performs cluster assignment solely based on the neighborhood relationships among objects. The key feature of this algorithm is that the neighborhood relationships among neighboring objects in the feature space are used to constrain the memberships of neighboring objects in the fuzzy membership space.',\n",
       "  'href': '/wiki/FLAME_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'FLAME clustering'},\n",
       " {'definition': 'In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:[1]',\n",
       "  'href': '/wiki/Hierarchical_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Hierarchical clustering'},\n",
       " {'definition': 'The information bottleneck method is a technique in information theory introduced by Naftali Tishby, Fernando C. Pereira, and William Bialek.[1] It is designed for finding the best tradeoff between accuracy and complexity (compression) when summarizing (e.g. clustering) a random variable X, given a joint probability distribution p(X,Y) between X and an observed relevant variable Y. Other applications include distributional clustering and dimension reduction. More recently it has been suggested as a theoretical foundation for deep learning. It generalized the classical notion of minimal sufficient statistics from parametric statistics to arbitrary distributions, not necessarily of exponential form. It does so by relaxing the sufficiency condition to capture some fraction of the mutual information with the relevant variable Y.',\n",
       "  'href': '/wiki/Information_bottleneck_method',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Information bottleneck method'},\n",
       " {'definition': \"In computer science and electrical engineering, Lloyd's algorithm, also known as Voronoi iteration or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of Euclidean spaces and partitions of these subsets into well-shaped and uniformly sized convex cells.[1] Like the closely related k-means clustering algorithm, it repeatedly finds the centroid of each set in the partition and then re-partitions the input according to which of these centroids is closest. However, Lloyd's algorithm differs from k-means clustering in that its input is a continuous geometric region rather than a discrete set of points. Thus, when re-partitioning the input, Lloyd's algorithm uses Voronoi diagrams rather than simply determining the nearest center to each of a finite set of points as the k-means algorithm does.\",\n",
       "  'href': '/wiki/Lloyd%27s_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': \"Lloyd's algorithm\"},\n",
       " {'definition': \"In the theory of cluster analysis, the nearest-neighbor chain algorithm is an algorithm that can speed up several methods for agglomerative hierarchical clustering. These are methods that take a collection of points as input, and create a hierarchy of clusters of points by repeatedly merging pairs of smaller clusters to form larger clusters. The clustering methods that the nearest-neighbor chain algorithm can be used for include Ward's method, complete-linkage clustering, and single-linkage clustering; these all work by repeatedly merging the closest two clusters but use different definitions of the distance between clusters. The cluster distances for which the nearest-neighbor chain algorithm works are called reducible and are characterized by a simple inequality among certain cluster distances.\",\n",
       "  'href': '/wiki/Nearest-neighbor_chain_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Nearest-neighbor chain algorithm'},\n",
       " {'definition': 'In bioinformatics, neighbor joining is a bottom-up (agglomerative) clustering method for the creation of phylogenetic trees, created by Naruya Saitou and Masatoshi Nei in 1987.[1] Usually used for trees based on DNA or protein sequence data, the algorithm requires knowledge of the distance between each pair of taxa (e.g., species or sequences) to form the tree.[2]',\n",
       "  'href': '/wiki/Neighbor_joining',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Neighbor joining'},\n",
       " {'definition': \"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based[1] clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.[2] Its basic idea is similar to DBSCAN,[3] but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. In order to do so, the points of the database are (linearly) ordered such that points which are spatially closest become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that needs to be accepted for a cluster in order to have both points belong to the same cluster. This is represented as a dendrogram.\",\n",
       "  'href': '/wiki/OPTICS_algorithm',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'OPTICS algorithm'},\n",
       " {'definition': 'In probability theory, a Pitman–Yor process[1][2][3][4] denoted PY(d,\\xa0θ,\\xa0G0), is a stochastic process whose sample path is a probability distribution. A random sample from this process is an infinite discrete probability distribution, consisting of an infinite set of atoms drawn from G0, with weights drawn from a two-parameter Poisson–Dirichlet distribution. The process is named after Jim Pitman and Marc Yor.',\n",
       "  'href': '/wiki/Pitman%E2%80%93Yor_process',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Pitman–Yor process'},\n",
       " {'definition': 'In statistics, single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other.',\n",
       "  'href': '/wiki/Single-linkage_clustering',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Single-linkage clustering'},\n",
       " {'definition': 'SUBCLU is an algorithm for clustering high-dimensional data by Karin Kailing, Hans-Peter Kriegel and Peer Kröger.[1] It is a subspace clustering algorithm that builds on the density-based clustering algorithm DBSCAN. SUBCLU can find clusters in axis-parallel subspaces, and uses a bottom-up, greedy strategy to remain efficient.',\n",
       "  'href': '/wiki/SUBCLU',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'SUBCLU'},\n",
       " {'definition': 'Thresholding is the simplest method of image segmentation. From a grayscale image, thresholding can be used to create binary images (Shapiro, et al. 2001:83).',\n",
       "  'href': '/wiki/Thresholding_(image_processing)',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'Thresholding (image processing)'},\n",
       " {'definition': 'UPGMA (Unweighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method. The method is generally attributed to Sokal and Michener.[1]',\n",
       "  'href': '/wiki/UPGMA',\n",
       "  'section': 'Clustering',\n",
       "  'title': 'UPGMA'},\n",
       " {'definition': 'The Rand index[1] or Rand measure (named after William M. Rand) in statistics, and in particular in data clustering, is a measure of the similarity between two data clusterings. A form of the Rand index may be defined that is adjusted for the chance grouping of elements, this is the adjusted Rand index. From a mathematical standpoint, Rand index is related to the accuracy, but is applicable even when class labels are not used.',\n",
       "  'href': '/wiki/Rand_index',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'Rand index'},\n",
       " {'definition': 'The Dunn index (DI) (introduced by J. C. Dunn in 1974) is a metric for evaluating clustering algorithms.[1][2] This is part of a group of validity indices including the Davies–Bouldin index or Silhouette index, in that it is an internal evaluation scheme, where the result is based on the clustered data itself. As do all other such indices, the aim is to identify sets of clusters that are compact, with a small variance between members of the cluster, and well separated, where the means of different clusters are sufficiently far apart, as compared to the within cluster variance. For a given assignment of clusters, a higher Dunn index indicates better clustering. One of the drawbacks of using this is the computational cost as the number of clusters and dimensionality of the data increase.',\n",
       "  'href': '/wiki/Dunn_index',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'Dunn index'},\n",
       " {'definition': 'The Davies–Bouldin index (DBI) (introduced by David L. Davies and Donald W. Bouldin in 1979) is a metric for evaluating clustering algorithms.[1] This is an internal evaluation scheme, where the validation of how well the clustering has been done is made using quantities and features inherent to the dataset. This has a drawback that a good value reported by this method does not imply the best information retrieval.',\n",
       "  'href': '/wiki/Davies%E2%80%93Bouldin_index',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'Davies–Bouldin index'},\n",
       " {'definition': 'The Jaccard index, also known as Intersection over Union and the Jaccard similarity coefficient (originally coined coefficient de communauté by Paul Jaccard), is a statistic used for comparing the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:',\n",
       "  'href': '/wiki/Jaccard_index',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'Jaccard index'},\n",
       " {'definition': 'In computer science and data mining, MinHash (or the min-wise independent permutations locality sensitive hashing scheme) is a technique for quickly estimating how similar two sets are. The scheme was invented by Andrei Broder\\xa0(1997),[1] and initially used in the AltaVista search engine to detect duplicate web pages and eliminate them from search results.[2] It has also been applied in large-scale clustering problems, such as clustering documents by the similarity of their sets of words.[1]',\n",
       "  'href': '/wiki/MinHash',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'MinHash'},\n",
       " {'definition': '\\nIn data mining and machine learning, \\n\\n\\n\\nk\\n\\n\\n{\\\\displaystyle k}\\n\\n \\n\\n\\n\\nq\\n\\n\\n{\\\\displaystyle q}\\n\\n-flats algorithm [1] [2] is an iterative method which aims to partition \\n\\n\\n\\nm\\n\\n\\n{\\\\displaystyle m}\\n\\n observations into \\n\\n\\n\\nk\\n\\n\\n{\\\\displaystyle k}\\n\\n clusters where each cluster is close to a \\n\\n\\n\\nq\\n\\n\\n{\\\\displaystyle q}\\n\\n-flat, where \\n\\n\\n\\nq\\n\\n\\n{\\\\displaystyle q}\\n\\n is a given integer.',\n",
       "  'href': '/wiki/K_q-flats',\n",
       "  'section': 'Evaluation of Clustering Methods',\n",
       "  'title': 'K q-flats'},\n",
       " {'definition': 'A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.',\n",
       "  'href': '/wiki/Decision_rules',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'Decision rules'},\n",
       " {'definition': 'Rule induction is an area of machine learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.',\n",
       "  'href': '/wiki/Rule_induction',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'Rule induction'},\n",
       " {'definition': 'Given a population whose members each belong to one of a number of different sets or classes, a classification rule or classifier is a procedure by which the elements of the population set are each predicted to belong to one of the classes.[1] A perfect classification is one for which every element in the population is assigned to the class it really belongs to. An imperfect classification is one in which some errors appear, and then statistical analysis must be applied to analyse the classification.',\n",
       "  'href': '/wiki/Classification_rule',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'Classification rule'},\n",
       " {'definition': 'The CN2 induction algorithm is a learning algorithm for rule induction.[1] It is designed to work even when the training data is imperfect. It is based on ideas from the AQ algorithm and the ID3 algorithm. As a consequence it creates a rule set like that created by AQ but is able to handle noisy data like ID3.',\n",
       "  'href': '/wiki/CN2_algorithm',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'CN2 algorithm'},\n",
       " {'definition': 'Decision lists are a representation for Boolean functions which can be easily learnable from examples.[1] Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form.',\n",
       "  'href': '/wiki/Decision_list',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'Decision list'},\n",
       " {'definition': 'In machine learning, first-order inductive learner (FOIL) is a rule-based learning algorithm.',\n",
       "  'href': '/wiki/First_Order_Inductive_Learner',\n",
       "  'section': 'Rule Induction',\n",
       "  'title': 'First Order Inductive Learner'},\n",
       " {'definition': 'Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1]',\n",
       "  'href': '/wiki/Association_rule_learning',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'Association rule learning'},\n",
       " {'definition': 'Apriori[1] is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.',\n",
       "  'href': '/wiki/Apriori_algorithm',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'Apriori algorithm'},\n",
       " {'definition': \"Contrast set learning is a form of association rule learning that seeks to identify meaningful differences between separate groups by reverse-engineering the key predictors that identify for each particular group. For example, given a set of attributes for a pool of students (labeled by degree type), a contrast set learner would identify the contrasting features between students seeking bachelor's degrees and those working toward PhD degrees.\",\n",
       "  'href': '/wiki/Contrast_set_learning',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'Contrast set learning'},\n",
       " {'definition': 'Affinity analysis is a data analysis and data mining technique that discovers co-occurrence relationships among activities performed by (or recorded about) specific individuals or groups. In general, this can be applied to any process where agents can be uniquely identified and information about their activities can be recorded. In retail, affinity analysis is used to perform market basket analysis, in which retailers seek to understand the purchase behavior of customers. This information can then be used for purposes of cross-selling and up-selling, in addition to influencing sales promotions, loyalty programs, store design, and discount plans.[1]',\n",
       "  'href': '/wiki/Affinity_analysis',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'Affinity analysis'},\n",
       " {'definition': 'K-optimal pattern discovery is a data mining technique that provides an alternative to the frequent pattern discovery approach that underlies most association rule learning techniques.',\n",
       "  'href': '/wiki/K-optimal_pattern_discovery',\n",
       "  'section': 'Association rules and Frequent Item Sets',\n",
       "  'title': 'K-optimal pattern discovery'},\n",
       " {'definition': 'In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance that could be obtained from any of the constituent learning algorithms alone.[1][2][3][4] Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.',\n",
       "  'href': '/wiki/Ensemble_learning',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Ensemble learning'},\n",
       " {'definition': 'In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"',\n",
       "  'href': '/wiki/Ensemble_averaging',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Ensemble averaging'},\n",
       " {'definition': 'Clustering is the assignment of objects into groups (called clusters) so that objects from the same cluster are more similar to each other than objects from different clusters.[1] Often similarity is assessed according to a distance measure. Clustering is a common technique for statistical data analysis, which is used in many fields, including machine learning, data mining, pattern recognition, image analysis[2][3] and bioinformatics.',\n",
       "  'href': '/wiki/Consensus_clustering',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Consensus clustering'},\n",
       " {'definition': \"AdaBoost, short for Adaptive Boosting, is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003 Gödel Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.\",\n",
       "  'href': '/wiki/AdaBoost',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'AdaBoost'},\n",
       " {'definition': 'Boost or boosting may refer to:',\n",
       "  'href': '/wiki/Boosting',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Boosting'},\n",
       " {'definition': 'Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.',\n",
       "  'href': '/wiki/Bootstrap_aggregating',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Bootstrap aggregating'},\n",
       " {'definition': 'BrownBoost is a boosting algorithm that may be robust to noisy datasets. BrownBoost is an adaptive version of the boost by majority algorithm. As is true for all boosting algorithms, BrownBoost is used in conjunction with other machine learning methods. BrownBoost was introduced by Yoav Freund in 2001.[1]',\n",
       "  'href': '/wiki/BrownBoost',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'BrownBoost'},\n",
       " {'definition': 'Cascading is a particular case of ensemble learning based on the concatenation of several Classifiers, using all information collected from the output from a given classifier as additional information for the next classifier in the cascade. Unlike voting or stacking ensembles, which are multiexpert systems, cascading is a multistage one.',\n",
       "  'href': '/wiki/Cascading_classifiers',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Cascading classifiers'},\n",
       " {'definition': 'Co-training is a machine learning algorithm used when there are only small amounts of labeled data and large amounts of unlabeled data. One of its uses is in text mining for search engines. It was introduced by Avrim Blum and Tom Mitchell in 1998.',\n",
       "  'href': '/wiki/Co-training',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Co-training'},\n",
       " {'definition': 'CoBoost is a semi-supervised training algorithm proposed by Collins and Singer in 1999. The original application for the algorithm was the task of Named Entity Classification using very weak learners.[1] It can be used for performing semi-supervised learning in cases in which there exist redundancy in features.',\n",
       "  'href': '/wiki/CoBoosting',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'CoBoosting'},\n",
       " {'definition': 'In statistics, Gaussian process emulator is one name for a general type of statistical model that has been used in contexts where the problem is to make maximum use of the outputs of a complicated (often non-random) computer-based simulation model. Each run of the simulation model is computationally expensive and each run is based on many different controlling inputs. The variation of the outputs of the simulation model is expected to vary reasonably smoothly with the inputs, but in an unknown way.',\n",
       "  'href': '/wiki/Gaussian_process_emulator',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Gaussian process emulator'},\n",
       " {'definition': 'Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.',\n",
       "  'href': '/wiki/Gradient_boosting',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Gradient boosting'},\n",
       " {'definition': 'In machine learning and computational learning theory, LogitBoost is a boosting algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The original paper[1] casts the AdaBoost algorithm into a statistical framework. Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm.',\n",
       "  'href': '/wiki/LogitBoost',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'LogitBoost'},\n",
       " {'definition': 'Linear Programming Boosting (LPBoost) is a supervised classifier from the boosting family of classifiers. LPBoost maximizes a margin between training samples of different classes and hence also belongs to the class of margin-maximizing supervised classification algorithms. Consider a classification function',\n",
       "  'href': '/wiki/LPBoost',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'LPBoost'},\n",
       " {'definition': 'In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with \"mixture distributions\" relate to deriving the properties of the overall population from those of the sub-populations, \"mixture models\" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.',\n",
       "  'href': '/wiki/Mixture_model',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Mixture model'},\n",
       " {'definition': 'Product of experts (PoE) is a machine learning technique. It models a probability distribution by combining the output from several simpler distributions. It was proposed by Geoff Hinton, along with an algorithm for training the parameters of such a system.',\n",
       "  'href': '/wiki/Product_of_Experts',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Product of Experts'},\n",
       " {'definition': \"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]:587–588\",\n",
       "  'href': '/wiki/Random_multinomial_logit',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Random multinomial logit'},\n",
       " {'definition': 'In machine learning the random subspace method,[1] also called attribute bagging[2] or feature bagging, is an ensemble learning method that attempts to reduce the correlation between estimators in an ensemble by training them on random samples of features instead of the entire feature set.',\n",
       "  'href': '/wiki/Random_subspace_method',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Random subspace method'},\n",
       " {'definition': 'In machine learning, weighted majority algorithm (WMA) is a meta-learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts.[1][2] The algorithm assumes that we have no prior knowledge about the accuracy of the algorithms in the pool, but there are sufficient reasons to believe that one or more will perform well.',\n",
       "  'href': '/wiki/Weighted_Majority_Algorithm',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Weighted Majority Algorithm'},\n",
       " {'definition': 'The randomized weighted majority algorithm is an algorithm in machine learning theory.[1] It improves the mistake bound of the weighted majority algorithm.',\n",
       "  'href': '/wiki/Randomized_weighted_majority_algorithm',\n",
       "  'section': 'Ensemble Learning',\n",
       "  'title': 'Randomized weighted majority algorithm'},\n",
       " {'definition': 'A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.',\n",
       "  'href': '/wiki/Graphical_model',\n",
       "  'section': 'Graphical Models',\n",
       "  'title': 'Graphical model'},\n",
       " {'definition': 'A state transition network is a diagram that is developed from a set of data and charts the flow of data from particular data points (called states or nodes) to the next in a probabilistic manner.',\n",
       "  'href': '/wiki/State_transition_network',\n",
       "  'section': 'Graphical Models',\n",
       "  'title': 'State transition network'},\n",
       " {'definition': 'In machine learning, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features.',\n",
       "  'href': '/wiki/Naive_Bayes_classifier',\n",
       "  'section': 'Bayesian Learning Methods',\n",
       "  'title': 'Naive Bayes classifier'},\n",
       " {'definition': 'Averaged one-dependence estimators (AODE) is a probabilistic classification learning technique. It was developed to address the attribute-independence problem of the popular naive Bayes classifier. It frequently develops substantially more accurate classifiers than naive Bayes at the cost of a modest increase in the amount of computation.[1]',\n",
       "  'href': '/wiki/Averaged_one-dependence_estimators',\n",
       "  'section': 'Bayesian Learning Methods',\n",
       "  'title': 'Averaged one-dependence estimators'},\n",
       " {'definition': 'A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.',\n",
       "  'href': '/wiki/Bayesian_network',\n",
       "  'section': 'Bayesian Learning Methods',\n",
       "  'title': 'Bayesian network'},\n",
       " {'definition': \"Variational message passing (VMP) is an approximate inference technique for continuous- or discrete-valued Bayesian networks, with conjugate-exponential parents, developed by John Winn. VMP was developed as a means of generalizing the approximate variational methods used by such techniques as Latent Dirichlet allocation and works by updating an approximate distribution at each node through messages in the node's Markov blanket.\",\n",
       "  'href': '/wiki/Variational_message_passing',\n",
       "  'section': 'Bayesian Learning Methods',\n",
       "  'title': 'Variational message passing'},\n",
       " {'definition': 'In probability theory, a Markov model is a stochastic model used to model randomly changing systems.[1] It is assumed that future states depend only on the current state, not on the events that occurred before it (that is, it assumes the Markov property). Generally, this assumption enables reasoning and computation with the model that would otherwise be intractable. For this reason, in the fields of predictive modelling and probabilistic forecasting, it is desirable for a given model to exhibit the Markov property.',\n",
       "  'href': '/wiki/Markov_model',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Markov model'},\n",
       " {'definition': 'In machine learning, a maximum-entropy Markov model (MEMM), or conditional Markov model (CMM), is a graphical model for sequence labeling that combines features of hidden Markov models (HMMs) and maximum entropy (MaxEnt) models. An MEMM is a discriminative model that extends a standard maximum entropy classifier by assuming that the unknown values to be learnt are connected in a Markov chain rather than being conditionally independent of each other. MEMMs find applications in natural language processing, specifically in part-of-speech tagging[1] and information extraction.[2]',\n",
       "  'href': '/wiki/Maximum-entropy_Markov_model',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Maximum-entropy Markov model'},\n",
       " {'definition': 'Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states.',\n",
       "  'href': '/wiki/Hidden_Markov_model',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Hidden Markov model'},\n",
       " {'definition': 'In electrical engineering, computer science, statistical computing and bioinformatics, the Baum–Welch algorithm is used to find the unknown parameters of a hidden Markov model (HMM). It makes use of the forward-backward algorithm and is named for Leonard E. Baum and Lloyd R. Welch.',\n",
       "  'href': '/wiki/Baum%E2%80%93Welch_algorithm',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Baum–Welch algorithm'},\n",
       " {'definition': 'The forward–backward algorithm is an inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions \\n\\n\\n\\n\\no\\n\\n1\\n:\\nT\\n\\n\\n:=\\n\\no\\n\\n1\\n\\n\\n,\\n…\\n,\\n\\no\\n\\nT\\n\\n\\n\\n\\n{\\\\displaystyle o_{1:T}:=o_{1},\\\\dots ,o_{T}}\\n\\n, i.e. it computes, for all hidden state variables \\n\\n\\n\\n\\nX\\n\\nt\\n\\n\\n∈\\n{\\n\\nX\\n\\n1\\n\\n\\n,\\n…\\n,\\n\\nX\\n\\nT\\n\\n\\n}\\n\\n\\n{\\\\displaystyle X_{t}\\\\in \\\\{X_{1},\\\\dots ,X_{T}\\\\}}\\n\\n, the distribution \\n\\n\\n\\nP\\n(\\n\\nX\\n\\nt\\n\\n\\n\\xa0\\n\\n|\\n\\n\\xa0\\n\\no\\n\\n1\\n:\\nT\\n\\n\\n)\\n\\n\\n{\\\\displaystyle P(X_{t}\\\\ |\\\\ o_{1:T})}\\n\\n. This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to efficiently compute the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm.',\n",
       "  'href': '/wiki/Forward%E2%80%93backward_algorithm',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Forward–backward algorithm'},\n",
       " {'definition': 'The hierarchical hidden Markov model (HHMM) is a statistical model derived from the hidden Markov model (HMM). In an HHMM each state is considered to be a self-contained probabilistic model. More precisely each state of the HHMM is itself an HHMM.',\n",
       "  'href': '/wiki/Hierarchical_hidden_Markov_model',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Hierarchical hidden Markov model'},\n",
       " {'definition': 'A Markov logic network (MLN) is a probabilistic logic which applies the ideas of a Markov network to first-order logic, enabling uncertain inference. Markov logic networks generalize first-order logic, in the sense that, in a certain limit, all unsatisfiable statements have a probability of zero, and all tautologies have probability one.',\n",
       "  'href': '/wiki/Markov_logic_network',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Markov logic network'},\n",
       " {'definition': 'In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by observing the chain after a number of steps. The more steps there are, the more closely the distribution of the sample matches the actual desired distribution.',\n",
       "  'href': '/wiki/Markov_chain_Monte_Carlo',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Markov chain Monte Carlo'},\n",
       " {'definition': 'In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. In other words, a random field is said to be a Markov random field if it satisfies Markov properties.',\n",
       "  'href': '/wiki/Markov_random_field',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Markov random field'},\n",
       " {'definition': 'Conditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. CRFs fall into the sequence modeling family. Whereas a discrete classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account; e.g., the linear chain CRF (which is popular in natural language processing) predicts sequences of labels for sequences of input samples.',\n",
       "  'href': '/wiki/Conditional_random_field',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Conditional random field'},\n",
       " {'definition': \"In computer science, a predictive state representation (PSR) is a way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system.[1] A test is a sequence of action-observation pairs and its prediction is the probability of the test's observation-sequence happening if the test's action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities. This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states.[2]\",\n",
       "  'href': '/wiki/Predictive_state_representation',\n",
       "  'section': 'Markov Models',\n",
       "  'title': 'Predictive state representation'},\n",
       " {'definition': 'In computer science, computational learning theory (or just learning theory) is a subfield of Artificial Intelligence devoted to studying the design and analysis of machine learning algorithms.[1]',\n",
       "  'href': '/wiki/Computational_learning_theory',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Computational learning theory'},\n",
       " {'definition': 'Version space learning is a logical approach to machine learning, specifically binary classification. Version space learning algorithms search a predefined space of hypotheses, viewed as a set of logical sentences. Formally, the hypothesis space is a disjunction[1]',\n",
       "  'href': '/wiki/Version_space',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Version space'},\n",
       " {'definition': 'In computational learning theory, probably approximately correct learning (PAC learning) is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.[1]',\n",
       "  'href': '/wiki/Probably_approximately_correct_learning',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Probably approximately correct learning'},\n",
       " {'definition': 'Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.',\n",
       "  'href': '/wiki/Vapnik%E2%80%93Chervonenkis_theory',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Vapnik–Chervonenkis theory'},\n",
       " {'definition': 'The concept of shattered sets plays an important role in Vapnik–Chervonenkis theory, also known as VC-theory. Shattering and VC-theory are used in the study of empirical processes as well as in statistical computational learning theory.',\n",
       "  'href': '/wiki/Shattering_(machine_learning)',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Shattering (machine learning)'},\n",
       " {'definition': 'In Vapnik–Chervonenkis theory, the VC dimension (for Vapnik–Chervonenkis dimension) is a measure of the capacity (complexity, expressive power, richness, or flexibility) of a space of functions that can be learned by a statistical classification algorithm. It is defined as the cardinality of the largest set of points that the algorithm can shatter. It was originally defined by Vladimir Vapnik and Alexey Chervonenkis.[1]',\n",
       "  'href': '/wiki/VC_dimension',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'VC dimension'},\n",
       " {'definition': \"The minimum description length (MDL) principle is a formalization of Occam's razor in which the best hypothesis (a model and its parameters) for a given set of data is the one that leads to the best compression of the data. MDL was introduced by Jorma Rissanen in 1978.[1] It is an important concept in information theory and computational learning theory.[2][3][4]\",\n",
       "  'href': '/wiki/Minimum_description_length',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Minimum description length'},\n",
       " {'definition': \"In mathematics, Bondy's theorem is a bound on the number of elements needed to distinguish the sets in a family of sets from each other. It belongs to the field of combinatorics, and is named after John Adrian Bondy, who published it in 1972.[1]\",\n",
       "  'href': '/wiki/Bondy%27s_theorem',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': \"Bondy's theorem\"},\n",
       " {'definition': 'Inferential theory of learning (ITL) is an area of machine learning which describes inferential processes performed by learning agents. ITL has been developed by Ryszard S. Michalski in 1980s. In ITL learning process is viewed as a search (inference) through hypotheses space guided by a specific goal. Results of learning need to be stored, in order to be used in the future.',\n",
       "  'href': '/wiki/Inferential_theory_of_learning',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Inferential theory of learning'},\n",
       " {'definition': 'In computational learning theory (machine learning and theory of computation), Rademacher complexity, named after Hans Rademacher, measures richness of a class of real-valued functions with respect to a probability distribution.',\n",
       "  'href': '/wiki/Rademacher_complexity',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Rademacher complexity'},\n",
       " {'definition': 'In computational learning theory, the teaching dimension[1] of a concept class C is defined to be \\n\\n\\n\\n\\nmax\\n\\nc\\n∈\\nC\\n\\n\\n{\\n\\nw\\n\\nC\\n\\n\\n(\\nc\\n)\\n}\\n\\n\\n{\\\\displaystyle \\\\max _{c\\\\in C}\\\\{w_{C}(c)\\\\}}\\n\\n, where \\n\\n\\n\\n\\n\\nw\\n\\nC\\n\\n\\n(\\nc\\n)\\n\\n\\n\\n{\\\\displaystyle {w_{C}(c)}}\\n\\n is the minimum size of a witness set for c in C.',\n",
       "  'href': '/wiki/Teaching_dimension',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Teaching dimension'},\n",
       " {'definition': 'In computational learning theory in mathematics, given a class of concepts C, a subclass D is reachable if there exists a partial approximation S of some concept such that D contains exactly those concepts in C that are extensions to S (i.e., D=C|S).',\n",
       "  'href': '/wiki/Subclass_reachability',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Subclass reachability'},\n",
       " {'definition': 'In computational learning theory, sample exclusion dimensions arise in the study of exact concept learning with queries.[1]',\n",
       "  'href': '/wiki/Sample_exclusion_dimension',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Sample exclusion dimension'},\n",
       " {'definition': '\\nUnique negative dimension (UND) is a complexity measure for the model of learning from positive examples. The unique negative dimension of a class \\n\\n\\n\\nC\\n\\n\\n{\\\\displaystyle C}\\n\\n of concepts is the size of the maximum subclass \\n\\n\\n\\nD\\n⊆\\nC\\n\\n\\n{\\\\displaystyle D\\\\subseteq C}\\n\\n such that for every concept \\n\\n\\n\\nc\\n∈\\nD\\n\\n\\n{\\\\displaystyle c\\\\in D}\\n\\n, we have \\n\\n\\n\\n∩\\n(\\nD\\n∖\\n{\\nc\\n}\\n)\\n∖\\nc\\n\\n\\n{\\\\displaystyle \\\\cap (D\\\\setminus \\\\{c\\\\})\\\\setminus c}\\n\\n is nonempty.',\n",
       "  'href': '/wiki/Unique_negative_dimension',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Unique negative dimension'},\n",
       " {'definition': 'Uniform convergence in probability is a form of convergence in probability in statistical asymptotic theory and probability theory. It means that, under certain conditions, the empirical frequencies of all events in a certain event-family converge to their theoretical probabilities. Uniform convergence in probability has applications to statistics as well as machine learning as part of statistical learning theory.',\n",
       "  'href': '/wiki/Uniform_convergence_(combinatorics)',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Uniform convergence (combinatorics)'},\n",
       " {'definition': 'In computational learning theory, let C be a concept class over a domain X and c be a concept in C. A subset S of X is a witness set for c in C if c(S) verifies c (i.e., c is the only consistent concept with respect to c(S)). The minimum size of a witness set for c is called the witness size or specification number and is denoted by \\n\\n\\n\\n\\nw\\n\\nC\\n\\n\\n(\\nc\\n)\\n\\n\\n{\\\\displaystyle w_{C}(c)}\\n\\n. The value \\n\\n\\n\\nmax\\n{\\n\\nw\\n\\nC\\n\\n\\n(\\nc\\n)\\n:\\nc\\n∈\\nC\\n}\\n\\n\\n{\\\\displaystyle \\\\max\\\\{w_{C}(c):c\\\\in C\\\\}}\\n\\n is called the teaching dimension of C.',\n",
       "  'href': '/wiki/Witness_set',\n",
       "  'section': 'Learning Theory',\n",
       "  'title': 'Witness set'},\n",
       " {'definition': 'In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation.',\n",
       "  'href': '/wiki/Kernel_methods',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Kernel methods'},\n",
       " {'definition': 'In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.',\n",
       "  'href': '/wiki/Support_vector_machine',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Support vector machine'},\n",
       " {'definition': \"Structural risk minimization (SRM) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting – the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data.\",\n",
       "  'href': '/wiki/Structural_risk_minimization',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Structural risk minimization'},\n",
       " {'definition': 'Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance.',\n",
       "  'href': '/wiki/Empirical_risk_minimization',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Empirical risk minimization'},\n",
       " {'definition': 'In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation.',\n",
       "  'href': '/wiki/Kernel_trick',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Kernel trick'},\n",
       " {'definition': 'Least squares support vector machines (LS-SVM) are least squares versions of support vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis. In this version one finds the solution by solving a set of linear equations instead of a convex quadratic programming (QP) problem for classical SVMs. Least squares SVM classifiers, were proposed by Suykens and Vandewalle.[1] LS-SVMs are a class of kernel-based learning methods.',\n",
       "  'href': '/wiki/Least_squares_support_vector_machine',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Least squares support vector machine'},\n",
       " {'definition': 'In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.[1] The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.',\n",
       "  'href': '/wiki/Relevance_vector_machine',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Relevance vector machine'},\n",
       " {'definition': 'Sequential minimal optimization (SMO) is an algorithm for solving the quadratic programming (QP) problem that arises during the training of support vector machines. It was invented by John Platt in 1998 at Microsoft Research.[1] SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool.[2][3] The publication of the SMO algorithm in 1998 has generated a lot of excitement in the SVM community, as previously available methods for SVM training were much more complex and required expensive third-party QP solvers.[4]',\n",
       "  'href': '/wiki/Sequential_minimal_optimization',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Sequential minimal optimization'},\n",
       " {'definition': 'The structured support vector machine is a machine learning algorithm that generalizes the Support Vector Machine (SVM) classifier. Whereas the SVM classifier supports binary classification, multiclass classification and regression, the structured SVM allows training of a classifier for general structured output labels.',\n",
       "  'href': '/wiki/Structured_SVM',\n",
       "  'section': 'Support Vector Machines',\n",
       "  'title': 'Structured SVM'},\n",
       " {'definition': 'The following outline is provided as an overview of and topical guide to regression analysis:',\n",
       "  'href': '/wiki/Outline_of_regression_analysis',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Outline of regression analysis'},\n",
       " {'definition': \"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.\",\n",
       "  'href': '/wiki/Regression_analysis',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Regression analysis'},\n",
       " {'definition': 'In mathematical modeling, statistical modeling and experimental sciences, the values of dependent variables depend on the values of independent variables. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e., potential reasons for variation or, in the experimental setting, the variable controlled by the experimenter. Models and experiments test or determine the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly.',\n",
       "  'href': '/wiki/Dependent_and_independent_variables',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Dependent and independent variables'},\n",
       " {'definition': 'In statistics, the term linear model is used in different ways according to the context. The most common occurrence is in connection with regression models and the term is often taken as synonymous with linear regression model. However, the term is also used in time series analysis with a different meaning. In each case, the designation \"linear\" is used to identify a subclass of models for which substantial reduction in the complexity of the related statistical theory is possible.',\n",
       "  'href': '/wiki/Linear_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Linear model'},\n",
       " {'definition': 'In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2]',\n",
       "  'href': '/wiki/Linear_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Linear regression'},\n",
       " {'definition': 'The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems, i.e., sets of equations in which there are more equations than unknowns. \"Least squares\" means that the overall solution minimizes the sum of the squares of the residuals made in the results of every single equation.',\n",
       "  'href': '/wiki/Least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Least squares'},\n",
       " {'definition': 'In statistics and mathematics, linear least squares is an approach to fitting a mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model. The resulting fitted model can be used to summarize the data, to predict unobserved values from the same system, and to understand the mechanisms that may underlie the system.',\n",
       "  'href': '/wiki/Linear_least_squares_(mathematics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Linear least squares (mathematics)'},\n",
       " {'definition': 'LOESS (/ˈloʊɛs/) and LOWESS (locally weighted scatterplot smoothing) are two strongly related non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model. \"LOESS\" is a later generalization of LOWESS; although it is not a true acronym, it may be understood as standing for \"LOcal regrESSion\".[1]',\n",
       "  'href': '/wiki/Local_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Local regression'},\n",
       " {'definition': 'In statistics, an additive model (AM) is a nonparametric regression method. It was suggested by Jerome H. Friedman and Werner Stuetzle (1981)[1] and is an essential part of the ACE algorithm. The AM uses a one-dimensional smoother to build a restricted class of nonparametric regression models. Because of this, it is less affected by the curse of dimensionality than e.g. a p-dimensional smoother. Furthermore, the AM is more flexible than a standard linear model, while being more interpretable than a general regression surface at the cost of approximation errors. Problems with AM include model selection, overfitting, and multicollinearity.',\n",
       "  'href': '/wiki/Additive_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Additive model'},\n",
       " {'definition': 'In statistics and social sciences, an antecedent variable is a variable that can help to explain the apparent relationship (or part of the relationship) between other variables that are nominally in a cause and effect relationship. In a regression analysis, an antecedent variable would be one that influences both the independent variable and the dependent variable.',\n",
       "  'href': '/wiki/Antecedent_variable',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Antecedent variable'},\n",
       " {'definition': 'Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency in a signal implied by its harmonic frequencies. It is often used in signal processing for analyzing functions or series of values, such as time domain signals.',\n",
       "  'href': '/wiki/Autocorrelation',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Autocorrelation'},\n",
       " {'definition': 'In statistics, the backfitting algorithm is a simple iterative procedure used to fit a generalized additive model. It was introduced in 1985 by Leo Breiman and Jerome Friedman along with generalized additive models. In most cases, the backfitting algorithm is equivalent to the Gauss–Seidel method algorithm for solving a certain linear system of equations.',\n",
       "  'href': '/wiki/Backfitting_algorithm',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Backfitting algorithm'},\n",
       " {'definition': \"In statistics, Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference. When the regression model has errors that have a normal distribution, and if a particular form of prior distribution is assumed, explicit results are available for the posterior probability distributions of the model's parameters.\",\n",
       "  'href': '/wiki/Bayesian_linear_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Bayesian linear regression'},\n",
       " {'definition': 'In statistics, Bayesian multivariate linear regression is a Bayesian approach to multivariate linear regression, i.e. linear regression where the predicted outcome is a vector of correlated random variables rather than a single scalar random variable. A more general treatment of this approach can be found in the article MMSE estimator.',\n",
       "  'href': '/wiki/Bayesian_multivariate_linear_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Bayesian multivariate linear regression'},\n",
       " {'definition': 'In statistics, binomial regression is a technique in which the response (often referred to as Y) is the result of a series of Bernoulli trials, or a series of one of two possible disjoint outcomes (traditionally denoted \"success\" or 1, and \"failure\" or 0).[1] In binomial regression, the probability of a success is related to explanatory variables: the corresponding concept in ordinary regression is to relate the mean value of the unobserved response to explanatory variables.',\n",
       "  'href': '/wiki/Binomial_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Binomial regression'},\n",
       " {'definition': 'In statistics, canonical analysis (from Ancient Greek: κανων bar, measuring rod, ruler) belongs to the family of regression methods for data analysis. Regression analysis quantifies a relationship between a predictor variable and a criterion variable by the coefficient of correlation r, coefficient of determination r2, and the standard regression coefficient β. Multiple regression analysis expresses a relationship between a set of predictor variables and a single criterion variable by the multiple correlation\\xa0R, multiple coefficient of determination R², and a set of standard partial regression weights β1, β2, etc. Canonical variate analysis captures a relationship between a set of predictor variables and a set of criterion variables by the canonical correlations ρ1, ρ2, ..., and by the sets of canonical weights C and D.',\n",
       "  'href': '/wiki/Canonical_analysis',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Canonical analysis'},\n",
       " {'definition': 'Censored regression models commonly arise in econometrics in cases where the variable of interest is only observable under certain conditions. A common example is labor supply. Data are frequently available on the hours worked by employees, and a labor supply model estimates the relationship between hours worked and characteristics of employees such as age, education and family status. However, such estimates undertaken using linear regression will be biased by the fact that for people who are unemployed it is not possible to observe the number of hours they would have worked had they had employment. Still we know age, education and family status for those observations. The censored model shall not be confused with the truncated regression model, which is in general different and requires different types of estimators.[1]',\n",
       "  'href': '/wiki/Censored_regression_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Censored regression model'},\n",
       " {'definition': 'In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).',\n",
       "  'href': '/wiki/Coefficient_of_determination',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Coefficient of determination'},\n",
       " {'href': '/wiki/Comparison_of_general_and_generalized_linear_models',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Comparison of general and generalized linear models'},\n",
       " {'href': '/wiki/Compressed_sensing',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Compressed sensing'},\n",
       " {'href': '/wiki/Conditional_change_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Conditional change model'},\n",
       " {'href': '/wiki/Controlling_for_a_variable',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Controlling for a variable'},\n",
       " {'href': '/wiki/Cross-sectional_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Cross-sectional regression'},\n",
       " {'href': '/wiki/Curve_fitting',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Curve fitting'},\n",
       " {'href': '/wiki/Deming_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Deming regression'},\n",
       " {'href': '/wiki/Design_matrix',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Design matrix'},\n",
       " {'href': '/wiki/Difference_in_differences',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Difference in differences'},\n",
       " {'href': '/wiki/Dummy_variable_(statistics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Dummy variable (statistics)'},\n",
       " {'href': '/wiki/Errors_and_residuals_in_statistics',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Errors and residuals in statistics'},\n",
       " {'href': '/wiki/Errors-in-variables_models',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Errors-in-variables models'},\n",
       " {'href': '/wiki/Explained_sum_of_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Explained sum of squares'},\n",
       " {'href': '/wiki/Explained_variation',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Explained variation'},\n",
       " {'href': '/wiki/First-hitting-time_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'First-hitting-time model'},\n",
       " {'href': '/wiki/Fixed_effects_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Fixed effects model'},\n",
       " {'href': '/wiki/Fraction_of_variance_unexplained',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Fraction of variance unexplained'},\n",
       " {'href': '/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Frisch–Waugh–Lovell theorem'},\n",
       " {'href': '/wiki/General_linear_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'General linear model'},\n",
       " {'href': '/wiki/Generalized_additive_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized additive model'},\n",
       " {'href': '/wiki/Generalized_additive_model_for_location,_scale_and_shape',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized additive model for location, scale and shape'},\n",
       " {'href': '/wiki/Generalized_estimating_equation',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized estimating equation'},\n",
       " {'href': '/wiki/Generalized_least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized least squares'},\n",
       " {'href': '/wiki/Generalized_linear_array_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized linear array model'},\n",
       " {'href': '/wiki/Generalized_linear_mixed_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized linear mixed model'},\n",
       " {'href': '/wiki/Generalized_linear_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Generalized linear model'},\n",
       " {'href': '/wiki/Growth_curve',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Growth curve'},\n",
       " {'href': '/wiki/Guess_value',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Guess value'},\n",
       " {'href': '/wiki/Hat_matrix',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Hat matrix'},\n",
       " {'href': '/wiki/Heckman_correction',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Heckman correction'},\n",
       " {'href': '/wiki/Heteroscedasticity-consistent_standard_errors',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Heteroscedasticity-consistent standard errors'},\n",
       " {'href': '/wiki/Hosmer%E2%80%93Lemeshow_test',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Hosmer–Lemeshow test'},\n",
       " {'href': '/wiki/Instrumental_variable',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Instrumental variable'},\n",
       " {'href': '/wiki/Interaction_(statistics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Interaction (statistics)'},\n",
       " {'href': '/wiki/Isotonic_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Isotonic regression'},\n",
       " {'href': '/wiki/Iteratively_reweighted_least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Iteratively reweighted least squares'},\n",
       " {'href': '/wiki/Kitchen_sink_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Kitchen sink regression'},\n",
       " {'href': '/wiki/Lack-of-fit_sum_of_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Lack-of-fit sum of squares'},\n",
       " {'href': '/wiki/Leverage_(statistics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Leverage (statistics)'},\n",
       " {'href': '/wiki/Limited_dependent_variable',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Limited dependent variable'},\n",
       " {'href': '/wiki/Linear_probability_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Linear probability model'},\n",
       " {'href': '/wiki/Mallows%27s_Cp',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': \"Mallows's Cp\"},\n",
       " {'href': '/wiki/Mean_and_predicted_response',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Mean and predicted response'},\n",
       " {'href': '/wiki/Mixed_model',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Mixed model'},\n",
       " {'href': '/wiki/Moderation_(statistics)',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Moderation (statistics)'},\n",
       " {'href': '/wiki/Moving_least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Moving least squares'},\n",
       " {'href': '/wiki/Multicollinearity',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Multicollinearity'},\n",
       " {'href': '/wiki/Multiple_correlation',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Multiple correlation'},\n",
       " {'href': '/wiki/Multivariate_probit',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Multivariate probit'},\n",
       " {'href': '/wiki/Multivariate_adaptive_regression_splines',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Multivariate adaptive regression splines'},\n",
       " {'href': '/wiki/Newey%E2%80%93West_estimator',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Newey–West estimator'},\n",
       " {'href': '/wiki/Non-linear_least_squares',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Non-linear least squares'},\n",
       " {'href': '/wiki/Nonlinear_regression',\n",
       "  'section': 'Regression analysis',\n",
       "  'title': 'Nonlinear regression'},\n",
       " {'href': '/wiki/Logit', 'section': 'Logistic Regression', 'title': 'Logit'},\n",
       " {'href': '/wiki/Multinomial_logit',\n",
       "  'section': 'Logistic Regression',\n",
       "  'title': 'Multinomial logit'},\n",
       " {'href': '/wiki/Logistic_regression',\n",
       "  'section': 'Logistic Regression',\n",
       "  'title': 'Logistic regression'},\n",
       " {'href': '/wiki/Bio-inspired_computing',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Bio-inspired computing'},\n",
       " {'href': '/wiki/Metaheuristic',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Metaheuristic'},\n",
       " {'href': '/wiki/Swarm_intelligence',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Swarm intelligence'},\n",
       " None,\n",
       " {'href': '/wiki/Particle_swarm_optimization',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Particle swarm optimization'},\n",
       " {'href': '/wiki/Ant_colony_optimization_algorithms',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Ant colony optimization algorithms'},\n",
       " {'href': '/wiki/Artificial_immune_system',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Artificial immune system'},\n",
       " {'href': '/wiki/Firefly_algorithm',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Firefly algorithm'},\n",
       " {'href': '/wiki/Cuckoo_search',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Cuckoo search'},\n",
       " {'href': '/wiki/Bat_algorithm',\n",
       "  'section': 'Bio-inspired Methods',\n",
       "  'title': 'Bat algorithm'},\n",
       " {'href': '/wiki/Evolvability_(computer_science)',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Evolvability (computer science)'},\n",
       " {'href': '/wiki/Evolutionary_computation',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Evolutionary computation'},\n",
       " {'href': '/wiki/Evolutionary_algorithm',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Evolutionary algorithm'},\n",
       " {'href': '/wiki/Genetic_algorithm',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Genetic algorithm'},\n",
       " {'href': '/wiki/Chromosome_(genetic_algorithm)',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Chromosome (genetic algorithm)'},\n",
       " {'href': '/wiki/Crossover_(genetic_algorithm)',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Crossover (genetic algorithm)'},\n",
       " {'href': '/wiki/Fitness_function',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Fitness function'},\n",
       " {'href': '/wiki/Evolutionary_data_mining',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Evolutionary data mining'},\n",
       " {'href': '/wiki/Genetic_programming',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Genetic programming'},\n",
       " {'href': '/wiki/Learnable_Evolution_Model',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Learnable Evolution Model'},\n",
       " {'href': '/wiki/Stochastic_diffusion_search',\n",
       "  'section': 'Evolutionary Algorithms',\n",
       "  'title': 'Stochastic diffusion search'},\n",
       " {'href': '/wiki/Neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural network'},\n",
       " {'href': '/wiki/Artificial_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Artificial neural network'},\n",
       " {'href': '/wiki/Artificial_neuron',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Artificial neuron'},\n",
       " {'href': '/wiki/Types_of_artificial_neural_networks',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Types of artificial neural networks'},\n",
       " {'href': '/wiki/Perceptron',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Perceptron'},\n",
       " {'href': '/wiki/Multilayer_perceptron',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Multilayer perceptron'},\n",
       " {'href': '/wiki/Activation_function',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Activation function'},\n",
       " {'href': '/wiki/Self-organizing_map',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Self-organizing map'},\n",
       " {'href': '/wiki/Attractor_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Attractor network'},\n",
       " {'href': '/wiki/ADALINE', 'section': 'Neural Networks', 'title': 'ADALINE'},\n",
       " {'href': '/wiki/Adaptive_Neuro_Fuzzy_Inference_System',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Adaptive Neuro Fuzzy Inference System'},\n",
       " {'href': '/wiki/Adaptive_resonance_theory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Adaptive resonance theory'},\n",
       " {'href': '/wiki/IPO_underpricing_algorithm',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'IPO underpricing algorithm'},\n",
       " {'href': '/wiki/ALOPEX', 'section': 'Neural Networks', 'title': 'ALOPEX'},\n",
       " {'href': '/wiki/Artificial_Intelligence_System',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Artificial Intelligence System'},\n",
       " {'href': '/wiki/Autoassociative_memory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Autoassociative memory'},\n",
       " {'href': '/wiki/Autoencoder',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Autoencoder'},\n",
       " {'href': '/wiki/Backpropagation',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Backpropagation'},\n",
       " {'href': '/wiki/Bcpnn', 'section': 'Neural Networks', 'title': 'Bcpnn'},\n",
       " {'href': '/wiki/Bidirectional_associative_memory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Bidirectional associative memory'},\n",
       " {'href': '/wiki/Biological_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Biological neural network'},\n",
       " {'href': '/wiki/Boltzmann_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Boltzmann machine'},\n",
       " {'href': '/wiki/Restricted_Boltzmann_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Restricted Boltzmann machine'},\n",
       " {'href': '/wiki/Cellular_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Cellular neural network'},\n",
       " {'href': '/wiki/Cerebellar_Model_Articulation_Controller',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Cerebellar Model Articulation Controller'},\n",
       " {'href': '/wiki/Committee_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Committee machine'},\n",
       " {'href': '/wiki/Competitive_learning',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Competitive learning'},\n",
       " {'href': '/wiki/Compositional_pattern-producing_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Compositional pattern-producing network'},\n",
       " {'href': '/wiki/Computational_cybernetics',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Computational cybernetics'},\n",
       " {'href': '/wiki/Computational_neurogenetic_modeling',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Computational neurogenetic modeling'},\n",
       " {'href': '/wiki/Confabulation_(neural_networks)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Confabulation (neural networks)'},\n",
       " {'href': '/wiki/Cortical_column',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Cortical column'},\n",
       " {'href': '/w/index.php?title=Counterpropagation_network&action=edit&redlink=1',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Counterpropagation network (page does not exist)'},\n",
       " {'href': '/wiki/Cover%27s_theorem',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': \"Cover's theorem\"},\n",
       " {'href': '/wiki/Cultured_neuronal_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Cultured neuronal network'},\n",
       " {'href': '/wiki/Dehaene-Changeux_Model',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Dehaene-Changeux Model'},\n",
       " {'href': '/wiki/Delta_rule',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Delta rule'},\n",
       " {'href': '/wiki/Early_stopping',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Early stopping'},\n",
       " {'href': '/wiki/Echo_state_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Echo state network'},\n",
       " {'href': '/wiki/The_Emotion_Machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'The Emotion Machine'},\n",
       " {'href': '/wiki/Evolutionary_Acquisition_of_Neural_Topologies',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Evolutionary Acquisition of Neural Topologies'},\n",
       " {'href': '/wiki/Extension_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Extension neural network'},\n",
       " {'href': '/wiki/Feed_forward_(control)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Feed forward (control)'},\n",
       " {'href': '/wiki/Feedforward_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Feedforward neural network'},\n",
       " {'href': '/wiki/Generalized_Hebbian_Algorithm',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Generalized Hebbian Algorithm'},\n",
       " {'href': '/wiki/Generative_topographic_map',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Generative topographic map'},\n",
       " {'href': '/wiki/Group_method_of_data_handling',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Group method of data handling'},\n",
       " {'href': '/wiki/Growing_self-organizing_map',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Growing self-organizing map'},\n",
       " {'href': '/wiki/Memory-prediction_framework',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Memory-prediction framework'},\n",
       " {'href': '/wiki/Helmholtz_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Helmholtz machine'},\n",
       " {'href': '/wiki/Hierarchical_temporal_memory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Hierarchical temporal memory'},\n",
       " {'href': '/wiki/Hopfield_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Hopfield network'},\n",
       " {'href': '/wiki/Hybrid_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Hybrid neural network'},\n",
       " {'href': '/wiki/HyperNEAT',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'HyperNEAT'},\n",
       " {'href': '/wiki/Infomax', 'section': 'Neural Networks', 'title': 'Infomax'},\n",
       " {'href': '/wiki/Instantaneously_trained_neural_networks',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Instantaneously trained neural networks'},\n",
       " {'href': '/wiki/Interactive_Activation_and_Competition',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Interactive Activation and Competition'},\n",
       " {'href': '/wiki/Leabra', 'section': 'Neural Networks', 'title': 'Leabra'},\n",
       " {'href': '/wiki/Learning_Vector_Quantization',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Learning Vector Quantization'},\n",
       " {'href': '/wiki/Lernmatrix',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Lernmatrix'},\n",
       " {'href': '/wiki/Linde%E2%80%93Buzo%E2%80%93Gray_algorithm',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Linde–Buzo–Gray algorithm'},\n",
       " {'href': '/wiki/Liquid_state_machine',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Liquid state machine'},\n",
       " {'href': '/wiki/Long_short-term_memory',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Long short-term memory'},\n",
       " {'href': '/wiki/Madaline', 'section': 'Neural Networks', 'title': 'Madaline'},\n",
       " {'href': '/wiki/Modular_neural_networks',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Modular neural networks'},\n",
       " {'href': '/wiki/MoneyBee', 'section': 'Neural Networks', 'title': 'MoneyBee'},\n",
       " {'href': '/wiki/Neocognitron',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neocognitron'},\n",
       " {'href': '/wiki/Nervous_system_network_models',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Nervous system network models'},\n",
       " {'href': '/wiki/NETtalk_(artificial_neural_network)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'NETtalk (artificial neural network)'},\n",
       " {'href': '/wiki/Neural_backpropagation',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural backpropagation'},\n",
       " {'href': '/wiki/Neural_coding',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural coding'},\n",
       " {'href': '/wiki/Neural_cryptography',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural cryptography'},\n",
       " {'href': '/wiki/Neural_decoding',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural decoding'},\n",
       " {'href': '/wiki/Neural_gas',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural gas'},\n",
       " {'href': '/wiki/Neural_Information_Processing_Systems',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural Information Processing Systems'},\n",
       " {'href': '/wiki/Neural_modeling_fields',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural modeling fields'},\n",
       " {'href': '/wiki/Neural_oscillation',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neural oscillation'},\n",
       " {'href': '/wiki/Neurally_controlled_animat',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neurally controlled animat'},\n",
       " {'href': '/wiki/Neuroevolution_of_augmenting_topologies',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neuroevolution of augmenting topologies'},\n",
       " {'href': '/wiki/Neuroplasticity',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Neuroplasticity'},\n",
       " {'href': '/wiki/Ni1000', 'section': 'Neural Networks', 'title': 'Ni1000'},\n",
       " {'href': '/wiki/Nonspiking_neurons',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Nonspiking neurons'},\n",
       " {'href': '/wiki/Nonsynaptic_plasticity',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Nonsynaptic plasticity'},\n",
       " {'href': '/wiki/Oja%27s_rule',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': \"Oja's rule\"},\n",
       " {'href': '/wiki/Optical_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Optical neural network'},\n",
       " {'href': '/wiki/Phase-of-firing_code',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Phase-of-firing code'},\n",
       " {'href': '/wiki/Promoter_based_genetic_algorithm',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Promoter based genetic algorithm'},\n",
       " {'href': '/wiki/Pulse-coupled_networks',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Pulse-coupled networks'},\n",
       " {'href': '/wiki/Quantum_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Quantum neural network'},\n",
       " {'href': '/wiki/Radial_basis_function',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Radial basis function'},\n",
       " {'href': '/wiki/Radial_basis_function_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Radial basis function network'},\n",
       " {'href': '/wiki/Random_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Random neural network'},\n",
       " {'href': '/wiki/Recurrent_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Recurrent neural network'},\n",
       " {'href': '/wiki/Reentry_(neural_circuitry)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Reentry (neural circuitry)'},\n",
       " {'href': '/wiki/Reservoir_computing',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Reservoir computing'},\n",
       " {'href': '/wiki/Rprop', 'section': 'Neural Networks', 'title': 'Rprop'},\n",
       " {'href': '/wiki/Semantic_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Semantic neural network'},\n",
       " {'href': '/wiki/Sigmoid_function',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Sigmoid function'},\n",
       " {'href': '/wiki/SNARC', 'section': 'Neural Networks', 'title': 'SNARC'},\n",
       " {'href': '/wiki/Softmax_activation_function',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Softmax activation function'},\n",
       " {'href': '/wiki/Spiking_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Spiking neural network'},\n",
       " {'href': '/wiki/Stochastic_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Stochastic neural network'},\n",
       " {'href': '/wiki/Synaptic_plasticity',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Synaptic plasticity'},\n",
       " {'href': '/wiki/Synaptic_weight',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Synaptic weight'},\n",
       " {'href': '/wiki/Tensor_product_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Tensor product network'},\n",
       " {'href': '/wiki/Time_delay_neural_network',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Time delay neural network'},\n",
       " {'href': '/wiki/U-Matrix', 'section': 'Neural Networks', 'title': 'U-Matrix'},\n",
       " {'href': '/wiki/Universal_approximation_theorem',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Universal approximation theorem'},\n",
       " {'href': '/wiki/Winner-take-all_(computing)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Winner-take-all (computing)'},\n",
       " {'href': '/wiki/Winnow_(algorithm)',\n",
       "  'section': 'Neural Networks',\n",
       "  'title': 'Winnow (algorithm)'},\n",
       " {'href': '/wiki/Reinforcement_learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Reinforcement learning'},\n",
       " {'href': '/wiki/Markov_decision_process',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Markov decision process'},\n",
       " {'href': '/wiki/Bellman_equation',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Bellman equation'},\n",
       " {'href': '/wiki/Q-learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Q-learning'},\n",
       " {'href': '/wiki/Temporal_difference_learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Temporal difference learning'},\n",
       " {'href': '/wiki/SARSA',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'SARSA'},\n",
       " {'href': '/wiki/Multi-armed_bandit',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Multi-armed bandit'},\n",
       " {'href': '/wiki/Apprenticeship_learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Apprenticeship learning'},\n",
       " {'href': '/wiki/Predictive_learning',\n",
       "  'section': 'Reinforcement learning',\n",
       "  'title': 'Predictive learning'},\n",
       " {'href': '/wiki/Text_mining',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Text mining'},\n",
       " {'href': '/wiki/Natural_language_processing',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Natural language processing'},\n",
       " {'href': '/wiki/Document_classification',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Document classification'},\n",
       " {'href': '/wiki/Bag_of_words_model',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Bag of words model'},\n",
       " {'href': '/wiki/N-gram', 'section': 'Text Mining', 'title': 'N-gram'},\n",
       " {'href': '/wiki/Part-of-speech_tagging',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Part-of-speech tagging'},\n",
       " {'href': '/wiki/Sentiment_analysis',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Sentiment analysis'},\n",
       " {'href': '/wiki/Information_extraction',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Information extraction'},\n",
       " {'href': '/wiki/Topic_model',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Topic model'},\n",
       " {'href': '/wiki/Concept_mining',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Concept mining'},\n",
       " {'href': '/wiki/Semantic_analysis_(machine_learning)',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Semantic analysis (machine learning)'},\n",
       " {'href': '/wiki/Automatic_summarization',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Automatic summarization'},\n",
       " {'href': '/wiki/String_kernel',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'String kernel'},\n",
       " {'href': '/wiki/Biomedical_text_mining',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Biomedical text mining'},\n",
       " {'href': '/wiki/Never-Ending_Language_Learning',\n",
       "  'section': 'Text Mining',\n",
       "  'title': 'Never-Ending Language Learning'},\n",
       " {'href': '/wiki/Structure_mining',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Structure mining'},\n",
       " {'href': '/wiki/Structured_learning',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Structured learning'},\n",
       " {'href': '/wiki/Structured_prediction',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Structured prediction'},\n",
       " {'href': '/wiki/Sequence_mining',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Sequence mining'},\n",
       " {'href': '/wiki/Sequence_labeling',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Sequence labeling'},\n",
       " {'href': '/wiki/Process_mining',\n",
       "  'section': 'Structure Mining',\n",
       "  'title': 'Process mining'},\n",
       " {'href': '/wiki/Multi-label_classification',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Multi-label classification'},\n",
       " {'href': '/wiki/Automated_machine_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Automated machine learning'},\n",
       " {'href': '/wiki/Classifier_chains',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Classifier chains'},\n",
       " {'href': '/wiki/Web_mining',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Web mining'},\n",
       " {'href': '/wiki/Anomaly_detection',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Anomaly detection'},\n",
       " {'href': '/wiki/Anomaly_Detection_at_Multiple_Scales',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Anomaly Detection at Multiple Scales'},\n",
       " {'href': '/wiki/Local_outlier_factor',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Local outlier factor'},\n",
       " {'href': '/wiki/Novelty_detection',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Novelty detection'},\n",
       " {'href': '/wiki/GSP_Algorithm',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'GSP Algorithm'},\n",
       " {'href': '/wiki/Optimal_matching',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Optimal matching'},\n",
       " {'href': '/wiki/Record_linkage',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Record linkage'},\n",
       " {'href': '/wiki/Meta_learning_(computer_science)',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Meta learning (computer science)'},\n",
       " {'href': '/wiki/Learning_automata',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Learning automata'},\n",
       " {'href': '/wiki/Learning_to_rank',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Learning to rank'},\n",
       " {'href': '/wiki/Multiple-instance_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Multiple-instance learning'},\n",
       " {'href': '/wiki/Statistical_relational_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Statistical relational learning'},\n",
       " {'href': '/wiki/Relational_classification',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Relational classification'},\n",
       " {'href': '/wiki/Data_stream_mining',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Data stream mining'},\n",
       " {'href': '/wiki/Alpha_algorithm',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Alpha algorithm'},\n",
       " {'href': '/wiki/Syntactic_pattern_recognition',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Syntactic pattern recognition'},\n",
       " {'href': '/wiki/Multispectral_pattern_recognition',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Multispectral pattern recognition'},\n",
       " {'href': '/wiki/Algorithmic_learning_theory',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Algorithmic learning theory'},\n",
       " {'href': '/wiki/Deep_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Deep learning'},\n",
       " {'href': '/wiki/Bongard_problem',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Bongard problem'},\n",
       " {'href': '/wiki/Learning_with_errors',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Learning with errors'},\n",
       " {'href': '/wiki/Parity_learning',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Parity learning'},\n",
       " {'href': '/wiki/Inductive_transfer',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Inductive transfer'},\n",
       " {'href': '/wiki/Granular_computing',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Granular computing'},\n",
       " {'href': '/wiki/Conceptual_clustering',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Conceptual clustering'},\n",
       " {'href': '/wiki/Formal_concept_analysis',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Formal concept analysis'},\n",
       " {'href': '/wiki/Biclustering',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Biclustering'},\n",
       " {'href': '/wiki/Information_visualization',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Information visualization'},\n",
       " {'href': '/wiki/Co-occurrence_networks',\n",
       "  'section': 'Advanced Learning Tasks',\n",
       "  'title': 'Co-occurrence networks'},\n",
       " {'href': '/wiki/Problem_domain',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Problem domain'},\n",
       " {'href': '/wiki/Recommender_system',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Recommender system'},\n",
       " {'href': '/wiki/Collaborative_filtering',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Collaborative filtering'},\n",
       " {'href': '/wiki/Profiling_(information_science)',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Profiling (information science)'},\n",
       " {'href': '/wiki/Speech_recognition',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Speech recognition'},\n",
       " {'href': '/wiki/Stock_forecast',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Stock forecast'},\n",
       " {'href': '/wiki/Activity_recognition',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Activity recognition'},\n",
       " {'href': '/wiki/Data_Analysis_Techniques_for_Fraud_Detection',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Data Analysis Techniques for Fraud Detection'},\n",
       " {'href': '/wiki/Molecule_mining',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Molecule mining'},\n",
       " {'href': '/wiki/Behavioral_targeting',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Behavioral targeting'},\n",
       " {'href': '/wiki/Proactive_Discovery_of_Insider_Threats_Using_Graph_Analysis_and_Learning',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Proactive Discovery of Insider Threats Using Graph Analysis and Learning'},\n",
       " {'href': '/wiki/Robot_learning',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Robot learning'},\n",
       " {'href': '/wiki/Computer_vision',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Computer vision'},\n",
       " {'href': '/wiki/Facial_recognition_system',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Facial recognition system'},\n",
       " {'href': '/wiki/Outlier_detection',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Outlier detection'},\n",
       " {'href': '/wiki/Anomaly_detection',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Anomaly detection'},\n",
       " {'href': '/wiki/Novelty_detection',\n",
       "  'section': 'Applications',\n",
       "  'title': 'Novelty detection'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_links = [d for d in parsed_links if d is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_def_df = pd.DataFrame(parsed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>definition</th>\n",
       "      <th>href</th>\n",
       "      <th>section</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning is a field of computer scienc...</td>\n",
       "      <td>/wiki/Machine_learning</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Numerical analysis · Simulation</td>\n",
       "      <td>/wiki/Data_analysis</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Data analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Occam's razor (also Ockham's razor or Ocham's ...</td>\n",
       "      <td>/wiki/Occam%27s_razor</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Occam's razor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The curse of dimensionality refers to various ...</td>\n",
       "      <td>/wiki/Curse_of_dimensionality</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Curse of dimensionality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In mathematical folklore, the \"no free lunch\" ...</td>\n",
       "      <td>/wiki/No_free_lunch_theorem</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>No free lunch theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The accuracy paradox for predictive analytics ...</td>\n",
       "      <td>/wiki/Accuracy_paradox</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Accuracy paradox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In statistics, overfitting is \"the production ...</td>\n",
       "      <td>/wiki/Overfitting</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Overfitting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In mathematics, statistics, and computer scien...</td>\n",
       "      <td>/wiki/Regularization_(machine_learning)</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Regularization (machine learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The inductive bias (also known as learning bia...</td>\n",
       "      <td>/wiki/Inductive_bias</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Inductive bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data dredging (also data fishing, data snoopin...</td>\n",
       "      <td>/wiki/Data_dredging</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Data dredging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Ugly Duckling theorem is an argument asser...</td>\n",
       "      <td>/wiki/Ugly_duckling_theorem</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Ugly duckling theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>In computer science, uncertain data is data th...</td>\n",
       "      <td>/wiki/Uncertain_data</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Uncertain data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Knowledge extraction is the creation of knowle...</td>\n",
       "      <td>/wiki/Knowledge_discovery</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Knowledge discovery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data mining is the process of discovering patt...</td>\n",
       "      <td>/wiki/Data_mining</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Data mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td>/wiki/Predictive_analytics</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Predictive analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Predictive modelling uses statistics to predic...</td>\n",
       "      <td>/wiki/Predictive_modelling</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Predictive modelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Business intelligence (BI) comprises the strat...</td>\n",
       "      <td>/wiki/Business_intelligence</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Business intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LIONsolver is an integrated software for data ...</td>\n",
       "      <td>/wiki/Reactive_business_intelligence</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Reactive business intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Business analytics (BA) refers to the skills, ...</td>\n",
       "      <td>/wiki/Business_analytics</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Business analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LIONsolver is an integrated software for data ...</td>\n",
       "      <td>/wiki/Reactive_business_intelligence</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Reactive business intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Pattern recognition is a branch of machine lea...</td>\n",
       "      <td>/wiki/Pattern_recognition</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Pattern recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Abductive reasoning (also called abduction,[1]...</td>\n",
       "      <td>/wiki/Abductive_reasoning</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Abductive reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Inductive reasoning (as opposed to deductive r...</td>\n",
       "      <td>/wiki/Inductive_reasoning</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Inductive reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>First-order logic—also known as first-order pr...</td>\n",
       "      <td>/wiki/First-order_logic</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>First-order logic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Inductive logic programming (ILP) is a subfiel...</td>\n",
       "      <td>/wiki/Inductive_logic_programming</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Inductive logic programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>In information technology a reasoning system i...</td>\n",
       "      <td>/wiki/Reasoning_system</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Reasoning system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Case-based reasoning (CBR), broadly construed,...</td>\n",
       "      <td>/wiki/Case-based_reasoning</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Case-based reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Textual case-based reasoning is a subtopic of ...</td>\n",
       "      <td>/wiki/Textual_case_based_reasoning</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Textual case based reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Causality (also referred to as causation,[1] o...</td>\n",
       "      <td>/wiki/Causality</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Causality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Nearest neighbor search (NNS), as a form of pr...</td>\n",
       "      <td>/wiki/Nearest_neighbor_search</td>\n",
       "      <td>Search Methods</td>\n",
       "      <td>Nearest neighbor search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Multispectral_pattern_recognition</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Multispectral pattern recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Algorithmic_learning_theory</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Algorithmic learning theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Deep_learning</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Bongard_problem</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Bongard problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Learning_with_errors</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Learning with errors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Parity_learning</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Parity learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Inductive_transfer</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Inductive transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Granular_computing</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Granular computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Conceptual_clustering</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Conceptual clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Formal_concept_analysis</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Formal concept analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Biclustering</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Biclustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Information_visualization</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Information visualization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Co-occurrence_networks</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Co-occurrence networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Problem_domain</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Problem domain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Recommender_system</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Recommender system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Collaborative_filtering</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Collaborative filtering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Profiling_(information_science)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Profiling (information science)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Speech_recognition</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Speech recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Stock_forecast</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Stock forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Activity_recognition</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Activity recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Data_Analysis_Techniques_for_Fraud_Detec...</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Data Analysis Techniques for Fraud Detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Molecule_mining</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Molecule mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Behavioral_targeting</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Behavioral targeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Proactive_Discovery_of_Insider_Threats_U...</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Proactive Discovery of Insider Threats Using G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Robot_learning</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Robot learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Computer_vision</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Computer vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Facial_recognition_system</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Facial recognition system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Outlier_detection</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Outlier detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Anomaly_detection</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Anomaly detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Novelty_detection</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Novelty detection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            definition  \\\n",
       "0    Machine learning is a field of computer scienc...   \n",
       "1                      Numerical analysis · Simulation   \n",
       "2    Occam's razor (also Ockham's razor or Ocham's ...   \n",
       "3    The curse of dimensionality refers to various ...   \n",
       "4    In mathematical folklore, the \"no free lunch\" ...   \n",
       "5    The accuracy paradox for predictive analytics ...   \n",
       "6    In statistics, overfitting is \"the production ...   \n",
       "7    In mathematics, statistics, and computer scien...   \n",
       "8    The inductive bias (also known as learning bia...   \n",
       "9    Data dredging (also data fishing, data snoopin...   \n",
       "10   The Ugly Duckling theorem is an argument asser...   \n",
       "11   In computer science, uncertain data is data th...   \n",
       "12   Knowledge extraction is the creation of knowle...   \n",
       "13   Data mining is the process of discovering patt...   \n",
       "14   Predictive analytics encompasses a variety of ...   \n",
       "15   Predictive modelling uses statistics to predic...   \n",
       "16   Business intelligence (BI) comprises the strat...   \n",
       "17   LIONsolver is an integrated software for data ...   \n",
       "18   Business analytics (BA) refers to the skills, ...   \n",
       "19   LIONsolver is an integrated software for data ...   \n",
       "20   Pattern recognition is a branch of machine lea...   \n",
       "21   Abductive reasoning (also called abduction,[1]...   \n",
       "22   Inductive reasoning (as opposed to deductive r...   \n",
       "23   First-order logic—also known as first-order pr...   \n",
       "24   Inductive logic programming (ILP) is a subfiel...   \n",
       "25   In information technology a reasoning system i...   \n",
       "26   Case-based reasoning (CBR), broadly construed,...   \n",
       "27   Textual case-based reasoning is a subtopic of ...   \n",
       "28   Causality (also referred to as causation,[1] o...   \n",
       "29   Nearest neighbor search (NNS), as a form of pr...   \n",
       "..                                                 ...   \n",
       "530                                                NaN   \n",
       "531                                                NaN   \n",
       "532                                                NaN   \n",
       "533                                                NaN   \n",
       "534                                                NaN   \n",
       "535                                                NaN   \n",
       "536                                                NaN   \n",
       "537                                                NaN   \n",
       "538                                                NaN   \n",
       "539                                                NaN   \n",
       "540                                                NaN   \n",
       "541                                                NaN   \n",
       "542                                                NaN   \n",
       "543                                                NaN   \n",
       "544                                                NaN   \n",
       "545                                                NaN   \n",
       "546                                                NaN   \n",
       "547                                                NaN   \n",
       "548                                                NaN   \n",
       "549                                                NaN   \n",
       "550                                                NaN   \n",
       "551                                                NaN   \n",
       "552                                                NaN   \n",
       "553                                                NaN   \n",
       "554                                                NaN   \n",
       "555                                                NaN   \n",
       "556                                                NaN   \n",
       "557                                                NaN   \n",
       "558                                                NaN   \n",
       "559                                                NaN   \n",
       "\n",
       "                                                  href  \\\n",
       "0                               /wiki/Machine_learning   \n",
       "1                                  /wiki/Data_analysis   \n",
       "2                                /wiki/Occam%27s_razor   \n",
       "3                        /wiki/Curse_of_dimensionality   \n",
       "4                          /wiki/No_free_lunch_theorem   \n",
       "5                               /wiki/Accuracy_paradox   \n",
       "6                                    /wiki/Overfitting   \n",
       "7              /wiki/Regularization_(machine_learning)   \n",
       "8                                 /wiki/Inductive_bias   \n",
       "9                                  /wiki/Data_dredging   \n",
       "10                         /wiki/Ugly_duckling_theorem   \n",
       "11                                /wiki/Uncertain_data   \n",
       "12                           /wiki/Knowledge_discovery   \n",
       "13                                   /wiki/Data_mining   \n",
       "14                          /wiki/Predictive_analytics   \n",
       "15                          /wiki/Predictive_modelling   \n",
       "16                         /wiki/Business_intelligence   \n",
       "17                /wiki/Reactive_business_intelligence   \n",
       "18                            /wiki/Business_analytics   \n",
       "19                /wiki/Reactive_business_intelligence   \n",
       "20                           /wiki/Pattern_recognition   \n",
       "21                           /wiki/Abductive_reasoning   \n",
       "22                           /wiki/Inductive_reasoning   \n",
       "23                             /wiki/First-order_logic   \n",
       "24                   /wiki/Inductive_logic_programming   \n",
       "25                              /wiki/Reasoning_system   \n",
       "26                          /wiki/Case-based_reasoning   \n",
       "27                  /wiki/Textual_case_based_reasoning   \n",
       "28                                     /wiki/Causality   \n",
       "29                       /wiki/Nearest_neighbor_search   \n",
       "..                                                 ...   \n",
       "530            /wiki/Multispectral_pattern_recognition   \n",
       "531                  /wiki/Algorithmic_learning_theory   \n",
       "532                                /wiki/Deep_learning   \n",
       "533                              /wiki/Bongard_problem   \n",
       "534                         /wiki/Learning_with_errors   \n",
       "535                              /wiki/Parity_learning   \n",
       "536                           /wiki/Inductive_transfer   \n",
       "537                           /wiki/Granular_computing   \n",
       "538                        /wiki/Conceptual_clustering   \n",
       "539                      /wiki/Formal_concept_analysis   \n",
       "540                                 /wiki/Biclustering   \n",
       "541                    /wiki/Information_visualization   \n",
       "542                       /wiki/Co-occurrence_networks   \n",
       "543                               /wiki/Problem_domain   \n",
       "544                           /wiki/Recommender_system   \n",
       "545                      /wiki/Collaborative_filtering   \n",
       "546              /wiki/Profiling_(information_science)   \n",
       "547                           /wiki/Speech_recognition   \n",
       "548                               /wiki/Stock_forecast   \n",
       "549                         /wiki/Activity_recognition   \n",
       "550  /wiki/Data_Analysis_Techniques_for_Fraud_Detec...   \n",
       "551                              /wiki/Molecule_mining   \n",
       "552                         /wiki/Behavioral_targeting   \n",
       "553  /wiki/Proactive_Discovery_of_Insider_Threats_U...   \n",
       "554                               /wiki/Robot_learning   \n",
       "555                              /wiki/Computer_vision   \n",
       "556                    /wiki/Facial_recognition_system   \n",
       "557                            /wiki/Outlier_detection   \n",
       "558                            /wiki/Anomaly_detection   \n",
       "559                            /wiki/Novelty_detection   \n",
       "\n",
       "                              section  \\\n",
       "0    Introduction and Main Principles   \n",
       "1    Introduction and Main Principles   \n",
       "2    Introduction and Main Principles   \n",
       "3    Introduction and Main Principles   \n",
       "4    Introduction and Main Principles   \n",
       "5    Introduction and Main Principles   \n",
       "6    Introduction and Main Principles   \n",
       "7    Introduction and Main Principles   \n",
       "8    Introduction and Main Principles   \n",
       "9    Introduction and Main Principles   \n",
       "10   Introduction and Main Principles   \n",
       "11   Introduction and Main Principles   \n",
       "12       Background and Preliminaries   \n",
       "13       Background and Preliminaries   \n",
       "14       Background and Preliminaries   \n",
       "15       Background and Preliminaries   \n",
       "16       Background and Preliminaries   \n",
       "17       Background and Preliminaries   \n",
       "18       Background and Preliminaries   \n",
       "19       Background and Preliminaries   \n",
       "20       Background and Preliminaries   \n",
       "21                          Reasoning   \n",
       "22                          Reasoning   \n",
       "23                          Reasoning   \n",
       "24                          Reasoning   \n",
       "25                          Reasoning   \n",
       "26                          Reasoning   \n",
       "27                          Reasoning   \n",
       "28                          Reasoning   \n",
       "29                     Search Methods   \n",
       "..                                ...   \n",
       "530           Advanced Learning Tasks   \n",
       "531           Advanced Learning Tasks   \n",
       "532           Advanced Learning Tasks   \n",
       "533           Advanced Learning Tasks   \n",
       "534           Advanced Learning Tasks   \n",
       "535           Advanced Learning Tasks   \n",
       "536           Advanced Learning Tasks   \n",
       "537           Advanced Learning Tasks   \n",
       "538           Advanced Learning Tasks   \n",
       "539           Advanced Learning Tasks   \n",
       "540           Advanced Learning Tasks   \n",
       "541           Advanced Learning Tasks   \n",
       "542           Advanced Learning Tasks   \n",
       "543                      Applications   \n",
       "544                      Applications   \n",
       "545                      Applications   \n",
       "546                      Applications   \n",
       "547                      Applications   \n",
       "548                      Applications   \n",
       "549                      Applications   \n",
       "550                      Applications   \n",
       "551                      Applications   \n",
       "552                      Applications   \n",
       "553                      Applications   \n",
       "554                      Applications   \n",
       "555                      Applications   \n",
       "556                      Applications   \n",
       "557                      Applications   \n",
       "558                      Applications   \n",
       "559                      Applications   \n",
       "\n",
       "                                                 title  \n",
       "0                                     Machine learning  \n",
       "1                                        Data analysis  \n",
       "2                                        Occam's razor  \n",
       "3                              Curse of dimensionality  \n",
       "4                                No free lunch theorem  \n",
       "5                                     Accuracy paradox  \n",
       "6                                          Overfitting  \n",
       "7                    Regularization (machine learning)  \n",
       "8                                       Inductive bias  \n",
       "9                                        Data dredging  \n",
       "10                               Ugly duckling theorem  \n",
       "11                                      Uncertain data  \n",
       "12                                 Knowledge discovery  \n",
       "13                                         Data mining  \n",
       "14                                Predictive analytics  \n",
       "15                                Predictive modelling  \n",
       "16                               Business intelligence  \n",
       "17                      Reactive business intelligence  \n",
       "18                                  Business analytics  \n",
       "19                      Reactive business intelligence  \n",
       "20                                 Pattern recognition  \n",
       "21                                 Abductive reasoning  \n",
       "22                                 Inductive reasoning  \n",
       "23                                   First-order logic  \n",
       "24                         Inductive logic programming  \n",
       "25                                    Reasoning system  \n",
       "26                                Case-based reasoning  \n",
       "27                        Textual case based reasoning  \n",
       "28                                           Causality  \n",
       "29                             Nearest neighbor search  \n",
       "..                                                 ...  \n",
       "530                  Multispectral pattern recognition  \n",
       "531                        Algorithmic learning theory  \n",
       "532                                      Deep learning  \n",
       "533                                    Bongard problem  \n",
       "534                               Learning with errors  \n",
       "535                                    Parity learning  \n",
       "536                                 Inductive transfer  \n",
       "537                                 Granular computing  \n",
       "538                              Conceptual clustering  \n",
       "539                            Formal concept analysis  \n",
       "540                                       Biclustering  \n",
       "541                          Information visualization  \n",
       "542                             Co-occurrence networks  \n",
       "543                                     Problem domain  \n",
       "544                                 Recommender system  \n",
       "545                            Collaborative filtering  \n",
       "546                    Profiling (information science)  \n",
       "547                                 Speech recognition  \n",
       "548                                     Stock forecast  \n",
       "549                               Activity recognition  \n",
       "550       Data Analysis Techniques for Fraud Detection  \n",
       "551                                    Molecule mining  \n",
       "552                               Behavioral targeting  \n",
       "553  Proactive Discovery of Insider Threats Using G...  \n",
       "554                                     Robot learning  \n",
       "555                                    Computer vision  \n",
       "556                          Facial recognition system  \n",
       "557                                  Outlier detection  \n",
       "558                                  Anomaly detection  \n",
       "559                                  Novelty detection  \n",
       "\n",
       "[560 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_def_df.to_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see if pickling worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/180530_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>definition</th>\n",
       "      <th>href</th>\n",
       "      <th>section</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning is a field of computer scienc...</td>\n",
       "      <td>/wiki/Machine_learning</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Numerical analysis · Simulation</td>\n",
       "      <td>/wiki/Data_analysis</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Data analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Occam's razor (also Ockham's razor or Ocham's ...</td>\n",
       "      <td>/wiki/Occam%27s_razor</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Occam's razor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The curse of dimensionality refers to various ...</td>\n",
       "      <td>/wiki/Curse_of_dimensionality</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Curse of dimensionality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In mathematical folklore, the \"no free lunch\" ...</td>\n",
       "      <td>/wiki/No_free_lunch_theorem</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>No free lunch theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The accuracy paradox for predictive analytics ...</td>\n",
       "      <td>/wiki/Accuracy_paradox</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Accuracy paradox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In statistics, overfitting is \"the production ...</td>\n",
       "      <td>/wiki/Overfitting</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Overfitting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In mathematics, statistics, and computer scien...</td>\n",
       "      <td>/wiki/Regularization_(machine_learning)</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Regularization (machine learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The inductive bias (also known as learning bia...</td>\n",
       "      <td>/wiki/Inductive_bias</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Inductive bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data dredging (also data fishing, data snoopin...</td>\n",
       "      <td>/wiki/Data_dredging</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Data dredging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Ugly Duckling theorem is an argument asser...</td>\n",
       "      <td>/wiki/Ugly_duckling_theorem</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Ugly duckling theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>In computer science, uncertain data is data th...</td>\n",
       "      <td>/wiki/Uncertain_data</td>\n",
       "      <td>Introduction and Main Principles</td>\n",
       "      <td>Uncertain data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Knowledge extraction is the creation of knowle...</td>\n",
       "      <td>/wiki/Knowledge_discovery</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Knowledge discovery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data mining is the process of discovering patt...</td>\n",
       "      <td>/wiki/Data_mining</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Data mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td>/wiki/Predictive_analytics</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Predictive analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Predictive modelling uses statistics to predic...</td>\n",
       "      <td>/wiki/Predictive_modelling</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Predictive modelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Business intelligence (BI) comprises the strat...</td>\n",
       "      <td>/wiki/Business_intelligence</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Business intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LIONsolver is an integrated software for data ...</td>\n",
       "      <td>/wiki/Reactive_business_intelligence</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Reactive business intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Business analytics (BA) refers to the skills, ...</td>\n",
       "      <td>/wiki/Business_analytics</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Business analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LIONsolver is an integrated software for data ...</td>\n",
       "      <td>/wiki/Reactive_business_intelligence</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Reactive business intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Pattern recognition is a branch of machine lea...</td>\n",
       "      <td>/wiki/Pattern_recognition</td>\n",
       "      <td>Background and Preliminaries</td>\n",
       "      <td>Pattern recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Abductive reasoning (also called abduction,[1]...</td>\n",
       "      <td>/wiki/Abductive_reasoning</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Abductive reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Inductive reasoning (as opposed to deductive r...</td>\n",
       "      <td>/wiki/Inductive_reasoning</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Inductive reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>First-order logic—also known as first-order pr...</td>\n",
       "      <td>/wiki/First-order_logic</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>First-order logic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Inductive logic programming (ILP) is a subfiel...</td>\n",
       "      <td>/wiki/Inductive_logic_programming</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Inductive logic programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>In information technology a reasoning system i...</td>\n",
       "      <td>/wiki/Reasoning_system</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Reasoning system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Case-based reasoning (CBR), broadly construed,...</td>\n",
       "      <td>/wiki/Case-based_reasoning</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Case-based reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Textual case-based reasoning is a subtopic of ...</td>\n",
       "      <td>/wiki/Textual_case_based_reasoning</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Textual case based reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Causality (also referred to as causation,[1] o...</td>\n",
       "      <td>/wiki/Causality</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Causality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Nearest neighbor search (NNS), as a form of pr...</td>\n",
       "      <td>/wiki/Nearest_neighbor_search</td>\n",
       "      <td>Search Methods</td>\n",
       "      <td>Nearest neighbor search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Multispectral remote sensing is the collection...</td>\n",
       "      <td>/wiki/Multispectral_pattern_recognition</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Multispectral pattern recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>Algorithmic learning theory is a mathematical ...</td>\n",
       "      <td>/wiki/Algorithmic_learning_theory</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Algorithmic learning theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>Deep learning (also known as deep structured l...</td>\n",
       "      <td>/wiki/Deep_learning</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>A Bongard problem is a kind of puzzle invented...</td>\n",
       "      <td>/wiki/Bongard_problem</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Bongard problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Learning with errors (LWE) is a problem in mac...</td>\n",
       "      <td>/wiki/Learning_with_errors</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Learning with errors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Parity learning is a problem in machine learni...</td>\n",
       "      <td>/wiki/Parity_learning</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Parity learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Transfer learning or inductive transfer is a r...</td>\n",
       "      <td>/wiki/Inductive_transfer</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Inductive transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Granular computing (GrC) is an emerging comput...</td>\n",
       "      <td>/wiki/Granular_computing</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Granular computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Conceptual clustering is a machine learning pa...</td>\n",
       "      <td>/wiki/Conceptual_clustering</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Conceptual clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>Formal concept analysis (FCA) is a principled ...</td>\n",
       "      <td>/wiki/Formal_concept_analysis</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Formal concept analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>Biclustering, block clustering ,[1] [2] co-clu...</td>\n",
       "      <td>/wiki/Biclustering</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Biclustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>Information visualization or information visua...</td>\n",
       "      <td>/wiki/Information_visualization</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Information visualization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>Co-occurrence networks are generally used to p...</td>\n",
       "      <td>/wiki/Co-occurrence_networks</td>\n",
       "      <td>Advanced Learning Tasks</td>\n",
       "      <td>Co-occurrence networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>A problem domain is the area of expertise or a...</td>\n",
       "      <td>/wiki/Problem_domain</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Problem domain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>A recommender system or a recommendation syste...</td>\n",
       "      <td>/wiki/Recommender_system</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Recommender system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Collaborative filtering (CF) is a technique us...</td>\n",
       "      <td>/wiki/Collaborative_filtering</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Collaborative filtering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>In information science, profiling refers to th...</td>\n",
       "      <td>/wiki/Profiling_(information_science)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Profiling (information science)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Speech recognition is the inter-disciplinary s...</td>\n",
       "      <td>/wiki/Speech_recognition</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Speech recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>Stock Forecast can refer to:</td>\n",
       "      <td>/wiki/Stock_forecast</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Stock forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Activity recognition aims to recognize the act...</td>\n",
       "      <td>/wiki/Activity_recognition</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Activity recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>Fraud is a billion-dollar business and it is i...</td>\n",
       "      <td>/wiki/Data_Analysis_Techniques_for_Fraud_Detec...</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Data Analysis Techniques for Fraud Detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>This page describes mining for molecules. Sinc...</td>\n",
       "      <td>/wiki/Molecule_mining</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Molecule mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Behavioral targeting comprises a range of tech...</td>\n",
       "      <td>/wiki/Behavioral_targeting</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Behavioral targeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>PRODIGAL (Proactive discovery of insider threa...</td>\n",
       "      <td>/wiki/Proactive_Discovery_of_Insider_Threats_U...</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Proactive Discovery of Insider Threats Using G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Robot learning is a research field at the inte...</td>\n",
       "      <td>/wiki/Robot_learning</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Robot learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Computer vision is an interdisciplinary field ...</td>\n",
       "      <td>/wiki/Computer_vision</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Computer vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>A facial recognition system is a technology ca...</td>\n",
       "      <td>/wiki/Facial_recognition_system</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Facial recognition system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>In data mining, anomaly detection (also outlie...</td>\n",
       "      <td>/wiki/Outlier_detection</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Outlier detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>In data mining, anomaly detection (also outlie...</td>\n",
       "      <td>/wiki/Anomaly_detection</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Anomaly detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>In data mining, anomaly detection (also outlie...</td>\n",
       "      <td>/wiki/Novelty_detection</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Novelty detection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            definition  \\\n",
       "0    Machine learning is a field of computer scienc...   \n",
       "1                      Numerical analysis · Simulation   \n",
       "2    Occam's razor (also Ockham's razor or Ocham's ...   \n",
       "3    The curse of dimensionality refers to various ...   \n",
       "4    In mathematical folklore, the \"no free lunch\" ...   \n",
       "5    The accuracy paradox for predictive analytics ...   \n",
       "6    In statistics, overfitting is \"the production ...   \n",
       "7    In mathematics, statistics, and computer scien...   \n",
       "8    The inductive bias (also known as learning bia...   \n",
       "9    Data dredging (also data fishing, data snoopin...   \n",
       "10   The Ugly Duckling theorem is an argument asser...   \n",
       "11   In computer science, uncertain data is data th...   \n",
       "12   Knowledge extraction is the creation of knowle...   \n",
       "13   Data mining is the process of discovering patt...   \n",
       "14   Predictive analytics encompasses a variety of ...   \n",
       "15   Predictive modelling uses statistics to predic...   \n",
       "16   Business intelligence (BI) comprises the strat...   \n",
       "17   LIONsolver is an integrated software for data ...   \n",
       "18   Business analytics (BA) refers to the skills, ...   \n",
       "19   LIONsolver is an integrated software for data ...   \n",
       "20   Pattern recognition is a branch of machine lea...   \n",
       "21   Abductive reasoning (also called abduction,[1]...   \n",
       "22   Inductive reasoning (as opposed to deductive r...   \n",
       "23   First-order logic—also known as first-order pr...   \n",
       "24   Inductive logic programming (ILP) is a subfiel...   \n",
       "25   In information technology a reasoning system i...   \n",
       "26   Case-based reasoning (CBR), broadly construed,...   \n",
       "27   Textual case-based reasoning is a subtopic of ...   \n",
       "28   Causality (also referred to as causation,[1] o...   \n",
       "29   Nearest neighbor search (NNS), as a form of pr...   \n",
       "..                                                 ...   \n",
       "530  Multispectral remote sensing is the collection...   \n",
       "531  Algorithmic learning theory is a mathematical ...   \n",
       "532  Deep learning (also known as deep structured l...   \n",
       "533  A Bongard problem is a kind of puzzle invented...   \n",
       "534  Learning with errors (LWE) is a problem in mac...   \n",
       "535  Parity learning is a problem in machine learni...   \n",
       "536  Transfer learning or inductive transfer is a r...   \n",
       "537  Granular computing (GrC) is an emerging comput...   \n",
       "538  Conceptual clustering is a machine learning pa...   \n",
       "539  Formal concept analysis (FCA) is a principled ...   \n",
       "540  Biclustering, block clustering ,[1] [2] co-clu...   \n",
       "541  Information visualization or information visua...   \n",
       "542  Co-occurrence networks are generally used to p...   \n",
       "543  A problem domain is the area of expertise or a...   \n",
       "544  A recommender system or a recommendation syste...   \n",
       "545  Collaborative filtering (CF) is a technique us...   \n",
       "546  In information science, profiling refers to th...   \n",
       "547  Speech recognition is the inter-disciplinary s...   \n",
       "548                       Stock Forecast can refer to:   \n",
       "549  Activity recognition aims to recognize the act...   \n",
       "550  Fraud is a billion-dollar business and it is i...   \n",
       "551  This page describes mining for molecules. Sinc...   \n",
       "552  Behavioral targeting comprises a range of tech...   \n",
       "553  PRODIGAL (Proactive discovery of insider threa...   \n",
       "554  Robot learning is a research field at the inte...   \n",
       "555  Computer vision is an interdisciplinary field ...   \n",
       "556  A facial recognition system is a technology ca...   \n",
       "557  In data mining, anomaly detection (also outlie...   \n",
       "558  In data mining, anomaly detection (also outlie...   \n",
       "559  In data mining, anomaly detection (also outlie...   \n",
       "\n",
       "                                                  href  \\\n",
       "0                               /wiki/Machine_learning   \n",
       "1                                  /wiki/Data_analysis   \n",
       "2                                /wiki/Occam%27s_razor   \n",
       "3                        /wiki/Curse_of_dimensionality   \n",
       "4                          /wiki/No_free_lunch_theorem   \n",
       "5                               /wiki/Accuracy_paradox   \n",
       "6                                    /wiki/Overfitting   \n",
       "7              /wiki/Regularization_(machine_learning)   \n",
       "8                                 /wiki/Inductive_bias   \n",
       "9                                  /wiki/Data_dredging   \n",
       "10                         /wiki/Ugly_duckling_theorem   \n",
       "11                                /wiki/Uncertain_data   \n",
       "12                           /wiki/Knowledge_discovery   \n",
       "13                                   /wiki/Data_mining   \n",
       "14                          /wiki/Predictive_analytics   \n",
       "15                          /wiki/Predictive_modelling   \n",
       "16                         /wiki/Business_intelligence   \n",
       "17                /wiki/Reactive_business_intelligence   \n",
       "18                            /wiki/Business_analytics   \n",
       "19                /wiki/Reactive_business_intelligence   \n",
       "20                           /wiki/Pattern_recognition   \n",
       "21                           /wiki/Abductive_reasoning   \n",
       "22                           /wiki/Inductive_reasoning   \n",
       "23                             /wiki/First-order_logic   \n",
       "24                   /wiki/Inductive_logic_programming   \n",
       "25                              /wiki/Reasoning_system   \n",
       "26                          /wiki/Case-based_reasoning   \n",
       "27                  /wiki/Textual_case_based_reasoning   \n",
       "28                                     /wiki/Causality   \n",
       "29                       /wiki/Nearest_neighbor_search   \n",
       "..                                                 ...   \n",
       "530            /wiki/Multispectral_pattern_recognition   \n",
       "531                  /wiki/Algorithmic_learning_theory   \n",
       "532                                /wiki/Deep_learning   \n",
       "533                              /wiki/Bongard_problem   \n",
       "534                         /wiki/Learning_with_errors   \n",
       "535                              /wiki/Parity_learning   \n",
       "536                           /wiki/Inductive_transfer   \n",
       "537                           /wiki/Granular_computing   \n",
       "538                        /wiki/Conceptual_clustering   \n",
       "539                      /wiki/Formal_concept_analysis   \n",
       "540                                 /wiki/Biclustering   \n",
       "541                    /wiki/Information_visualization   \n",
       "542                       /wiki/Co-occurrence_networks   \n",
       "543                               /wiki/Problem_domain   \n",
       "544                           /wiki/Recommender_system   \n",
       "545                      /wiki/Collaborative_filtering   \n",
       "546              /wiki/Profiling_(information_science)   \n",
       "547                           /wiki/Speech_recognition   \n",
       "548                               /wiki/Stock_forecast   \n",
       "549                         /wiki/Activity_recognition   \n",
       "550  /wiki/Data_Analysis_Techniques_for_Fraud_Detec...   \n",
       "551                              /wiki/Molecule_mining   \n",
       "552                         /wiki/Behavioral_targeting   \n",
       "553  /wiki/Proactive_Discovery_of_Insider_Threats_U...   \n",
       "554                               /wiki/Robot_learning   \n",
       "555                              /wiki/Computer_vision   \n",
       "556                    /wiki/Facial_recognition_system   \n",
       "557                            /wiki/Outlier_detection   \n",
       "558                            /wiki/Anomaly_detection   \n",
       "559                            /wiki/Novelty_detection   \n",
       "\n",
       "                              section  \\\n",
       "0    Introduction and Main Principles   \n",
       "1    Introduction and Main Principles   \n",
       "2    Introduction and Main Principles   \n",
       "3    Introduction and Main Principles   \n",
       "4    Introduction and Main Principles   \n",
       "5    Introduction and Main Principles   \n",
       "6    Introduction and Main Principles   \n",
       "7    Introduction and Main Principles   \n",
       "8    Introduction and Main Principles   \n",
       "9    Introduction and Main Principles   \n",
       "10   Introduction and Main Principles   \n",
       "11   Introduction and Main Principles   \n",
       "12       Background and Preliminaries   \n",
       "13       Background and Preliminaries   \n",
       "14       Background and Preliminaries   \n",
       "15       Background and Preliminaries   \n",
       "16       Background and Preliminaries   \n",
       "17       Background and Preliminaries   \n",
       "18       Background and Preliminaries   \n",
       "19       Background and Preliminaries   \n",
       "20       Background and Preliminaries   \n",
       "21                          Reasoning   \n",
       "22                          Reasoning   \n",
       "23                          Reasoning   \n",
       "24                          Reasoning   \n",
       "25                          Reasoning   \n",
       "26                          Reasoning   \n",
       "27                          Reasoning   \n",
       "28                          Reasoning   \n",
       "29                     Search Methods   \n",
       "..                                ...   \n",
       "530           Advanced Learning Tasks   \n",
       "531           Advanced Learning Tasks   \n",
       "532           Advanced Learning Tasks   \n",
       "533           Advanced Learning Tasks   \n",
       "534           Advanced Learning Tasks   \n",
       "535           Advanced Learning Tasks   \n",
       "536           Advanced Learning Tasks   \n",
       "537           Advanced Learning Tasks   \n",
       "538           Advanced Learning Tasks   \n",
       "539           Advanced Learning Tasks   \n",
       "540           Advanced Learning Tasks   \n",
       "541           Advanced Learning Tasks   \n",
       "542           Advanced Learning Tasks   \n",
       "543                      Applications   \n",
       "544                      Applications   \n",
       "545                      Applications   \n",
       "546                      Applications   \n",
       "547                      Applications   \n",
       "548                      Applications   \n",
       "549                      Applications   \n",
       "550                      Applications   \n",
       "551                      Applications   \n",
       "552                      Applications   \n",
       "553                      Applications   \n",
       "554                      Applications   \n",
       "555                      Applications   \n",
       "556                      Applications   \n",
       "557                      Applications   \n",
       "558                      Applications   \n",
       "559                      Applications   \n",
       "\n",
       "                                                 title  \n",
       "0                                     Machine learning  \n",
       "1                                        Data analysis  \n",
       "2                                        Occam's razor  \n",
       "3                              Curse of dimensionality  \n",
       "4                                No free lunch theorem  \n",
       "5                                     Accuracy paradox  \n",
       "6                                          Overfitting  \n",
       "7                    Regularization (machine learning)  \n",
       "8                                       Inductive bias  \n",
       "9                                        Data dredging  \n",
       "10                               Ugly duckling theorem  \n",
       "11                                      Uncertain data  \n",
       "12                                 Knowledge discovery  \n",
       "13                                         Data mining  \n",
       "14                                Predictive analytics  \n",
       "15                                Predictive modelling  \n",
       "16                               Business intelligence  \n",
       "17                      Reactive business intelligence  \n",
       "18                                  Business analytics  \n",
       "19                      Reactive business intelligence  \n",
       "20                                 Pattern recognition  \n",
       "21                                 Abductive reasoning  \n",
       "22                                 Inductive reasoning  \n",
       "23                                   First-order logic  \n",
       "24                         Inductive logic programming  \n",
       "25                                    Reasoning system  \n",
       "26                                Case-based reasoning  \n",
       "27                        Textual case based reasoning  \n",
       "28                                           Causality  \n",
       "29                             Nearest neighbor search  \n",
       "..                                                 ...  \n",
       "530                  Multispectral pattern recognition  \n",
       "531                        Algorithmic learning theory  \n",
       "532                                      Deep learning  \n",
       "533                                    Bongard problem  \n",
       "534                               Learning with errors  \n",
       "535                                    Parity learning  \n",
       "536                                 Inductive transfer  \n",
       "537                                 Granular computing  \n",
       "538                              Conceptual clustering  \n",
       "539                            Formal concept analysis  \n",
       "540                                       Biclustering  \n",
       "541                          Information visualization  \n",
       "542                             Co-occurrence networks  \n",
       "543                                     Problem domain  \n",
       "544                                 Recommender system  \n",
       "545                            Collaborative filtering  \n",
       "546                    Profiling (information science)  \n",
       "547                                 Speech recognition  \n",
       "548                                     Stock forecast  \n",
       "549                               Activity recognition  \n",
       "550       Data Analysis Techniques for Fraud Detection  \n",
       "551                                    Molecule mining  \n",
       "552                               Behavioral targeting  \n",
       "553  Proactive Discovery of Insider Threats Using G...  \n",
       "554                                     Robot learning  \n",
       "555                                    Computer vision  \n",
       "556                          Facial recognition system  \n",
       "557                                  Outlier detection  \n",
       "558                                  Anomaly detection  \n",
       "559                                  Novelty detection  \n",
       "\n",
       "[560 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 560 entries, 0 to 559\n",
      "Data columns (total 4 columns):\n",
      "definition    558 non-null object\n",
      "href          560 non-null object\n",
      "section       560 non-null object\n",
      "title         560 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 17.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>definition</th>\n",
       "      <th>href</th>\n",
       "      <th>section</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/wiki/Comparison_of_general_and_generalized_li...</td>\n",
       "      <td>Regression analysis</td>\n",
       "      <td>Comparison of general and generalized linear m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/w/index.php?title=Counterpropagation_network&amp;...</td>\n",
       "      <td>Neural Networks</td>\n",
       "      <td>Counterpropagation network (page does not exist)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    definition                                               href  \\\n",
       "294        NaN  /wiki/Comparison_of_general_and_generalized_li...   \n",
       "402        NaN  /w/index.php?title=Counterpropagation_network&...   \n",
       "\n",
       "                 section                                              title  \n",
       "294  Regression analysis  Comparison of general and generalized linear m...  \n",
       "402      Neural Networks   Counterpropagation network (page does not exist)  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['definition'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(html) == bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup) == BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
